# Routing & Partitioning

**Date:** December 29, 2025
**Status:** Design Draft
**Component:** Streamer Routing Engine

## 1. Overview

This document describes how the Streamer routes matched events to Gateway nodes, including partitioning strategies, consistent hashing, broadcast modes, and backpressure handling.

## 2. Routing Goals

### 2.1 Requirements

- **Directed Routing**: Route events to the specific Gateway holding the subscription (primary mode)
- **Scalability**: Support horizontal scaling of both Streamer and Gateway nodes
- **Fault Tolerance**: Handle Gateway failures and reconnections
- **Load Balancing**: Distribute load evenly across Gateway nodes
- **Backpressure**: Prevent overwhelming slow consumers

### 2.2 Routing Modes

```
Mode 1: Direct Routing (Subscription-Aware)
  • Subscription knows its Gateway ID
  • 1-to-1 routing (most efficient)
  • Used when Gateway registers subscription

Mode 2: Consistent Hash (Subscription-Unaware)
  • Subscription ID hashed to Gateway
  • Used for stateless routing
  • Enables Gateway failover

Mode 3: Broadcast (Fallback)
  • Send to all Gateways (or subset)
  • Used when routing info unavailable
  • Gateways filter locally
```

## 3. Partitioning Strategy

### 3.1 Why Partition?

**Problem:** A single change stream may become a bottleneck under extreme load:
- Very high event rates (10k+ events/sec)
- Complex matching logic on each event
- Need for parallel processing

**Solution (When Needed):** Partition the workload by `(collection, documentKey)`:
```
Partition = hash(collection + documentKey) % numPartitions
```

**Default Approach:** Start with **1 partition (1 MongoDB change stream)**:
- ✅ Simpler implementation and debugging
- ✅ Lower MongoDB resource usage
- ✅ Easier state management (fewer checkpoints)
- ✅ Sufficient for most workloads (< 5k events/sec)

**When to Add Partitions:**
- Event rate consistently exceeds 5k events/sec
- Single partition shows high CPU usage (> 80%)
- Need to distribute load across multiple Streamer nodes
- Recommended: Start with 4 partitions, then scale to 8 if needed

### 3.2 Partition Assignment

```go
type PartitionManager struct {
    numPartitions int
    assignments   map[int]string  // partitionID → streamerNodeID
    ring          *hashring.HashRing
}

func (pm *PartitionManager) GetPartition(collection, docKey string) int {
    // With numPartitions = 1, always returns 0 (single change stream)
    if pm.numPartitions == 1 {
        return 0
    }

    // When partitioning is enabled (numPartitions > 1)
    key := collection + "/" + docKey
    hash := fnv.New64a()
    hash.Write([]byte(key))
    return int(hash.Sum64() % uint64(pm.numPartitions))
}

func (pm *PartitionManager) GetOwner(partitionID int) string {
    return pm.assignments[partitionID]
}
```

### 3.3 Single Partition Mode (Default)

When `numPartitions = 1`:
- Only 1 MongoDB change stream connection
- Simpler code paths (no hash computation needed)
- Single checkpoint to manage
- Sufficient for most use cases

```go
// Example: Single partition startup
func (s *Streamer) Start(ctx context.Context) error {
    // With 1 partition, just one goroutine consuming from MongoDB
    go s.startPartitionConsumer(ctx, 0)  // partitionID = 0
    return nil
}
```

### 3.4 Multi-Partition Mode (When Needed)

When `numPartitions > 1`:
- Multiple change streams for parallel processing
- Hash-based partition assignment
- More complex state management
- Enable when single partition can't keep up

### 3.5 Change Stream Topology Summary

The total number of change streams depends on MongoDB deployment topology:

| Scenario | MongoDB Setup | Partitions | Change Streams | Notes |
|----------|---------------|------------|----------------|-------|
| **Single Instance** | 1 Primary | 1 | **1** | Default setup |
| **Read/Write Split** | 1 Primary + 1 Replica | 1 | **1** | Only monitor Primary |
| **Multi-Backend (3)** | 3 separate instances | 1+1+1 | **3** | One per backend |
| **Partitioned** | 1 Primary | 4 | **4** | High load scenario |
| **Mixed** | 2 backends (4+1) | 4+1 | **5** | default_mongo(4) + vip(1) |

**Formula:**
```
Total Change Streams = Σ (partitions per MongoDB backend)
```

**Key Rules:**
- ✅ Monitor Primary only (not replicas) in read/write split
- ✅ One change stream per distinct MongoDB instance
- ✅ Each backend can have multiple partitions
- ❌ Never monitor replicas (avoid duplicate events)

### 3.6 Partition Lifecycle (Multi-Node Cluster)

```
┌─────────────────────────────────────────────────────┐
│ Streamer Cluster Startup                            │
└──────────────────┬──────────────────────────────────┘
                   │
                   ▼
         ┌─────────────────────┐
         │ Register in etcd    │
         │ /syntrix/streamer/  │
         │   node-1: ready     │
         └──────────┬──────────┘
                    │
                    ▼
         ┌─────────────────────┐
         │ Control Plane       │
         │ assigns partitions  │
         │                     │
         │ node-1: [0,1,2,3]   │
         │ node-2: [4,5,6,7]   │
         └──────────┬──────────┘
                    │
                    ▼
         ┌─────────────────────┐
         │ Start consuming     │
         │ assigned partitions │
         └─────────────────────┘
```

### 3.7 Rebalancing

When a Streamer node joins or leaves:

```go
func (pm *PartitionManager) Rebalance(nodes []string) error {
    // 1. Build consistent hash ring
    ring := hashring.New(nodes)

    // 2. Assign each partition to a node
    newAssignments := make(map[int]string)
    for partID := 0; partID < pm.numPartitions; partID++ {
        node := ring.Get(fmt.Sprintf("partition-%d", partID))
        newAssignments[partID] = node
    }

    // 3. Detect moves
    moves := pm.detectMoves(pm.assignments, newAssignments)

    // 4. Handoff partitions
    for _, move := range moves {
        if err := pm.handoffPartition(move); err != nil {
            return err
        }
    }

    pm.assignments = newAssignments
    return nil
}

type PartitionMove struct {
    PartitionID int
    FromNode    string
    ToNode      string
    ResumeToken bson.Raw
}

func (pm *PartitionManager) handoffPartition(move *PartitionMove) error {
    // 1. Donor stops consuming
    pm.sendCommand(move.FromNode, StopPartition{ID: move.PartitionID})

    // 2. Donor flushes pending events
    pm.sendCommand(move.FromNode, FlushPartition{ID: move.PartitionID})

    // 3. Donor checkpoints resume token
    token, err := pm.getResumeToken(move.FromNode, move.PartitionID)
    if err != nil {
        return err
    }

    // 4. Recipient starts consuming from token
    pm.sendCommand(move.ToNode, StartPartition{
        ID:          move.PartitionID,
        ResumeToken: token,
    })

    log.Infof("Handed off partition %d: %s → %s", move.PartitionID, move.FromNode, move.ToNode)
    return nil
}
```

## 4. Routing Engine

### 4.1 Routing Interface

```go
type Router interface {
    Route(matches []*MatchResult) (map[string][]*DeliveryBatch, error)
}

type DeliveryBatch struct {
    GatewayID string
    Events    []*MatchResult
    Priority  Priority
}

type Priority int

const (
    PriorityHigh   Priority = iota  // Real-time subscriptions
    PriorityNormal                   // Standard subscriptions
    PriorityLow                      // Background/audit subscriptions
)
```

### 4.2 Direct Router (Primary)

```go
type DirectRouter struct {
    batcher    *EventBatcher
    backpressure *BackpressureMonitor
}

func (r *DirectRouter) Route(matches []*MatchResult) (map[string][]*DeliveryBatch, error) {
    // Group by Gateway ID
    byGateway := make(map[string][]*MatchResult)
    for _, match := range matches {
        byGateway[match.GatewayID] = append(byGateway[match.GatewayID], match)
    }

    // Create batches
    batches := make(map[string][]*DeliveryBatch)
    for gwID, events := range byGateway {
        // Check backpressure
        bp := r.backpressure.Check(gwID)
        if bp == BackpressureHard {
            // Drop or DLQ
            log.Warnf("Gateway %s under hard backpressure, dropping %d events", gwID, len(events))
            continue
        }

        // Add to batch
        batch := r.batcher.Add(gwID, events)
        if batch != nil {
            batches[gwID] = []*DeliveryBatch{batch}
        }
    }

    return batches, nil
}
```

### 4.3 Consistent Hash Router (Fallback)

```go
type ConsistentHashRouter struct {
    ring       *hashring.HashRing
    gatewayIDs []string
    batcher    *EventBatcher
}

func (r *ConsistentHashRouter) Route(matches []*MatchResult) (map[string][]*DeliveryBatch, error) {
    byGateway := make(map[string][]*MatchResult)

    for _, match := range matches {
        // Hash subscription ID to Gateway
        gwID := r.ring.Get(match.SubscriptionID)
        byGateway[gwID] = append(byGateway[gwID], match)
    }

    // Create batches
    batches := make(map[string][]*DeliveryBatch)
    for gwID, events := range byGateway {
        batch := r.batcher.Add(gwID, events)
        if batch != nil {
            batches[gwID] = []*DeliveryBatch{batch}
        }
    }

    return batches, nil
}
```

### 4.4 Broadcast Router (Emergency)

```go
type BroadcastRouter struct {
    gatewayIDs []string
    maxFanout  int  // Limit broadcast scope (default: 3)
}

func (r *BroadcastRouter) Route(matches []*MatchResult) (map[string][]*DeliveryBatch, error) {
    // Broadcast to all Gateways (or subset)
    batch := &DeliveryBatch{
        Events:   matches,
        Priority: PriorityLow,  // Broadcast has lower priority
    }

    batches := make(map[string][]*DeliveryBatch)

    // Limit fanout to avoid amplification
    recipients := r.selectRecipients(r.maxFanout)
    for _, gwID := range recipients {
        batches[gwID] = []*DeliveryBatch{batch}
    }

    return batches, nil
}

func (r *BroadcastRouter) selectRecipients(max int) []string {
    if max <= 0 || max >= len(r.gatewayIDs) {
        return r.gatewayIDs
    }

    // Select random subset
    rand.Shuffle(len(r.gatewayIDs), func(i, j int) {
        r.gatewayIDs[i], r.gatewayIDs[j] = r.gatewayIDs[j], r.gatewayIDs[i]
    })

    return r.gatewayIDs[:max]
}
```

## 5. Event Batching

### 5.1 Batching Strategy

```go
type EventBatcher struct {
    batches   map[string]*Batch
    maxSize   int           // Max events per batch (default: 100)
    maxWait   time.Duration // Max wait time (default: 50ms)
    mu        sync.Mutex
}

type Batch struct {
    GatewayID string
    Events    []*MatchResult
    StartTime time.Time
}

func (b *EventBatcher) Add(gwID string, events []*MatchResult) *DeliveryBatch {
    b.mu.Lock()
    defer b.mu.Unlock()

    batch := b.getOrCreate(gwID)
    batch.Events = append(batch.Events, events...)

    // Check if ready to send
    if b.isReady(batch) {
        result := &DeliveryBatch{
            GatewayID: gwID,
            Events:    batch.Events,
            Priority:  PriorityNormal,
        }

        // Reset batch
        delete(b.batches, gwID)

        return result
    }

    return nil
}

func (b *EventBatcher) isReady(batch *Batch) bool {
    return len(batch.Events) >= b.maxSize ||
        time.Since(batch.StartTime) >= b.maxWait
}
```

### 5.2 Flush Timer

```go
func (b *EventBatcher) StartFlushLoop(ctx context.Context) {
    ticker := time.NewTicker(10 * time.Millisecond)
    defer ticker.Stop()

    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            b.flushStale()
        }
    }
}

func (b *EventBatcher) flushStale() {
    b.mu.Lock()
    defer b.mu.Unlock()

    now := time.Now()
    for gwID, batch := range b.batches {
        if now.Sub(batch.StartTime) >= b.maxWait {
            // Send partial batch
            // (delivered via callback or channel)
            log.Debugf("Flushing stale batch for gateway %s (%d events)", gwID, len(batch.Events))
            delete(b.batches, gwID)
        }
    }
}
```

## 6. Backpressure Management

### 6.1 Backpressure Levels

```go
type BackpressureLevel int

const (
    BackpressureNone BackpressureLevel = iota
    BackpressureSoft  // Warning, start shedding optional load
    BackpressureHard  // Critical, drop or DLQ
)

type BackpressureMonitor struct {
    queueSizes map[string]int  // gatewayID → queue size
    watermarks Watermarks
    mu         sync.RWMutex
}

type Watermarks struct {
    SoftPercent int  // 70%
    HardPercent int  // 90%
    QueueSize   int  // Max queue size (default: 10000)
}
```

### 6.2 Backpressure Check

```go
func (bm *BackpressureMonitor) Check(gatewayID string) BackpressureLevel {
    bm.mu.RLock()
    defer bm.mu.RUnlock()

    size := bm.queueSizes[gatewayID]
    softThreshold := (bm.watermarks.QueueSize * bm.watermarks.SoftPercent) / 100
    hardThreshold := (bm.watermarks.QueueSize * bm.watermarks.HardPercent) / 100

    if size >= hardThreshold {
        return BackpressureHard
    }
    if size >= softThreshold {
        return BackpressureSoft
    }
    return BackpressureNone
}

func (bm *BackpressureMonitor) UpdateQueueSize(gatewayID string, size int) {
    bm.mu.Lock()
    defer bm.mu.Unlock()
    bm.queueSizes[gatewayID] = size
}
```

### 6.3 Backpressure Response

```go
func (r *DirectRouter) handleBackpressure(gwID string, events []*MatchResult) error {
    bp := r.backpressure.Check(gwID)

    switch bp {
    case BackpressureNone:
        // Send normally
        return r.deliver(gwID, events)

    case BackpressureSoft:
        // Shed low-priority events
        highPri := r.filterHighPriority(events)
        log.Warnf("Gateway %s soft backpressure: shedding %d/%d events",
            gwID, len(events)-len(highPri), len(events))
        return r.deliver(gwID, highPri)

    case BackpressureHard:
        // Send to DLQ or drop
        log.Errorf("Gateway %s hard backpressure: sending %d events to DLQ", gwID, len(events))
        return r.sendToDLQ(gwID, events)
    }

    return nil
}
```

## 7. Delivery Protocol

### 7.1 HTTP/2 Delivery (Primary)

```go
type HTTPDelivery struct {
    client    *http.Client
    endpoints map[string]string  // gatewayID → URL
}

func (d *HTTPDelivery) Deliver(ctx context.Context, batch *DeliveryBatch) error {
    endpoint := d.endpoints[batch.GatewayID]
    if endpoint == "" {
        return fmt.Errorf("gateway %s not found", batch.GatewayID)
    }

    // Serialize events
    payload := &StreamerEventBatch{
        Events: make([]*StreamerEvent, len(batch.Events)),
    }
    for i, match := range batch.Events {
        payload.Events[i] = &StreamerEvent{
            SubscriptionID: match.SubscriptionID,
            Event:          match.Event,
            Seq:            match.Seq,
            PartitionID:    match.PartitionID,
        }
    }

    body, _ := json.Marshal(payload)

    // Send HTTP POST
    req, err := http.NewRequestWithContext(ctx, "POST",
        fmt.Sprintf("%s/internal/v1/streamer/events", endpoint),
        bytes.NewBuffer(body))
    if err != nil {
        return err
    }

    req.Header.Set("Content-Type", "application/json")
    req.Header.Set("X-Streamer-Node", d.nodeID)

    resp, err := d.client.Do(req)
    if err != nil {
        return fmt.Errorf("delivery failed: %w", err)
    }
    defer resp.Body.Close()

    if resp.StatusCode != http.StatusOK {
        return fmt.Errorf("delivery failed with status %d", resp.StatusCode)
    }

    return nil
}
```

### 7.2 gRPC Delivery (Alternative)

```protobuf
syntax = "proto3";

service StreamerService {
  rpc DeliverEvents(EventBatch) returns (DeliveryAck);
}

message EventBatch {
  string gateway_id = 1;
  repeated StreamerEvent events = 2;
}

message StreamerEvent {
  string subscription_id = 1;
  string event_type = 2;
  string tenant = 3;
  string collection = 4;
  string document_id = 5;
  bytes payload = 6;
  int64 seq = 7;
  int32 partition_id = 8;
}

message DeliveryAck {
  int32 accepted = 1;
  int32 rejected = 2;
}
```

### 7.3 NATS Delivery (Push-based)

```go
type NATSDelivery struct {
    conn *nats.Conn
}

func (d *NATSDelivery) Deliver(ctx context.Context, batch *DeliveryBatch) error {
    subject := fmt.Sprintf("syntrix.streamer.events.%s", batch.GatewayID)

    payload, _ := json.Marshal(batch.Events)

    if err := d.conn.Publish(subject, payload); err != nil {
        return fmt.Errorf("NATS publish failed: %w", err)
    }

    return nil
}
```

## 8. Retry & DLQ

### 8.1 Retry Policy

```go
type RetryPolicy struct {
    MaxAttempts int           // Default: 3
    InitialDelay time.Duration // Default: 100ms
    MaxDelay    time.Duration // Default: 5s
    Multiplier  float64       // Default: 2.0
}

func (r *RetryPolicy) Backoff(attempt int) time.Duration {
    delay := float64(r.InitialDelay) * math.Pow(r.Multiplier, float64(attempt))
    if delay > float64(r.MaxDelay) {
        delay = float64(r.MaxDelay)
    }
    return time.Duration(delay)
}
```

### 8.2 Retry Logic

```go
func (d *HTTPDelivery) DeliverWithRetry(ctx context.Context, batch *DeliveryBatch) error {
    policy := d.retryPolicy

    for attempt := 0; attempt < policy.MaxAttempts; attempt++ {
        err := d.Deliver(ctx, batch)
        if err == nil {
            return nil
        }

        if attempt < policy.MaxAttempts-1 {
            backoff := policy.Backoff(attempt)
            log.Warnf("Delivery attempt %d failed, retrying in %v: %v", attempt+1, backoff, err)
            time.Sleep(backoff)
        }
    }

    // Max retries exhausted, send to DLQ
    return d.sendToDLQ(batch)
}
```

### 8.3 Dead Letter Queue

```go
type DLQManager struct {
    storage DLQStorage
    metrics *DLQMetrics
}

type DLQStorage interface {
    Store(batch *DeliveryBatch, reason string) error
    List(limit int) ([]*DLQEntry, error)
    Retry(entryID string) error
    Delete(entryID string) error
}

type DLQEntry struct {
    ID        string
    Batch     *DeliveryBatch
    Reason    string
    Timestamp time.Time
    Retries   int
}

func (d *DLQManager) Send(batch *DeliveryBatch, reason string) error {
    entry := &DLQEntry{
        ID:        uuid.New().String(),
        Batch:     batch,
        Reason:    reason,
        Timestamp: time.Now(),
    }

    if err := d.storage.Store(batch, reason); err != nil {
        return err
    }

    d.metrics.DLQEvents.Add(float64(len(batch.Events)))
    log.Errorf("Sent batch to DLQ: gateway=%s events=%d reason=%s", batch.GatewayID, len(batch.Events), reason)

    return nil
}
```

## 9. Metrics

```go
type RoutingMetrics struct {
    // Routing
    RoutingLatency        prometheus.Histogram
    EventsRouted          prometheus.Counter
    BatchesDelivered      prometheus.Counter

    // Backpressure
    BackpressureEvents    *prometheus.CounterVec  // Labels: gatewayId, level
    QueueSizes            *prometheus.GaugeVec    // Labels: gatewayId

    // Delivery
    DeliveryLatency       prometheus.Histogram
    DeliverySuccess       prometheus.Counter
    DeliveryErrors        prometheus.Counter
    DeliveryRetries       prometheus.Counter

    // DLQ
    DLQEvents             prometheus.Counter
}
```

## 10. Configuration

```yaml
routing:
  # Mode
  mode: direct  # direct, consistent_hash, broadcast

  # Partitioning
  # Start with 1 for simplicity, scale to 4-8 when needed
  num_partitions: 1

  # Batching
  batch:
    max_size: 100
    max_wait: 50ms
    flush_interval: 10ms

  # Backpressure
  backpressure:
    soft_watermark: 70
    hard_watermark: 90
    queue_size: 10000

  # Delivery
  delivery:
    protocol: http  # http, grpc, nats
    timeout: 5s
    retry:
      max_attempts: 3
      initial_delay: 100ms
      max_delay: 5s
      multiplier: 2.0

  # DLQ
  dlq:
    enabled: true
    max_age: 7d
```

## 11. Testing

### 11.1 Unit Tests
- Partition assignment
- Routing logic (direct, hash, broadcast)
- Backpressure detection
- Batch creation

### 11.2 Integration Tests
- End-to-end routing
- Gateway failover
- Partition rebalancing
- DLQ flow

### 11.3 Chaos Tests
- Gateway crash during delivery
- Network partition
- Slow consumer (backpressure)
