# Streamer Service Overview

**Date:** December 29, 2025
**Status:** Design Draft
**Component:** Streamer (Real-time Event Streaming Service)

## 1. What is Streamer?

The **Streamer** is the central real-time event streaming service in Syntrix. It sits between the storage layer (MongoDB) and the Gateway layer, acting as an intelligent event processor that:

1. **Listens** to data changes from the storage backend
2. **Matches** changes against active client subscriptions
3. **Routes** matched events to the appropriate Gateway nodes
4. **Fans out** a single storage event to multiple interested subscribers

Think of it as the "heart" of Syntrix's real-time system - continuously pumping change events from the database to connected clients.

## 2. Why Do We Need Streamer?

### 2.1 The Problem Without Streamer

Without a dedicated streaming service, we would face several critical issues:

**Problem 1: Storage Overload**
```
┌─────────────┐
│  Gateway A  │────┐
└─────────────┘    │
                   ├──► MongoDB Change Stream (Collection: messages)
┌─────────────┐    │    - 1000 concurrent streams
│  Gateway B  │────┤    - High CPU/memory usage
└─────────────┘    │    - Connection pool exhaustion
       ...         │
┌─────────────┐    │
│  Gateway Z  │────┘
└─────────────┘

Each client subscription = 1 MongoDB change stream = Unsustainable at scale
```

**Problem 2: Inefficient Matching**
- Every Gateway node would need to run the same query matching logic
- Duplicate computation across nodes
- No way to share matching results

**Problem 3: Poor Scalability**
- Gateway nodes are stateful (holding client connections)
- Storage backend becomes the bottleneck
- Can't scale horizontally without storage pressure

### 2.2 The Solution: Streamer as Event Hub

```
                        ┌──────────────────────────────────┐
                        │         Streamer Cluster         │
                        │  ┌────────────────────────────┐  │
┌──────────┐           │  │  Subscription Index        │  │
│ MongoDB  │──────────►│  │  (Bloom Filter + CEL)      │  │
│ Change   │  1 stream │  └────────────────────────────┘  │
│ Stream   │           │            │                      │
└──────────┘           │            ▼                      │
                        │  ┌────────────────────────────┐  │
                        │  │  Event Matching Engine     │  │
                        │  └────────────────────────────┘  │
                        │            │                      │
                        │            ▼                      │
                        │  ┌────────────────────────────┐  │
                        │  │  Router (Fan-out to GWs)   │  │
                        │  └────────────────────────────┘  │
                        └──────────┬────────────┬──────────┘
                                   │            │
                       ┌───────────┘            └───────────┐
                       ▼                                    ▼
              ┌──────────────┐                    ┌──────────────┐
              │  Gateway A   │                    │  Gateway B   │
              │  (10k conns) │                    │  (10k conns) │
              └──────────────┘                    └──────────────┘
                      │                                    │
                      ▼                                    ▼
              ┌──────────────┐                    ┌──────────────┐
              │   Clients    │                    │   Clients    │
              └──────────────┘                    └──────────────┘
```

**Benefits:**
1. **1 MongoDB change stream → N Gateway nodes → M clients** (efficient fan-out)
2. **Centralized matching logic** (compute once, route many)
3. **Horizontal scalability** (Streamer nodes can be partitioned when needed)
4. **Storage protection** (bounded number of change streams: default 1, scale to 4-8 if needed)

## 3. Core Responsibilities

### 3.1 Subscription Management
- **Register** subscriptions from Gateway nodes
- **Build** and maintain a subscription index (Bloom filter + metadata)
- **Expire** inactive subscriptions (TTL)
- **Track** subscription distribution across Gateway nodes

### 3.2 Event Ingestion
- **Consume** MongoDB change streams (default: 1 stream, can partition if needed)
- **Deserialize** and normalize change events
- **Deduplicate** events (handle replays and retries)
- **Checkpoint** resume tokens for fault tolerance

### 3.3 Event Matching
- **Fast-path**: Bloom filter for probabilistic filtering
- **Slow-path**: CEL expression evaluation for exact matching
- **Optimization**: Prefix matching, range queries, field-level indexes

### 3.4 Event Routing
- **Partition-aware**: Route events to responsible Gateway nodes via consistent hashing
- **Fan-out**: Broadcast to multiple Gateways when needed (range queries)
- **Backpressure**: Handle slow consumers gracefully

### 3.5 Reliability
- **At-least-once delivery**: Guarantee no event loss
- **Resume capability**: Support Gateway reconnections without data loss
- **Gap detection**: Detect and recover from missing events

## 4. Multi-Tenant Handling

Syntrix is designed as a **multi-tenant system** from day one. Streamer handles tenants as follows:

### 4.1 Tenant Isolation

```
MongoDB (tenant_id in documents)
    ↓
1 Change Stream (all tenants)
    ↓
Streamer (filters by tenant_id)
    ↓
Subscriptions (tenant-scoped)
    ↓
Gateway (per-tenant events)
```

### 4.2 How It Works

**Storage Layer:**
- Every document has a `tenant_id` field (e.g., `"default"`, `"tenant-a"`)
- Document `_id` includes tenant prefix: `tenant_id:hash(fullpath)`
- Indexes: `(tenant_id, collection_hash)` for efficient queries

**Streamer Layer:**
- Subscriptions are **tenant-scoped** (each subscription belongs to one tenant)
- Events extracted from change stream include `tenant_id`
- Matching engine **only matches events to subscriptions with the same tenant**
- Quota limits enforced per-tenant (max subscriptions per tenant)

**Example:**
```go
// Event from MongoDB
{
  "_id": "tenant-a:abc123",
  "tenant_id": "tenant-a",
  "collection": "messages",
  "data": {...}
}

// Subscription from Gateway
{
  "subscriptionId": "sub-123",
  "tenant": "tenant-a",  // Must match!
  "collection": "messages",
  "filters": [...]
}

// Streamer matching
if event.Tenant == subscription.Tenant {
    // Match filters
}
```

### 4.3 Single Change Stream Strategy

**Current Design:** 1 change stream per MongoDB backend

**Default Setup (Single MongoDB):**
```
MongoDB Primary (all tenants)
    ↓
1 Change Stream
    ↓
Streamer (filters by tenant_id)
    ↓
Gateway
```

**Read/Write Split Setup:**
```
MongoDB Primary (writes)    MongoDB Replica (reads)
    ↓                           ↓
1 Change Stream              (No stream needed)
    ↓
Streamer
```

**Multi-Backend Setup (VIP tenants):**
```
default_mongo (tenant A,B,C)    mongo_vip_a (VIP tenant)
    ↓                                ↓
1 Change Stream                  1 Change Stream
    ↓                                ↓
    └────────────┬───────────────────┘
                 ▼
             Streamer
             (2 streams total)
```

**Why:**
- ✅ Simple: No per-tenant stream management
- ✅ Efficient: MongoDB connection count = number of distinct backends (not tenant count)
- ✅ Flexible: Tenant count can grow without infrastructure changes

**Trade-offs:**
- ⚠️ No hard isolation: All tenant events flow through same stream
- ⚠️ Noisy neighbor: High-volume tenant affects latency for others
- ✅ Mitigated by: Application-layer filtering and quota limits

### 4.4 Future Optimizations (When Needed)

If tenant isolation becomes a concern:

**Option 1: MongoDB Change Stream filtering (most efficient)**
```yaml
change_stream:
  pipeline:
    - $match:
        "fullDocument.tenant_id": { $in: ["tenant-a", "tenant-b"] }
```

**Option 2: Dedicated MongoDB backends for VIP tenants**
```yaml
# Storage configuration (from storage layer)
storage:
  tenants:
    default:
      backend: default_mongo
    vip-tenant-a:
      backend: mongo_vip_a  # Dedicated MongoDB instance

# Streamer configuration
streamer:
  backends:
    default_mongo:
      partitions: 1
      connection: "mongodb://primary:27017"
    mongo_vip_a:
      partitions: 1  # Separate change stream for VIP tenant
      connection: "mongodb://vip-a:27017"
```

**Important:**
- For **read/write split**: Only monitor the **Primary** (writes), not replicas
- For **dedicated backends**: Create one change stream per distinct MongoDB instance
- Total change streams = number of distinct MongoDB backends × partitions per backend

## 5. Non-Goals (Out of Scope)

The Streamer does **NOT**:
- ❌ **Maintain client WebSocket/SSE connections** (Gateway's job)
- ❌ **Authenticate or authorize clients** (Gateway's job)
- ❌ **Store historical events** (Query Engine's job)
- ❌ **Execute trigger webhooks** (Trigger Service's job)
- ❌ **Perform complex aggregations** (Query Engine's job)
- ❌ **Enforce tenant-level auth** (Gateway validates tenant access)

**Streamer's focus:** Pure event streaming and routing, nothing more.

## 6. Key Design Principles

### 6.1 Separation of Concerns
```
Gateway:   Client connection management + Protocol handling
Streamer:  Event matching + Routing
Storage:   Data persistence + Change streams
```

### 6.2 Stateless Where Possible
- Subscription index can be rebuilt from Gateway registrations
- Resume tokens persisted externally (etcd/DB)
- No local state required for event processing

### 6.3 Performance First
- Target: < 200ms end-to-end latency (storage → client)
- Strategy: Bloom filters, batching, efficient serialization
- Monitoring: Per-partition lag, match rate, routing latency

### 6.4 Fail-Safe Degradation
- If matching is slow → fall back to broadcast (small scope)
- If Bloom FP rate is high → trigger rebuild
- If partition is lagging → pause and catch up

## 7. Architecture at a Glance

### 7.1 Components
```
┌──────────────────────────────────────────────────────────┐
│                    Streamer Node                         │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  ┌─────────────┐  ┌──────────────┐  ┌───────────────┐  │
│  │  Ingestion  │  │ Subscription │  │    Router     │  │
│  │   Engine    │  │    Index     │  │   (Fan-out)   │  │
│  │             │  │              │  │               │  │
│  │ • Consumer  │  │ • Bloom      │  │ • Consistent  │  │
│  │ • Dedup     │  │ • CEL Cache  │  │   Hash        │  │
│  │ • Checkpoint│  │ • TTL Expiry │  │ • Broadcast   │  │
│  └──────┬──────┘  └──────┬───────┘  └───────┬───────┘  │
│         │                │                  │          │
│         └────────────────┼──────────────────┘          │
│                          │                             │
│                   ┌──────▼──────┐                      │
│                   │   Matcher   │                      │
│                   │  (Executor) │                      │
│                   └─────────────┘                      │
│                                                          │
└──────────────────────────────────────────────────────────┘
```

### 7.2 Data Flow
```
1. MongoDB Change → Ingestion Engine (default: 1 change stream, can partition if needed)
2. Ingestion → Dedupe + Checkpoint
3. Event → Matcher (Bloom filter → CEL evaluation)
4. Matched → Router (select target Gateways)
5. Router → Gateway (push via HTTP/gRPC/NATS)
6. Gateway → Clients (over WebSocket/SSE)
```

## 8. Performance Targets

Based on the requirements in [000_requirements.md](../000_requirements.md):

| Metric | Target | Strategy |
|--------|--------|----------|
| **Latency** | < 200ms (storage → client) | Bloom filters, batching, efficient routing |
| **Throughput** | 10k events/s | Single partition sufficient for MVP, add partitions if needed |
| **Connections** | 1M concurrent clients | Gateway handles connections, Streamer routes |
| **Matching** | 100k active subscriptions | Bloom filter (< 1% FP rate) |
| **Availability** | 99.9% uptime | Partition rebalancing, resume tokens |

## 9. Integration Points

### 9.1 Upstream: Storage Layer
- **Input**: MongoDB change stream events
- **Protocol**: Native driver (mongo-go-driver)
- **Contract**: `storage.Event` struct

### 9.2 Downstream: Gateway Cluster
- **Output**: Matched events
- **Protocol**: HTTP/2 (gRPC) or NATS (push-based)
- **Contract**: `StreamerEvent` (subID, event, seq, partition)

### 9.3 Sidecar: Subscription Registry
- **Registration**: Gateway → Streamer (HTTP POST)
- **Heartbeat**: Keep-alive for active subscriptions
- **Expiry**: TTL-based cleanup (default 5 minutes)

### 9.4 Control Plane
- **Coordination**: etcd or Kubernetes API
- **State**: Resume tokens, partition assignments
- **Health**: Metrics, status endpoints

## 10. Evolution Path

### Phase 1: MVP (Current → 3 months)
- Basic subscription index (in-memory map)
- Single Streamer node with 1 partition (1 MongoDB change stream)
- Direct MongoDB change stream consumption
- HTTP streaming to Gateway
- Target: 10k connections, 1k events/s

### Phase 2: Scale (3-6 months)
- Bloom filter implementation
- Multi-partition processing (4-8 partitions) when needed
- Multi-node Streamer cluster
- gRPC or NATS for Gateway push
- Target: 100k connections, 5k events/s

### Phase 3: Production (6-12 months)
- CEL expression caching and optimization
- Resume token persistence (etcd)
- Gap detection and recovery
- Shadow validation for matcher changes
- Target: 1M connections, 10k events/s

## 11. Success Metrics

### 10.1 Correctness
- ✅ Zero event loss (at-least-once delivery)
- ✅ No duplicate events for same subscription
- ✅ Correct matching (no false negatives)

### 10.2 Performance
- ✅ P99 latency < 200ms (storage event → Gateway)
- ✅ Bloom FP rate < 1% (false positives)
- ✅ Throughput ≥ 10k events/s per node

### 10.3 Reliability
- ✅ Resume without data loss after node restart
- ✅ Graceful degradation under load
- ✅ No MongoDB connection exhaustion

## 12. Next Steps

This overview provides the "why" and "what" of the Streamer service. For detailed design:

- **[001.architecture.md](001.architecture.md)** - Component design and interfaces
- **[002.subscription.md](002.subscription.md)** - Subscription management details
- **[003.matching.md](003.matching.md)** - Event matching algorithms
- **[004.routing.md](004.routing.md)** - Routing strategies and partitioning
- **[005.reliability.md](005.reliability.md)** - Fault tolerance and recovery
- **[006.observability.md](006.observability.md)** - Monitoring and debugging

## 13. References

- [Requirements](../000_requirements.md) - Overall system requirements
- [Architecture](../001_architecture.md) - Syntrix architecture overview
- [Realtime Watching](../006_realtime_watching.md) - Original CSP design (to be replaced)
- [Query Engine](../engine/001.architecture.md) - Data layer integration
