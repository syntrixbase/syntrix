# Streamer Architecture

**Date:** December 29, 2025
**Status:** Design Draft
**Component:** Streamer Internal Architecture

## 1. Overview

This document details the internal architecture of the Streamer service, including component design, interfaces, data structures, and interaction patterns.

## 2. Component Architecture

### 2.1 High-Level Component Diagram

```
                    Puller Service
                   (gRPC Streaming)
                           │
                           ▼
┌────────────────────────────────────────────────────────────────────┐
│                         Streamer Service                            │
├────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  ┌──────────────────────────────────────────────────────────────┐ │
│  │                    Control Plane API                          │ │
│  │  • Subscription Registration (Gateway → Streamer)             │ │
│  │  • Heartbeat / Keep-Alive                                     │ │
│  │  • Health & Metrics                                           │ │
│  └────────────────────┬─────────────────────────────────────────┘ │
│                       │                                            │
│  ┌────────────────────▼─────────────────────────────────────────┐ │
│  │              Subscription Index Manager                       │ │
│  │  ┌────────────┐  ┌──────────────┐  ┌──────────────────────┐ │ │
│  │  │   Index    │  │ Bloom Filter │  │  CEL Expression      │ │ │
│  │  │  Registry  │  │   Builder    │  │      Cache           │ │ │
│  │  └────────────┘  └──────────────┘  └──────────────────────┘ │ │
│  └────────────────────┬─────────────────────────────────────────┘ │
│                       │                                            │
│  ┌────────────────────▼─────────────────────────────────────────┐ │
│  │           Puller gRPC Consumer                               │ │
│  │  ┌────────────┐  ┌──────────────┐  ┌──────────────────────┐ │ │
│  │  │   gRPC     │  │ Deduplicator │  │   Event Parser       │ │ │
│  │  │ Subscriber │  │              │  │                      │ │ │
│  │  │            │  │ (3min cache) │  │                      │ │ │
│  │  └────────────┘  └──────────────┘  └──────────────────────┘ │ │
│  └────────────────────┬─────────────────────────────────────────┘ │
│                       │                                            │
│  ┌────────────────────▼─────────────────────────────────────────┐ │
│  │              Matching Engine (CEL EXECUTION HERE!)            │ │
│  │  ┌────────────┐  ┌──────────────┐  ┌──────────────────────┐ │ │
│  │  │   Bloom    │  │     CEL      │  │   Match Result       │ │ │
│  │  │   Filter   │──►│  Evaluator   │──►│    Builder           │ │ │
│  │  │ (Fast Path)│  │ (Exact Match)│  │                      │ │ │
│  │  └────────────┘  └──────────────┘  └──────────────────────┘ │ │
│  └────────────────────┬─────────────────────────────────────────┘ │
│                       │                                            │
│  ┌────────────────────▼─────────────────────────────────────────┐ │
│  │            JetStream Event Publisher                         │ │
│  │  ┌────────────┐  ┌──────────────┐  ┌──────────────────────┐ │ │
│  │  │  Publisher │  │    Event     │  │   Backpressure       │ │ │
│  │  │            │  │   Batcher    │  │     Monitor          │ │ │
│  │  │            │  │              │  │                      │ │ │
│  │  └────────────┘  └──────────────┘  └──────────────────────┘ │ │
│  └──────────────────────────────────────────────────────────────┘ │
│                                                                     │
└─────────────────────────────┬───────────────────────────────────────┘
                              │ NATS JetStream
                              │ events.{gatewayId}.{subscriptionId}
                              ▼
                    API Gateway Realtime
```

**Key Changes from Original Design:**
- **Removed Direct MongoDB Connection**: Streamer now consumes from Puller via gRPC
- **Removed HTTP/2 Delivery**: Replaced with JetStream publish to Gateway
- **Simplified Routing**: No complex routing needed, JetStream subject-based routing
- **CEL in Streamer**: Full CEL evaluation happens in Streamer, Gateway only forwards

## 3. Core Components

### 3.1 Control Plane API

**Purpose:** Expose HTTP/gRPC endpoints for subscription management and monitoring.

**Endpoints:**

```go
// Subscription Registration
POST /internal/v1/subscriptions
{
  "gatewayId": "gw-1",
  "subscriptionId": "sub-123",
  "tenant": "default",
  "collection": "messages",
  "filters": [
    {"field": "status", "op": "==", "value": "active"}
  ],
  "ttl": 300  // seconds
}

// Subscription Renewal (Heartbeat)
PUT /internal/v1/subscriptions/:id/heartbeat

// Subscription Removal
DELETE /internal/v1/subscriptions/:id

// Health Check
GET /health

// Metrics
GET /metrics
```

**Interface:**

```go
type ControlPlaneAPI interface {
    RegisterSubscription(ctx context.Context, req *RegisterRequest) (*RegisterResponse, error)
    RenewSubscription(ctx context.Context, subID string) error
    UnregisterSubscription(ctx context.Context, subID string) error
    GetHealth(ctx context.Context) (*HealthResponse, error)
    GetMetrics(ctx context.Context) (*MetricsResponse, error)
}
```

### 3.2 Subscription Index Manager

**Purpose:** Maintain an efficient index of all active subscriptions for fast matching.

**Data Structures:**

```go
type Subscription struct {
    ID          string
    GatewayID   string
    Tenant      string
    Collection  string
    Filters     []Filter
    CELProgram  cel.Program  // Pre-compiled CEL expression
    BloomKeys   []uint64     // Hash keys for Bloom filter
    ExpiresAt   time.Time
    CreatedAt   time.Time
    UpdatedAt   time.Time
}

type SubscriptionIndex struct {
    // Fast lookup by collection
    byCollection map[string][]*Subscription

    // Fast lookup by subscription ID
    byID map[string]*Subscription

    // Bloom filter per collection
    bloomFilters map[string]*BloomFilter

    // TTL expiry heap
    expiryHeap *TTLHeap
}
```

**Interface:**

```go
type SubscriptionIndexManager interface {
    Add(sub *Subscription) error
    Remove(subID string) error
    Renew(subID string) error
    GetByCollection(collection string) []*Subscription
    GetBloomFilter(collection string) *BloomFilter
    ExpireStale(ctx context.Context) int  // Returns number expired
}
```

**Operations:**

1. **Add**: Insert subscription, update Bloom filter, add to expiry heap
2. **Remove**: Delete from all indexes, rebuild Bloom if needed
3. **Renew**: Update TTL timestamp
4. **Expire**: Background goroutine checks expiry heap every 10s

### 3.3 Puller gRPC Consumer

**Purpose:** Consume normalized events from Puller service via gRPC streaming.

**Data Flow:**

```go
Puller (gRPC Stream) → Consumer → Deduplicator → Matcher
```

**Consumer:**

```go
type PullerGRPCConsumer struct {
    client       pullerv1.PullerServiceClient
    consumerID   string
    coalesceOnCatchUp bool
    progress     string  // last saved progress marker
}

func (c *PullerGRPCConsumer) Start(ctx context.Context) error {
    // Load last saved progress marker
    c.progress = c.loadProgress()

    // Subscribe to Puller events via gRPC streaming
    req := &pullerv1.SubscribeRequest{
        ConsumerId:        c.consumerID,
        After:             c.progress,  // resume from last position
        CoalesceOnCatchUp: c.coalesceOnCatchUp,
    }

    stream, err := c.client.Subscribe(ctx, req)
    if err != nil {
        return fmt.Errorf("failed to subscribe to puller: %w", err)
    }

    go c.consumeLoop(ctx, stream)
    return nil
}

func (c *PullerGRPCConsumer) consumeLoop(ctx context.Context, stream pullerv1.PullerService_SubscribeClient) {
    for {
        evt, err := stream.Recv()
        if err == io.EOF {
            return
        }
        if err != nil {
            log.Error("Failed to receive event: %v", err)
            // Reconnect logic
            c.reconnect(ctx)
            return
        }

        c.handleEvent(ctx, evt)
    }
}

func (c *PullerGRPCConsumer) handleEvent(ctx context.Context, evt *pullerv1.Event) {
    // Decode full document
    var fullDoc map[string]interface{}
    if len(evt.FullDocument) > 0 {
        json.Unmarshal(evt.FullDocument, &fullDoc)
    }

    // Convert to NormalizedEvent
    normalized := &NormalizedEvent{
        Type:       EventType(evt.OperationType),
        Tenant:     evt.Tenant,
        Collection: evt.Collection,
        DocumentID: evt.DocumentId,
        FullDoc:    fullDoc,
    }

    // Process: Deduplicate → Match → Route → Publish
    if err := c.processEvent(normalized); err != nil {
        log.Error("Failed to process event: %v", err)
        // Continue processing other events
    }

    // Save progress marker (consumer's responsibility)
    c.saveProgress(evt.Progress)
}
```

**Reconnection Strategy:**

```go
func (c *PullerGRPCConsumer) reconnect(ctx context.Context) {
    backoff := 100 * time.Millisecond
    maxBackoff := 30 * time.Second

    for {
        select {
        case <-ctx.Done():
            return
        case <-time.After(backoff):
            if err := c.Start(ctx); err != nil {
                log.Warn("Reconnect failed: %v, retrying...", err)
                backoff = min(backoff*2, maxBackoff)
                continue
            }
            log.Info("Reconnected to Puller successfully")
            return
        }
    }
}
```

**Benefits of gRPC Integration:**

1. **No Direct MongoDB Connection**: Streamer doesn't need MongoDB credentials or connection management
2. **Progress Marker**: Each event includes progress marker for consumer to save, enables resume on reconnect
3. **Catch-up Coalescing**: When behind, Puller can coalesce events to speed up catch-up
4. **Backpressure Handling**: gRPC flow control handles slow consumers
5. **Decoupling**: Puller and Streamer can scale independently

**Deduplicator:**

```go
type Deduplicator struct {
    cache *lru.Cache  // Key: (clusterTime, txnID, docKey), TTL: 3 minutes
}

func (d *Deduplicator) IsDuplicate(evt *NormalizedEvent) bool {
    // Deduplication is still needed in case of gRPC redelivery
    // or multiple Streamer instances
    key := d.makeKey(evt.ClusterTime, evt.TxnNumber, evt.DocumentID)
    if d.cache.Contains(key) {
        return true
    }
    d.cache.Add(key, true)
    return false
}
```

### 3.4 Matching Engine

**Purpose:** Match incoming events against subscriptions using Bloom filter + CEL.

**CRITICAL: CEL Execution Happens Here**

The Matching Engine is where CEL filter expressions are evaluated. This design reduces Gateway traffic by ~90% compared to pushing all events to Gateway and filtering there.

**Two-Phase Matching:**

```go
type MatchingEngine struct {
    indexManager *SubscriptionIndexManager
    celEvaluator *CELEvaluator
}

func (m *MatchingEngine) Match(evt *NormalizedEvent) ([]*MatchResult, error) {
    // Phase 1: Bloom Filter (Fast Path)
    // Quick check: "Could this event match any subscription?"
    bloom := m.indexManager.GetBloomFilter(evt.Collection)
    candidates := m.fastPath(evt, bloom)

    if len(candidates) == 0 {
        return nil, nil  // No potential matches, skip CEL evaluation
    }

    // Phase 2: CEL Evaluation (Exact Matching)
    // This is the compute-intensive step - evaluated HERE in Streamer
    // NOT in Gateway (which only manages WebSocket connections)
    matches := make([]*MatchResult, 0)
    for _, sub := range candidates {
        // Evaluate pre-compiled CEL program against event
        if m.celEvaluator.Evaluate(sub.CELProgram, evt) {
            matches = append(matches, &MatchResult{
                SubscriptionID: sub.ID,
                GatewayID:      sub.GatewayID,
                ClientID:       sub.ClientID,
                Event:          evt,
            })
        }
    }

    return matches, nil
}
```

**CEL Evaluator:**

```go
type CELEvaluator struct {
    env *cel.Env
}

func (e *CELEvaluator) Evaluate(program cel.Program, evt *NormalizedEvent) bool {
    // Evaluate CEL expression: e.g., "doc.status == 'active' && doc.amount > 100"
    result, _, err := program.Eval(map[string]interface{}{
        "doc": evt.FullDoc,
        "op":  evt.Type,
    })

    if err != nil {
        log.Error("CEL evaluation error: %v", err)
        return false
    }

    boolResult, ok := result.Value().(bool)
    return ok && boolResult
}
```

**Why CEL in Streamer, Not Gateway?**

| Aspect | CEL in Gateway | CEL in Streamer |
|--------|---------------|-----------------|
| Gateway Traffic | 100% (all events) | ~10% (only matches) |
| Gateway CPU | High (CEL eval per client) | Low (only forward) |
| Scalability | Limited by Gateway instances | Horizontal scaling |
| Latency | Slightly lower | Negligible difference |

**Bloom Filter Configuration:**

```go
type BloomConfig struct {
    FPRate      float64  // Target false-positive rate (default: 0.01)
    BitsPerKey  int      // Bits per key (default: 10)
    HashFuncs   int      // Number of hash functions (default: 7)
    RebuildIntv time.Duration  // Rebuild interval (default: 24h)
}
```

**Note on Bloom Filter Limitations:**

Bloom filters only work for equality operators (`==`, `in`). For range queries (`>`, `<`, `>=`, `<=`), Streamer falls back to checking all subscriptions for that collection. This is a performance optimization, not a functional limitation - all filters are still evaluated correctly via CEL.

### 3.5 JetStream Event Publisher

**Purpose:** Publish matched events to Gateway nodes via NATS JetStream.

**Subject Routing Strategy:**

Each matched event is published to a specific subject that the Gateway instance subscribes to:

```
Subject Pattern: events.{gatewayId}.{subscriptionId}

Examples:
- events.gateway-1.sub-abc123
- events.gateway-2.sub-def456
```

**Publisher Implementation:**

```go
type JetStreamEventPublisher struct {
    js nats.JetStreamContext
    batcher *EventBatcher
}

func (p *JetStreamEventPublisher) Publish(ctx context.Context, matches []*MatchResult) error {
    // Publish each match to its dedicated subject
    for _, match := range matches {
        subject := fmt.Sprintf("events.%s.%s", match.GatewayID, match.SubscriptionID)

        payload, err := json.Marshal(&StreamerEvent{
            SubscriptionID: match.SubscriptionID,
            Event:          match.Event,
            PublishedAt:    time.Now(),
        })
        if err != nil {
            return fmt.Errorf("failed to marshal event: %w", err)
        }

        // Publish with async acknowledgment for performance
        _, err = p.js.PublishAsync(subject, payload)
        if err != nil {
            return fmt.Errorf("failed to publish event: %w", err)
        }
    }

    return nil
}
```

**Gateway Subscription Pattern:**

Gateway subscribes to all subjects for subscriptions it manages:

```go
// In API Gateway Realtime:
func (g *Gateway) SubscribeToStreamer(gatewayID string) error {
    // Subscribe to all events for this gateway instance
    subject := fmt.Sprintf("events.%s.>", gatewayID)

    _, err := g.js.Subscribe(subject, g.handleStreamerEvent,
        nats.ManualAck(),
        nats.DeliverAll(),
        nats.Durable(fmt.Sprintf("gateway-%s", gatewayID)))

    return err
}

func (g *Gateway) handleStreamerEvent(msg *nats.Msg) {
    var evt StreamerEvent
    json.Unmarshal(msg.Data, &evt)

    // Find client connection and forward
    client := g.hub.GetClient(evt.SubscriptionID)
    if client != nil {
        client.Send(evt.Event)
    }

    msg.Ack()
}
```

**Benefits of JetStream Pub/Sub:**

1. **Automatic Load Balancing**: NATS distributes events across Gateway instances
2. **No Explicit Routing Logic**: Subject-based routing is declarative
3. **Built-in Backpressure**: NATS handles flow control automatically
4. **Persistent Delivery**: Events are stored until acknowledged
5. **No HTTP Connection Pool**: No need to manage HTTP clients or connection limits
6. **Simplified Retry**: NATS redelivers unacked messages automatically

### 3.6 State Manager

**Purpose:** Manage Streamer's internal state (subscription metadata, metrics, health).

**Note:** Checkpoint management for MongoDB resume tokens has been moved to Puller service.

**Interface:**

```go
type StateManager interface {
    // Subscription state
    RecordSubscription(sub *Subscription) error
    RemoveSubscription(subID string) error
    GetActiveSubscriptions() ([]*Subscription, error)

    // Health metrics
    RecordMatchLatency(duration time.Duration)
    RecordPublishLatency(duration time.Duration)
    GetHealthStatus() HealthStatus
}

type EtcdStateManager struct {
    client *etcd.Client
    prefix string  // "/syntrix/streamer/state/"
}

func (s *EtcdStateManager) RecordSubscription(sub *Subscription) error {
    key := fmt.Sprintf("%s/subscriptions/%s", s.prefix, sub.ID)
    data, _ := json.Marshal(sub)
    _, err := s.client.Put(context.Background(), key, string(data))
    return err
}
```

**Health Status:**

```go
type HealthStatus struct {
    ActiveSubscriptions int
    MatchLatencyP50     time.Duration
    MatchLatencyP99     time.Duration
    PublishLatencyP50   time.Duration
    PublishLatencyP99   time.Duration
    EventRate           float64  // Events/sec processed
    Status              ServiceStatus
}

type ServiceStatus int

const (
    ServiceHealthy ServiceStatus = iota
    ServiceDegraded
    ServiceUnhealthy
)
```

## 4. Data Structures

### 4.1 Core Event Types

```go
// Raw event from MongoDB
type RawEvent struct {
    OperationType string
    FullDocument  bson.Raw
    DocumentKey   interface{}
    ClusterTime   primitive.Timestamp
    TxnNumber     *int64
}

// Normalized event for matching
type NormalizedEvent struct {
    Type        EventType
    Tenant      string
    Collection  string
    DocumentID  string
    FullDoc     map[string]interface{}
    UpdateDesc  *UpdateDescription
    ClusterTime primitive.Timestamp
    TxnNumber   *int64
}

// Match result
type MatchResult struct {
    SubscriptionID string
    GatewayID      string
    Event          *NormalizedEvent
    Seq            int64
}

// Event delivered to Gateway
type StreamerEvent struct {
    SubscriptionID string          `json:"subscriptionId"`
    Seq            int64           `json:"seq"`
    Event          *StorageEvent   `json:"event"`
}
```

## 5. Threading Model

### 5.1 Goroutine Structure

```go
func (s *Streamer) Start(ctx context.Context) error {
    // 1. Start Control Plane API (HTTP server)
    go s.controlPlane.Serve(ctx)

    // 2. Start TTL expiry loop
    go s.indexManager.StartExpiryLoop(ctx)

    // 3. Start Puller gRPC consumer (subscribes to Puller events)
    if err := s.pullerConsumer.Start(ctx); err != nil {
        return fmt.Errorf("failed to start Puller gRPC consumer: %w", err)
    }

    // 4. Start Bloom filter rebuild loop
    go s.startBloomRebuildLoop(ctx)

    // 5. Start health monitoring
    go s.stateManager.MonitorHealth(ctx)

    <-ctx.Done()
    return s.Shutdown()
}

func (s *Streamer) processEvent(evt *NormalizedEvent) error {
    // Pipeline: Dedupe → Match → Publish

    // 1. Deduplication
    if s.deduplicator.IsDuplicate(evt) {
        return nil
    }

    // 2. Matching (Bloom Filter + CEL)
    matches, err := s.matchingEngine.Match(evt)
    if err != nil {
        return fmt.Errorf("matching failed: %w", err)
    }

    if len(matches) == 0 {
        return nil  // No subscriptions matched
    }

    // 3. Publish to Gateway via JetStream
    if err := s.jetStreamPublisher.Publish(context.Background(), matches); err != nil {
        return fmt.Errorf("publish failed: %w", err)
    }

    return nil
}
```

## 6. Configuration

```yaml
streamer:
  # Server
  port: 8083

  # Puller gRPC Connection
  puller:
    address: "puller:50051"
    consumer_id: "streamer-consumer"
    coalesce_on_catch_up: true

    # Reconnect settings
    reconnect_backoff_min: 100ms
    reconnect_backoff_max: 30s

  # NATS JetStream (for Gateway delivery only)
  nats:
    urls:
      - nats://localhost:4222
    output_stream: "GATEWAY_EVENTS"

  # Subscription Index
  subscription:
    ttl: 300s              # Default subscription TTL
    expiry_check: 10s      # How often to check for expired subs

  # Bloom Filter
  bloom:
    fp_rate: 0.01          # False positive rate
    bits_per_key: 10
    hash_funcs: 7
    rebuild_interval: 24h

  # Deduplication
  dedup:
    cache_size: 10000      # Number of entries
    ttl: 3m                # Cache TTL

  # CEL Evaluation
  cel:
    max_eval_time: 100ms   # Max time for CEL evaluation per subscription
    worker_pool_size: 10   # Number of CEL evaluation workers

  # State Management
  state:
    backend: etcd          # etcd or memory
    etcd_endpoints:
      - http://localhost:2379
    sync_interval: 5s
```

## 7. Error Handling

### 7.1 Failure Modes

| Failure | Detection | Recovery |
|---------|-----------|----------|
| Puller gRPC connection lost | Stream error | Automatic reconnection with backoff |
| Puller consumer lag | Monitoring metrics | Horizontal scaling of Streamer instances |
| CEL evaluation timeout | Context timeout | Skip subscription, log error, continue |
| Bloom FP rate too high | Periodic sampling | Trigger rebuild |
| Publish to Gateway fails | JetStream error | Automatic retry by NATS (persistent stream) |
| Subscription expiry | TTL check | Remove from index, cleanup Bloom filter |

### 7.2 Graceful Degradation

```go
func (s *Streamer) handleMatchTimeout(evt *NormalizedEvent) []*MatchResult {
    // If matching takes > 100ms per subscription, skip slow ones
    ctx, cancel := context.WithTimeout(context.Background(), 100*time.Millisecond)
    defer cancel()

    matches, err := s.matchingEngine.Match(ctx, evt)
    if err == context.DeadlineExceeded {
        log.Warn("Matching timeout for event, skipping slow subscriptions")
        // Still return partial matches that completed
        return matches
    }
    return matches
}
```

### 7.3 Puller gRPC Error Handling

```go
func (c *PullerGRPCConsumer) consumeLoop(ctx context.Context, stream pullerv1.PullerService_SubscribeClient) {
    for {
        evt, err := stream.Recv()
        if err == io.EOF {
            log.Info("Puller stream closed")
            c.reconnect(ctx)
            return
        }
        if err != nil {
            log.Error("Failed to receive from Puller: %v", err)
            c.reconnect(ctx)
            return
        }

        if err := c.processEvent(evt); err != nil {
            log.Error("Failed to process event: %v", err)
            // Continue processing other events
        }

        // Save progress marker (consumer's responsibility)
        // Even if processing failed, save progress to avoid infinite retry
        c.saveProgress(evt.Progress)
    }
}
```

## 8. Next Steps

- **[002.subscription.md](002.subscription.md)** - Subscription lifecycle details
- **[003.matching.md](003.matching.md)** - Bloom filter and CEL matching algorithms
- **[004.routing.md](004.routing.md)** - Routing and partitioning strategies
- **[005.reliability.md](005.reliability.md)** - Fault tolerance and recovery
