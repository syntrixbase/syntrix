# Streamer Architecture

**Date:** December 29, 2025
**Status:** Design Draft
**Component:** Streamer Internal Architecture

## 1. Overview

This document details the internal architecture of the Streamer service, including component design, interfaces, data structures, and interaction patterns.

## 2. Component Architecture

### 2.1 High-Level Component Diagram

```
┌────────────────────────────────────────────────────────────────────┐
│                         Streamer Service                            │
├────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  ┌──────────────────────────────────────────────────────────────┐ │
│  │                    Control Plane API                          │ │
│  │  • Subscription Registration (Gateway → Streamer)             │ │
│  │  • Heartbeat / Keep-Alive                                     │ │
│  │  • Health & Metrics                                           │ │
│  └────────────────────┬─────────────────────────────────────────┘ │
│                       │                                            │
│  ┌────────────────────▼─────────────────────────────────────────┐ │
│  │              Subscription Index Manager                       │ │
│  │  ┌────────────┐  ┌──────────────┐  ┌──────────────────────┐ │ │
│  │  │   Index    │  │ Bloom Filter │  │  CEL Expression      │ │ │
│  │  │  Registry  │  │   Builder    │  │      Cache           │ │ │
│  │  └────────────┘  └──────────────┘  └──────────────────────┘ │ │
│  └────────────────────┬─────────────────────────────────────────┘ │
│                       │                                            │
│  ┌────────────────────▼─────────────────────────────────────────┐ │
│  │                  Ingestion Engine                             │ │
│  │  ┌────────────┐  ┌──────────────┐  ┌──────────────────────┐ │ │
│  │  │  Change    │  │  Normalizer  │  │    Deduplicator      │ │ │
│  │  │  Stream    │  │              │  │                      │ │ │
│  │  │  Consumer  │  │              │  │  (Cache: 3min TTL)   │ │ │
│  │  └────────────┘  └──────────────┘  └──────────────────────┘ │ │
│  └────────────────────┬─────────────────────────────────────────┘ │
│                       │                                            │
│  ┌────────────────────▼─────────────────────────────────────────┐ │
│  │                   Matching Engine                             │ │
│  │  ┌────────────┐  ┌──────────────┐  ┌──────────────────────┐ │ │
│  │  │   Bloom    │  │     CEL      │  │   Match Result       │ │ │
│  │  │   Filter   │──►│  Evaluator   │──►│    Aggregator        │ │ │
│  │  │ (Fast Path)│  │ (Slow Path)  │  │                      │ │ │
│  │  └────────────┘  └──────────────┘  └──────────────────────┘ │ │
│  └────────────────────┬─────────────────────────────────────────┘ │
│                       │                                            │
│  ┌────────────────────▼─────────────────────────────────────────┐ │
│  │                    Routing Engine                             │ │
│  │  ┌────────────┐  ┌──────────────┐  ┌──────────────────────┐ │ │
│  │  │ Consistent │  │   Broadcast  │  │   Backpressure       │ │ │
│  │  │    Hash    │  │   Strategy   │  │     Monitor          │ │ │
│  │  │   Router   │  │              │  │                      │ │ │
│  │  └────────────┘  └──────────────┘  └──────────────────────┘ │ │
│  └────────────────────┬─────────────────────────────────────────┘ │
│                       │                                            │
│  ┌────────────────────▼─────────────────────────────────────────┐ │
│  │                  Delivery Manager                             │ │
│  │  ┌────────────┐  ┌──────────────┐  ┌──────────────────────┐ │ │
│  │  │  Gateway   │  │    Event     │  │      Retry           │ │ │
│  │  │  Connector │  │   Batcher    │  │     Handler          │ │ │
│  │  │ (HTTP/gRPC)│  │              │  │                      │ │ │
│  │  └────────────┘  └──────────────┘  └──────────────────────┘ │ │
│  └──────────────────────────────────────────────────────────────┘ │
│                                                                     │
│  ┌──────────────────────────────────────────────────────────────┐ │
│  │              Checkpoint & State Manager                       │ │
│  │  • Resume Token Persistence (etcd/DB)                         │ │
│  │  • Partition Assignment Tracking                              │ │
│  │  • Gap Detection & Recovery                                   │ │
│  └──────────────────────────────────────────────────────────────┘ │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

## 3. Core Components

### 3.1 Control Plane API

**Purpose:** Expose HTTP/gRPC endpoints for subscription management and monitoring.

**Endpoints:**

```go
// Subscription Registration
POST /internal/v1/subscriptions
{
  "gatewayId": "gw-1",
  "subscriptionId": "sub-123",
  "tenant": "default",
  "collection": "messages",
  "filters": [
    {"field": "status", "op": "==", "value": "active"}
  ],
  "ttl": 300  // seconds
}

// Subscription Renewal (Heartbeat)
PUT /internal/v1/subscriptions/:id/heartbeat

// Subscription Removal
DELETE /internal/v1/subscriptions/:id

// Health Check
GET /health

// Metrics
GET /metrics
```

**Interface:**

```go
type ControlPlaneAPI interface {
    RegisterSubscription(ctx context.Context, req *RegisterRequest) (*RegisterResponse, error)
    RenewSubscription(ctx context.Context, subID string) error
    UnregisterSubscription(ctx context.Context, subID string) error
    GetHealth(ctx context.Context) (*HealthResponse, error)
    GetMetrics(ctx context.Context) (*MetricsResponse, error)
}
```

### 3.2 Subscription Index Manager

**Purpose:** Maintain an efficient index of all active subscriptions for fast matching.

**Data Structures:**

```go
type Subscription struct {
    ID          string
    GatewayID   string
    Tenant      string
    Collection  string
    Filters     []Filter
    CELProgram  cel.Program  // Pre-compiled CEL expression
    BloomKeys   []uint64     // Hash keys for Bloom filter
    ExpiresAt   time.Time
    CreatedAt   time.Time
    UpdatedAt   time.Time
}

type SubscriptionIndex struct {
    // Fast lookup by collection
    byCollection map[string][]*Subscription

    // Fast lookup by subscription ID
    byID map[string]*Subscription

    // Bloom filter per collection
    bloomFilters map[string]*BloomFilter

    // TTL expiry heap
    expiryHeap *TTLHeap
}
```

**Interface:**

```go
type SubscriptionIndexManager interface {
    Add(sub *Subscription) error
    Remove(subID string) error
    Renew(subID string) error
    GetByCollection(collection string) []*Subscription
    GetBloomFilter(collection string) *BloomFilter
    ExpireStale(ctx context.Context) int  // Returns number expired
}
```

**Operations:**

1. **Add**: Insert subscription, update Bloom filter, add to expiry heap
2. **Remove**: Delete from all indexes, rebuild Bloom if needed
3. **Renew**: Update TTL timestamp
4. **Expire**: Background goroutine checks expiry heap every 10s

### 3.3 Ingestion Engine

**Purpose:** Consume MongoDB change streams, normalize events, and deduplicate.

**Data Flow:**

```go
MongoDB Change Stream → Consumer → Normalizer → Deduplicator → Matcher
```

**Consumer:**

```go
type ChangeStreamConsumer struct {
    client       *mongo.Client
    dbName       string
    collection   string
    resumeToken  bson.Raw
    partition    int
    backend      string  // Backend identifier (for multi-backend support)
}

func (c *ChangeStreamConsumer) Start(ctx context.Context) (<-chan RawEvent, error)
func (c *ChangeStreamConsumer) Checkpoint(token bson.Raw) error
```

**Multi-Backend Support:**

When multiple MongoDB instances are configured (e.g., read/write split or multi-tenant backends), Streamer needs to handle:

1. **Read/Write Split**: Only monitor the **Primary** (write backend)
   - Change streams capture write operations
   - Replicas don't need separate streams (they replicate from primary)

2. **Multi-Tenant Dedicated Backends**: Monitor **each distinct backend**
   ```
   default_mongo (tenants: A, B, C) → 1 change stream
   mongo_vip_a (tenant: VIP-A)      → 1 change stream
   mongo_vip_b (tenant: VIP-B)      → 1 change stream
   ```

3. **Partition Strategy Per Backend**:
   - Each backend can have its own partition count
   - Default: 1 partition per backend
   - Example: `default_mongo` with 1 partition, `mongo_vip_a` with 1 partition

**Configuration Example:**

```yaml
streamer:
  backends:
    default_mongo:
      partitions: 1
      connection: "mongodb://primary:27017"
    mongo_vip_a:
      partitions: 1  # VIP tenant gets dedicated backend
      connection: "mongodb://vip-a:27017"
```

**Multi-Backend Deployment Scenarios:**

The number of change streams depends on your MongoDB topology:

**Scenario 1: Single MongoDB Instance (Default)**
```
MongoDB Primary (all tenants)
    ↓
1 Change Stream ✅
    ↓
Streamer

Total: 1 change stream
```

**Scenario 2: Read/Write Split**
```
MongoDB Primary (writes)    MongoDB Replica (reads)
    ↓                           ↓
1 Change Stream ✅          (No stream needed)
    ↓
Streamer

Total: 1 change stream (only monitor Primary)

Why:
- Change streams capture write operations
- Replicas sync from Primary
- Monitoring replicas would duplicate events
```

**Scenario 3: Multi-Tenant with Dedicated Backends**
```
default_mongo           mongo_vip_a         mongo_vip_b
(tenant A,B,C)          (VIP tenant)        (VIP tenant)
    ↓                      ↓                   ↓
1 Change Stream ✅     1 Change Stream ✅  1 Change Stream ✅
    ↓                      ↓                   ↓
    └──────────────────────┴───────────────────┘
                           ▼
                       Streamer

Total: 3 change streams (one per backend)
```

**Scenario 4: Multi-Backend with Partitioning (High Load)**
```
default_mongo (4 partitions)    mongo_vip_a (1 partition)
    ↓                                ↓
4 Change Streams ✅              1 Change Stream ✅
    ↓                                ↓
    └────────────────┬───────────────┘
                     ▼
                 Streamer

Total: 5 change streams (4 + 1)

Formula: Total = Σ(partitions per backend)
```

**Normalizer:**

```go
type EventNormalizer struct{}

func (n *EventNormalizer) Normalize(raw *mongo.ChangeEvent) (*NormalizedEvent, error) {
    return &NormalizedEvent{
        Type:        mapOperationType(raw.OperationType),
        Tenant:      extractTenant(raw),
        Collection:  extractCollection(raw),
        DocumentID:  raw.DocumentKey,
        FullDoc:     raw.FullDocument,
        UpdateDesc:  raw.UpdateDescription,
        ClusterTime: raw.ClusterTime,
        TxnNumber:   raw.TxnNumber,
    }, nil
}
```

**Deduplicator:**

```go
type Deduplicator struct {
    cache *lru.Cache  // Key: (clusterTime, txnID, docKey), TTL: 3 minutes
}

func (d *Deduplicator) IsDuplicate(evt *NormalizedEvent) bool {
    key := d.makeKey(evt.ClusterTime, evt.TxnNumber, evt.DocumentID)
    if d.cache.Contains(key) {
        return true
    }
    d.cache.Add(key, true)
    return false
}
```

### 3.4 Matching Engine

**Purpose:** Match incoming events against subscriptions using Bloom filter + CEL.

**Two-Phase Matching:**

```go
type MatchingEngine struct {
    indexManager *SubscriptionIndexManager
    celEvaluator *CELEvaluator
}

func (m *MatchingEngine) Match(evt *NormalizedEvent) ([]*MatchResult, error) {
    // Phase 1: Bloom Filter (Fast Path)
    bloom := m.indexManager.GetBloomFilter(evt.Collection)
    candidates := m.fastPath(evt, bloom)

    // Phase 2: CEL Evaluation (Slow Path)
    matches := make([]*MatchResult, 0)
    for _, sub := range candidates {
        if m.celEvaluator.Evaluate(sub.CELProgram, evt) {
            matches = append(matches, &MatchResult{
                SubscriptionID: sub.ID,
                GatewayID:      sub.GatewayID,
                Event:          evt,
            })
        }
    }

    return matches, nil
}
```

**Bloom Filter Configuration:**

```go
type BloomConfig struct {
    FPRate      float64  // Target false-positive rate (default: 0.01)
    BitsPerKey  int      // Bits per key (default: 10)
    HashFuncs   int      // Number of hash functions (default: 7)
    RebuildIntv time.Duration  // Rebuild interval (default: 24h)
}
```

### 3.5 Routing Engine

**Purpose:** Determine which Gateway nodes should receive matched events.

**Strategies:**

```go
type RoutingStrategy interface {
    Route(matches []*MatchResult) map[string][]*MatchResult  // GatewayID → Events
}

// Strategy 1: Direct Routing (subscription knows its Gateway)
type DirectRouter struct{}

func (r *DirectRouter) Route(matches []*MatchResult) map[string][]*MatchResult {
    result := make(map[string][]*MatchResult)
    for _, m := range matches {
        result[m.GatewayID] = append(result[m.GatewayID], m)
    }
    return result
}

// Strategy 2: Consistent Hash (for future: subscription doesn't specify Gateway)
type ConsistentHashRouter struct {
    ring *hashring.HashRing
}

func (r *ConsistentHashRouter) Route(matches []*MatchResult) map[string][]*MatchResult {
    result := make(map[string][]*MatchResult)
    for _, m := range matches {
        gw := r.ring.Get(m.SubscriptionID)
        result[gw] = append(result[gw], m)
    }
    return result
}
```

**Backpressure Monitoring:**

```go
type BackpressureMonitor struct {
    queueSizes map[string]int  // GatewayID → queue size
    watermarks struct {
        soft int  // 70% threshold
        hard int  // 90% threshold
    }
}

func (b *BackpressureMonitor) Check(gatewayID string) BackpressureLevel {
    size := b.queueSizes[gatewayID]
    if size >= b.watermarks.hard {
        return BackpressureHard
    }
    if size >= b.watermarks.soft {
        return BackpressureSoft
    }
    return BackpressureNone
}
```

### 3.6 Delivery Manager

**Purpose:** Deliver matched events to Gateway nodes.

**Interface:**

```go
type DeliveryManager interface {
    Deliver(ctx context.Context, gatewayID string, events []*MatchResult) error
}

type HTTPDeliveryManager struct {
    client  *http.Client
    baseURL string
    batcher *EventBatcher
}

func (d *HTTPDeliveryManager) Deliver(ctx context.Context, gatewayID string, events []*MatchResult) error {
    // Batch events
    batch := d.batcher.Add(gatewayID, events)
    if !batch.Ready() {
        return nil  // Wait for more events
    }

    // Send batch to Gateway
    payload, _ := json.Marshal(batch.Events)
    req, _ := http.NewRequestWithContext(ctx, "POST",
        fmt.Sprintf("%s/internal/v1/events", d.baseURL),
        bytes.NewBuffer(payload))

    resp, err := d.client.Do(req)
    if err != nil {
        return fmt.Errorf("delivery failed: %w", err)
    }
    defer resp.Body.Close()

    if resp.StatusCode != http.StatusOK {
        return fmt.Errorf("delivery failed with status: %d", resp.StatusCode)
    }

    return nil
}
```

**Event Batching:**

```go
type EventBatcher struct {
    batches   map[string]*Batch
    maxSize   int           // Max events per batch (default: 100)
    maxWait   time.Duration // Max wait time (default: 50ms)
}

type Batch struct {
    GatewayID string
    Events    []*MatchResult
    StartTime time.Time
}

func (b *EventBatcher) Ready() bool {
    return len(b.Events) >= b.maxSize || time.Since(b.StartTime) >= b.maxWait
}
```

### 3.7 Checkpoint & State Manager

**Purpose:** Persist resume tokens and manage partition state.

**Interface:**

```go
type CheckpointManager interface {
    Save(ctx context.Context, partition int, token bson.Raw) error
    Load(ctx context.Context, partition int) (bson.Raw, error)
    ListPartitions(ctx context.Context) ([]int, error)
}

type EtcdCheckpointManager struct {
    client *etcd.Client
    prefix string  // "/syntrix/streamer/checkpoints/"
}

func (c *EtcdCheckpointManager) Save(ctx context.Context, partition int, token bson.Raw) error {
    key := fmt.Sprintf("%s/partition-%d", c.prefix, partition)
    _, err := c.client.Put(ctx, key, string(token))
    return err
}
```

**Partition State:**

```go
type PartitionState struct {
    ID          int
    ResumeToken bson.Raw
    Lag         time.Duration  // Time behind latest change
    EventRate   float64        // Events/sec
    Status      PartitionStatus
}

type PartitionStatus int

const (
    PartitionHealthy PartitionStatus = iota
    PartitionLagging
    PartitionStale
    PartitionFailed
)
```

## 4. Data Structures

### 4.1 Core Event Types

```go
// Raw event from MongoDB
type RawEvent struct {
    OperationType string
    FullDocument  bson.Raw
    DocumentKey   interface{}
    ClusterTime   primitive.Timestamp
    TxnNumber     *int64
}

// Normalized event for matching
type NormalizedEvent struct {
    Type        EventType
    Tenant      string
    Collection  string
    DocumentID  string
    FullDoc     map[string]interface{}
    UpdateDesc  *UpdateDescription
    ClusterTime primitive.Timestamp
    TxnNumber   *int64
}

// Match result
type MatchResult struct {
    SubscriptionID string
    GatewayID      string
    Event          *NormalizedEvent
    Seq            int64  // Per-partition sequence number
    PartitionID    int
}

// Event delivered to Gateway
type StreamerEvent struct {
    SubscriptionID string          `json:"subscriptionId"`
    Seq            int64           `json:"seq"`
    PartitionID    int             `json:"partitionId"`
    Event          *StorageEvent   `json:"event"`
}
```

## 5. Threading Model

### 5.1 Goroutine Structure

```go
func (s *Streamer) Start(ctx context.Context) error {
    // 1. Start Control Plane API (HTTP server)
    go s.controlPlane.Serve(ctx)

    // 2. Start TTL expiry loop
    go s.indexManager.StartExpiryLoop(ctx)

    // 3. Start ingestion per partition (default: 1 partition = 1 MongoDB change stream)
    for partID := 0; partID < s.numPartitions; partID++ {
        go s.startPartitionIngestion(ctx, partID)
    }

    // 4. Start Bloom filter rebuild loop
    go s.startBloomRebuildLoop(ctx)

    // 5. Start backpressure monitoring
    go s.routingEngine.MonitorBackpressure(ctx)

    <-ctx.Done()
    return s.Shutdown()
}

func (s *Streamer) startPartitionIngestion(ctx context.Context, partID int) {
    consumer := s.createConsumer(partID)
    events, err := consumer.Start(ctx)
    if err != nil {
        log.Fatalf("Failed to start consumer for partition %d: %v", partID, err)
    }

    for {
        select {
        case <-ctx.Done():
            return
        case evt := <-events:
            // Normalize → Dedupe → Match → Route → Deliver
            s.processEvent(ctx, partID, evt)
        }
    }
}
```

## 6. Configuration

```yaml
streamer:
  # Server
  port: 8083

  # Partitioning
  # Start with 1 partition (1 MongoDB change stream) for simplicity
  # Scale to 4-8 partitions when throughput exceeds 5k events/sec
  # Each partition = 1 MongoDB change stream connection
  partitions: 1

  # Subscription Index
  subscription:
    ttl: 300s              # Default subscription TTL
    expiry_check: 10s      # How often to check for expired subs

  # Bloom Filter
  bloom:
    fp_rate: 0.01          # False positive rate
    bits_per_key: 10
    hash_funcs: 7
    rebuild_interval: 24h

  # Deduplication
  dedup:
    cache_size: 10000      # Number of entries
    ttl: 3m                # Cache TTL

  # Batching
  batch:
    max_size: 100          # Max events per batch
    max_wait: 50ms         # Max wait before sending partial batch

  # Backpressure
  backpressure:
    soft_watermark: 70     # Percentage
    hard_watermark: 90     # Percentage
    queue_size: 10000      # Per-gateway queue size

  # Checkpointing
  checkpoint:
    backend: etcd          # etcd or mongo
    etcd_endpoints:
      - http://localhost:2379
    save_interval: 5s
```

## 7. Error Handling

### 7.1 Failure Modes

| Failure | Detection | Recovery |
|---------|-----------|----------|
| MongoDB connection lost | Consumer error | Retry with exponential backoff |
| Gateway unreachable | Delivery timeout | Buffer events, retry, DLQ after 3 attempts |
| Bloom FP rate too high | Periodic sampling | Trigger rebuild |
| Partition lagging | Lag monitoring | Pause, catch up, resume |
| Checkpoint save failure | Save error | Log error, continue (rely on last successful checkpoint) |

### 7.2 Graceful Degradation

```go
func (s *Streamer) handleMatchTimeout(evt *NormalizedEvent) []*MatchResult {
    // If matching takes > 100ms, fall back to broadcast
    ctx, cancel := context.WithTimeout(context.Background(), 100*time.Millisecond)
    defer cancel()

    matches, err := s.matchingEngine.Match(ctx, evt)
    if err == context.DeadlineExceeded {
        log.Warn("Matching timeout, falling back to broadcast")
        return s.broadcastFallback(evt)
    }
    return matches
}
```

## 8. Next Steps

- **[002.subscription.md](002.subscription.md)** - Subscription lifecycle details
- **[003.matching.md](003.matching.md)** - Bloom filter and CEL matching algorithms
- **[004.routing.md](004.routing.md)** - Routing and partitioning strategies
- **[005.reliability.md](005.reliability.md)** - Fault tolerance and recovery
