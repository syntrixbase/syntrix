# Observability

**Date:** December 29, 2025
**Status:** Design Draft
**Component:** Streamer Observability & Monitoring

## 1. Overview

This document describes the observability strategy for the Streamer service, including metrics, logging, tracing, and operational dashboards.

## 2. Observability Goals

- **Visibility**: Understand system behavior in production
- **Debugging**: Quickly identify and diagnose issues
- **Alerting**: Proactive notification of problems
- **Capacity Planning**: Track resource usage and growth
- **SLO Monitoring**: Track compliance with service level objectives

## 3. Metrics

### 3.1 Metric Categories

```go
type StreamerMetrics struct {
    // Event Ingestion
    Ingestion IngestionMetrics

    // Subscription Management
    Subscriptions SubscriptionMetrics

    // Event Matching
    Matching MatchingMetrics

    // Routing & Delivery
    Routing RoutingMetrics

    // Reliability
    Reliability ReliabilityMetrics

    // System Resources
    System SystemMetrics
}
```

### 3.2 Ingestion Metrics

```go
type IngestionMetrics struct {
    // Events
    EventsIngested        prometheus.Counter
    EventsProcessed       prometheus.Counter
    EventsDropped         prometheus.Counter

    // Latency
    IngestionLatency      prometheus.Histogram  // Time from MongoDB to normalization

    // By Partition
    EventsByPartition     *prometheus.CounterVec  // Labels: partition
    PartitionLag          *prometheus.GaugeVec    // Labels: partition

    // Change Stream
    ChangeStreamErrors    prometheus.Counter
    ChangeStreamRestarts  prometheus.Counter

    // Deduplication
    DuplicateEvents       prometheus.Counter
    DedupCacheSize        prometheus.Gauge
}
```

**Key Metrics:**
- `streamer_events_ingested_total` - Total events received from MongoDB
- `streamer_ingestion_latency_seconds` - p50, p95, p99 ingestion latency
- `streamer_partition_lag_seconds{partition="0"}` - Current lag for partition 0

### 3.3 Subscription Metrics

```go
type SubscriptionMetrics struct {
    // Lifecycle
    SubscriptionsRegistered   prometheus.Counter
    SubscriptionsUnregistered prometheus.Counter
    SubscriptionsExpired      prometheus.Counter
    SubscriptionsActive       prometheus.Gauge

    // By Dimension
    SubscriptionsByCollection *prometheus.GaugeVec  // Labels: collection
    SubscriptionsByGateway    *prometheus.GaugeVec  // Labels: gatewayId
    SubscriptionsByTenant     *prometheus.GaugeVec  // Labels: tenant

    // Operations
    RegistrationLatency       prometheus.Histogram
    RenewalLatency            prometheus.Histogram

    // Errors
    RegistrationErrors        prometheus.Counter
    QuotaExceededErrors       *prometheus.CounterVec  // Labels: type (gateway, tenant, global)

    // Index
    IndexSize                 prometheus.Gauge
    IndexRebuildDuration      prometheus.Histogram
}
```

**Key Metrics:**
- `streamer_subscriptions_active` - Current active subscriptions
- `streamer_subscriptions_by_collection{collection="messages"}` - Subscriptions per collection
- `streamer_registration_latency_seconds` - Subscription registration time

### 3.4 Matching Metrics

```go
type MatchingMetrics struct {
    // Bloom Filter
    BloomFilterHits           prometheus.Counter
    BloomFilterMisses         prometheus.Counter
    BloomFilterFPRate         prometheus.Gauge
    BloomFilterCandidates     prometheus.Histogram
    BloomFilterLatency        prometheus.Histogram

    // CEL Evaluation
    CELEvaluations            prometheus.Counter
    CELEvaluationLatency      prometheus.Histogram
    CELEvaluationErrors       prometheus.Counter
    CELCacheHits              prometheus.Counter
    CELCacheMisses            prometheus.Counter

    // Overall Matching
    TotalMatchLatency         prometheus.Histogram
    MatchesFound              prometheus.Histogram
    MatchRate                 prometheus.Gauge  // Matches per second
}
```

**Key Metrics:**
- `streamer_match_latency_seconds` - p99 matching latency (target: < 10ms)
- `streamer_bloom_fp_rate` - False positive rate (target: < 1%)
- `streamer_cel_cache_hit_rate` - CEL program cache efficiency

### 3.5 Routing & Delivery Metrics

```go
type RoutingMetrics struct {
    // Routing
    EventsRouted              prometheus.Counter
    RoutingLatency            prometheus.Histogram

    // Batching
    BatchesCreated            prometheus.Counter
    BatchSize                 prometheus.Histogram
    BatchWaitTime             prometheus.Histogram

    // Delivery
    DeliveriesAttempted       prometheus.Counter
    DeliveriesSucceeded       prometheus.Counter
    DeliveriesFailed          prometheus.Counter
    DeliveryLatency           prometheus.Histogram
    DeliveryRetries           prometheus.Counter

    // By Gateway
    DeliveriesByGateway       *prometheus.CounterVec  // Labels: gatewayId, status
    DeliveryLatencyByGateway  *prometheus.HistogramVec // Labels: gatewayId

    // Backpressure
    BackpressureEvents        *prometheus.CounterVec  // Labels: gatewayId, level
    GatewayQueueSize          *prometheus.GaugeVec    // Labels: gatewayId

    // DLQ
    DLQEvents                 prometheus.Counter
    DLQRetries                prometheus.Counter
}
```

**Key Metrics:**
- `streamer_delivery_latency_seconds` - End-to-end delivery time
- `streamer_backpressure_events_total{gatewayId="gw-1",level="hard"}` - Backpressure incidents
- `streamer_dlq_events_total` - Events sent to dead letter queue

### 3.6 Reliability Metrics

```go
type ReliabilityMetrics struct {
    // Checkpoints
    CheckpointsSaved          prometheus.Counter
    CheckpointSaveLatency     prometheus.Histogram
    CheckpointSaveErrors      prometheus.Counter
    LastCheckpointTime        *prometheus.GaugeVec  // Labels: partition

    // Gap Detection
    GapsDetected              prometheus.Counter
    RescanEventsProcessed     prometheus.Counter

    // Partition Health
    PartitionHealth           *prometheus.GaugeVec  // Labels: partition, status
    PartitionRestarts         prometheus.Counter

    // Circuit Breaker
    CircuitBreakerState       *prometheus.GaugeVec  // Labels: target
    CircuitBreakerTrips       prometheus.Counter
}
```

**Key Metrics:**
- `streamer_checkpoint_save_errors_total` - Failed checkpoint saves (alert if > 0)
- `streamer_gaps_detected_total` - Data gaps detected (alert on any occurrence)
- `streamer_partition_health{partition="0",status="healthy"}` - Per-partition health

### 3.7 System Metrics

```go
type SystemMetrics struct {
    // Go Runtime
    GoroutineCount            prometheus.Gauge
    HeapAllocBytes            prometheus.Gauge
    HeapInUseBytes            prometheus.Gauge
    GCPauseSeconds            prometheus.Histogram

    // Process
    CPUUsagePercent           prometheus.Gauge
    MemoryUsageBytes          prometheus.Gauge
    OpenFileDescriptors       prometheus.Gauge

    // Network
    NetworkBytesReceived      prometheus.Counter
    NetworkBytesSent          prometheus.Counter
}
```

## 4. Logging

### 4.1 Log Levels

```go
type LogLevel int

const (
    LogLevelDebug LogLevel = iota
    LogLevelInfo
    LogLevelWarn
    LogLevelError
    LogLevelFatal
)
```

### 4.2 Structured Logging

```go
type StructuredLogger struct {
    logger *zap.Logger
}

func (l *StructuredLogger) Info(msg string, fields ...zap.Field) {
    l.logger.Info(msg, fields...)
}

// Example usage
logger.Info("Event matched",
    zap.String("subscriptionId", sub.ID),
    zap.String("collection", evt.Collection),
    zap.Int("matches", len(matches)),
    zap.Duration("latency", matchLatency),
)
```

### 4.3 Log Categories

**Ingestion Logs:**
```go
log.Info("Partition consumer started",
    zap.Int("partition", partID),
    zap.String("resumeToken", fmt.Sprintf("%x", token)),
)

log.Error("Change stream error",
    zap.Int("partition", partID),
    zap.Error(err),
    zap.Int("retryAttempt", attempt),
)
```

**Subscription Logs:**
```go
log.Info("Subscription registered",
    zap.String("subscriptionId", sub.ID),
    zap.String("gatewayId", sub.GatewayID),
    zap.String("collection", sub.Collection),
    zap.Time("expiresAt", sub.ExpiresAt),
)

log.Warn("Subscription expired",
    zap.String("subscriptionId", sub.ID),
    zap.Duration("age", time.Since(sub.CreatedAt)),
)
```

**Matching Logs:**
```go
log.Debug("Event matched",
    zap.String("eventId", evt.DocumentID),
    zap.Int("candidateCount", len(candidates)),
    zap.Int("matchCount", len(matches)),
    zap.Duration("bloomLatency", bloomLatency),
    zap.Duration("celLatency", celLatency),
)
```

**Delivery Logs:**
```go
log.Info("Batch delivered",
    zap.String("gatewayId", batch.GatewayID),
    zap.Int("eventCount", len(batch.Events)),
    zap.Duration("latency", deliveryLatency),
)

log.Error("Delivery failed",
    zap.String("gatewayId", batch.GatewayID),
    zap.Error(err),
    zap.Int("retryAttempt", attempt),
)
```

### 4.4 Log Sampling

To avoid overwhelming logs under high load:

```go
// Sample logs (1 out of 100 for debug level)
logger := zap.NewProduction(zap.WrapCore(func(core zapcore.Core) zapcore.Core {
    return zapcore.NewSamplerWithOptions(core,
        time.Second,
        100,  // First 100 per second
        10,   // Then 10 per second
    )
}))
```

## 5. Distributed Tracing

### 5.1 Trace Spans

```go
import "go.opentelemetry.io/otel"

func (s *Streamer) processEvent(ctx context.Context, partition int, evt *storage.Event) {
    // Start span
    ctx, span := otel.Tracer("streamer").Start(ctx, "processEvent")
    defer span.End()

    span.SetAttributes(
        attribute.Int("partition", partition),
        attribute.String("collection", evt.Collection),
        attribute.String("documentId", evt.DocumentKey),
    )

    // Child spans
    matches := s.matchEvent(ctx, evt)
    s.routeEvent(ctx, matches)
}

func (s *Streamer) matchEvent(ctx context.Context, evt *storage.Event) []*MatchResult {
    ctx, span := otel.Tracer("streamer").Start(ctx, "matchEvent")
    defer span.End()

    // Bloom filter phase
    candidates := s.bloomFilterPhase(ctx, evt)
    span.SetAttributes(attribute.Int("candidates", len(candidates)))

    // CEL evaluation phase
    matches := s.celEvaluationPhase(ctx, evt, candidates)
    span.SetAttributes(attribute.Int("matches", len(matches)))

    return matches
}
```

### 5.2 Trace Propagation

```go
// Inject trace context into delivery request
func (d *HTTPDelivery) Deliver(ctx context.Context, batch *DeliveryBatch) error {
    req, _ := http.NewRequestWithContext(ctx, "POST", endpoint, payload)

    // Propagate trace context
    otel.GetTextMapPropagator().Inject(ctx, propagation.HeaderCarrier(req.Header))

    resp, err := d.client.Do(req)
    // ...
}
```

### 5.3 Example Trace

```
Trace: Event Processing (200ms)
├─ ingestEvent (5ms)
│  └─ deduplicateEvent (1ms)
├─ matchEvent (10ms)
│  ├─ bloomFilterPhase (1ms)
│  └─ celEvaluationPhase (9ms)
├─ routeEvent (5ms)
│  └─ batchEvents (2ms)
└─ deliverEvent (180ms)
   ├─ serializePayload (10ms)
   └─ httpPost (170ms)
      ├─ networkLatency (150ms)
      └─ gatewayProcessing (20ms)
```

## 6. Health Checks

### 6.1 Liveness Probe

```go
// GET /health/live
func (s *Streamer) Liveness() HealthStatus {
    return HealthStatus{
        Status: "alive",
        Timestamp: time.Now(),
    }
}
```

### 6.2 Readiness Probe

```go
// GET /health/ready
func (s *Streamer) Readiness() HealthStatus {
    status := "ready"

    // Check MongoDB connection
    if err := s.mongoPing(); err != nil {
        status = "not_ready"
    }

    // Check etcd connection
    if err := s.etcdPing(); err != nil {
        status = "not_ready"
    }

    // Check partition health
    unhealthyPartitions := s.countUnhealthyPartitions()
    if unhealthyPartitions > s.numPartitions/2 {
        status = "not_ready"
    }

    return HealthStatus{
        Status: status,
        Checks: map[string]string{
            "mongodb": "ok",
            "etcd": "ok",
            "partitions": fmt.Sprintf("%d/%d healthy", s.numPartitions-unhealthyPartitions, s.numPartitions),
        },
    }
}
```

### 6.3 Detailed Health

```go
// GET /health
func (s *Streamer) Health() HealthStatus {
    return HealthStatus{
        Status: "healthy",
        Uptime: time.Since(s.startTime),
        Version: s.version,
        Partitions: []PartitionHealth{
            {ID: 0, Status: "healthy", Lag: 100 * time.Millisecond},
            {ID: 1, Status: "healthy", Lag: 50 * time.Millisecond},
            // ...
        },
        Subscriptions: SubscriptionHealth{
            Total: 10000,
            Active: 9500,
            Expired: 500,
        },
        Resources: ResourceHealth{
            CPUPercent: 45.2,
            MemoryMB: 2048,
            Goroutines: 1234,
        },
    }
}
```

## 7. Alerting Rules

### 7.1 Critical Alerts

```yaml
groups:
  - name: streamer_critical
    rules:
      # Event Processing Stopped
      - alert: StreamerEventProcessingStopped
        expr: rate(streamer_events_ingested_total[5m]) == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Streamer has stopped processing events"
          description: "No events ingested in the last 5 minutes"

      # High Error Rate
      - alert: StreamerHighErrorRate
        expr: rate(streamer_delivery_failures_total[5m]) > 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High delivery error rate"
          description: "{{ $value }} delivery failures/sec"

      # Checkpoint Failures
      - alert: StreamerCheckpointFailures
        expr: increase(streamer_checkpoint_save_errors_total[5m]) > 0
        labels:
          severity: critical
        annotations:
          summary: "Checkpoint saves failing"
          description: "Data loss risk: checkpoints cannot be saved"

      # Partition Unhealthy
      - alert: StreamerPartitionUnhealthy
        expr: streamer_partition_health{status!="healthy"} > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Partition {{ $labels.partition }} unhealthy"
```

### 7.2 Warning Alerts

```yaml
  - name: streamer_warning
    rules:
      # High Latency
      - alert: StreamerHighLatency
        expr: histogram_quantile(0.99, rate(streamer_match_latency_seconds_bucket[5m])) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High matching latency"
          description: "p99 latency: {{ $value }}s (target: < 0.01s)"

      # Bloom Filter Degradation
      - alert: StreamerBloomFilterDegraded
        expr: streamer_bloom_fp_rate > 0.02
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Bloom filter false positive rate high"
          description: "FP rate: {{ $value }} (target: < 0.01)"

      # Backpressure
      - alert: StreamerBackpressure
        expr: rate(streamer_backpressure_events_total{level="hard"}[5m]) > 0
        labels:
          severity: warning
        annotations:
          summary: "Gateway {{ $labels.gatewayId }} under hard backpressure"

      # DLQ Growing
      - alert: StreamerDLQGrowing
        expr: increase(streamer_dlq_events_total[1h]) > 100
        labels:
          severity: warning
        annotations:
          summary: "Dead letter queue growing"
          description: "{{ $value }} events in DLQ in the last hour"
```

## 8. Dashboards

### 8.1 Overview Dashboard

**Panels:**
1. Event Throughput (events/sec)
2. Active Subscriptions (gauge)
3. Match Latency (p50, p95, p99)
4. Delivery Success Rate (%)
5. Partition Health (status grid)
6. Error Rate (errors/sec)

### 8.2 Performance Dashboard

**Panels:**
1. Ingestion Latency Distribution
2. Bloom Filter Hit Rate
3. CEL Evaluation Latency
4. Batch Size Distribution
5. Delivery Latency by Gateway
6. Backpressure Events

### 8.3 Reliability Dashboard

**Panels:**
1. Partition Lag (by partition)
2. Checkpoint Save Success Rate
3. Gap Detection Events
4. Duplicate Events Filtered
5. Circuit Breaker State
6. Partition Restart Count

### 8.4 Resource Dashboard

**Panels:**
1. CPU Usage (%)
2. Memory Usage (MB)
3. Goroutine Count
4. GC Pause Time
5. Network I/O (bytes/sec)
6. Open File Descriptors

## 9. SLO Tracking

### 9.1 Service Level Objectives

```yaml
SLOs:
  # Availability
  - name: availability
    target: 99.9%
    measurement: uptime / total_time

  # Latency
  - name: match_latency_p99
    target: 10ms
    measurement: p99(streamer_match_latency_seconds)

  # Throughput
  - name: event_throughput
    target: 10000 events/sec
    measurement: rate(streamer_events_processed_total[1m])

  # Error Rate
  - name: delivery_success_rate
    target: 99.9%
    measurement: (successes / total_deliveries) * 100
```

### 9.2 SLO Monitoring

```go
type SLOMonitor struct {
    targets map[string]float64
    metrics *StreamerMetrics
}

func (m *SLOMonitor) CheckSLO(name string) SLOStatus {
    switch name {
    case "availability":
        uptime := m.calculateUptime()
        return SLOStatus{
            Name: "availability",
            Target: 99.9,
            Actual: uptime,
            Status: m.status(uptime, 99.9),
        }

    case "match_latency_p99":
        p99 := m.getP99MatchLatency()
        return SLOStatus{
            Name: "match_latency_p99",
            Target: 10.0,  // ms
            Actual: p99,
            Status: m.status(p99, 10.0),
        }

    // ...
    }
}

func (m *SLOMonitor) status(actual, target float64) string {
    if actual >= target {
        return "met"
    }
    if actual >= target*0.95 {
        return "at_risk"
    }
    return "violated"
}
```

## 10. Operational Runbooks

### 10.1 High Latency

**Symptoms:** p99 match latency > 50ms

**Investigation:**
1. Check Bloom filter false positive rate
2. Check CEL evaluation latency
3. Check CPU usage
4. Check active subscription count

**Remediation:**
1. Rebuild Bloom filters
2. Restart nodes (refresh CEL cache)
3. Scale up (add more nodes)
4. Review complex subscriptions

### 10.2 Delivery Failures

**Symptoms:** High delivery error rate

**Investigation:**
1. Check Gateway health
2. Check network connectivity
3. Check backpressure metrics
4. Check DLQ size

**Remediation:**
1. Restart affected Gateway
2. Increase delivery timeout
3. Enable circuit breaker
4. Process DLQ manually

### 10.3 Checkpoint Failures

**Symptoms:** Checkpoint save errors

**Investigation:**
1. Check etcd/MongoDB health
2. Check network connectivity
3. Check disk space
4. Check permissions

**Remediation:**
1. Restart etcd/MongoDB
2. Clear old checkpoints
3. Increase storage capacity
4. Fix permissions

## 11. Configuration

```yaml
observability:
  # Metrics
  metrics:
    enabled: true
    port: 9090
    path: /metrics

  # Logging
  logging:
    level: info  # debug, info, warn, error
    format: json  # json, text
    sampling:
      enabled: true
      initial: 100
      thereafter: 10

  # Tracing
  tracing:
    enabled: true
    endpoint: http://jaeger:14268/api/traces
    sample_rate: 0.1  # 10% sampling

  # Health
  health:
    port: 8080
    liveness_path: /health/live
    readiness_path: /health/ready
    detailed_path: /health
```

## 12. Best Practices

1. **Metric Cardinality**: Limit high-cardinality labels (e.g., subscriptionId)
2. **Log Sampling**: Sample debug logs under high load
3. **Trace Sampling**: Use adaptive sampling (more for errors)
4. **Dashboard Organization**: Separate overview, performance, and debug dashboards
5. **Alert Fatigue**: Set appropriate thresholds and for durations
6. **SLO Reviews**: Review SLO compliance weekly
