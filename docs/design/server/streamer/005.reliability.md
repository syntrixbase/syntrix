# Reliability & Fault Tolerance

**Date:** December 29, 2025
**Status:** Design Draft
**Component:** Streamer Reliability

## 1. Overview

This document describes the reliability mechanisms in Streamer to ensure **at-least-once delivery**, fault tolerance, and graceful recovery from failures.

## 2. Reliability Goals

### 2.1 Guarantees

- **At-least-once delivery**: Every event is delivered to matched subscriptions (duplicates possible)
- **No data loss**: Resume from last checkpoint after crash
- **Partition tolerance**: Handle network splits and node failures
- **Graceful degradation**: Continue operating under partial failures

### 2.2 Non-Goals

- ❌ **Exactly-once delivery** (too expensive, client must deduplicate)
- ❌ **Ordering across partitions** (only within-partition ordering)
- ❌ **Zero downtime rebalancing** (brief interruption acceptable)

## 3. Resume Token Management

### 3.1 What is a Resume Token?

MongoDB change streams provide **resume tokens** - opaque values that represent a point in time in the oplog. Streamer can resume from a token after a crash without missing or duplicating events.

```go
type ResumeToken struct {
    Token     bson.Raw      // Opaque MongoDB token
    Timestamp primitive.Timestamp
}
```

### 3.2 Checkpoint Storage

**Interface:**

```go
type CheckpointStore interface {
    Save(ctx context.Context, partition int, token ResumeToken) error
    Load(ctx context.Context, partition int) (*ResumeToken, error)
    List(ctx context.Context) (map[int]*ResumeToken, error)
    Delete(ctx context.Context, partition int) error
}
```

**etcd Implementation:**

```go
type EtcdCheckpointStore struct {
    client *etcd.Client
    prefix string  // "/syntrix/streamer/checkpoints/"
}

func (s *EtcdCheckpointStore) Save(ctx context.Context, partition int, token ResumeToken) error {
    key := fmt.Sprintf("%spartition-%d", s.prefix, partition)
    data, err := bson.Marshal(token)
    if err != nil {
        return err
    }

    _, err = s.client.Put(ctx, key, string(data))
    return err
}

func (s *EtcdCheckpointStore) Load(ctx context.Context, partition int) (*ResumeToken, error) {
    key := fmt.Sprintf("%spartition-%d", s.prefix, partition)
    resp, err := s.client.Get(ctx, key)
    if err != nil {
        return nil, err
    }

    if len(resp.Kvs) == 0 {
        return nil, ErrCheckpointNotFound
    }

    var token ResumeToken
    if err := bson.Unmarshal([]byte(resp.Kvs[0].Value), &token); err != nil {
        return nil, err
    }

    return &token, nil
}
```

**MongoDB Implementation (Alternative):**

```go
type MongoCheckpointStore struct {
    collection *mongo.Collection
}

func (s *MongoCheckpointStore) Save(ctx context.Context, partition int, token ResumeToken) error {
    filter := bson.M{"_id": fmt.Sprintf("partition-%d", partition)}
    update := bson.M{
        "$set": bson.M{
            "token":      token.Token,
            "timestamp":  token.Timestamp,
            "updated_at": time.Now(),
        },
    }

    opts := options.Update().SetUpsert(true)
    _, err := s.collection.UpdateOne(ctx, filter, update, opts)
    return err
}
```

### 3.3 Checkpoint Frequency

```go
type CheckpointPolicy struct {
    Interval  time.Duration  // Time-based (default: 5s)
    EventCount int           // Event-based (default: 1000)
}

type CheckpointManager struct {
    store      CheckpointStore
    policy     CheckpointPolicy
    lastSaved  map[int]time.Time
    eventCount map[int]int
}

func (cm *CheckpointManager) ShouldCheckpoint(partition int) bool {
    // Check time-based trigger
    if time.Since(cm.lastSaved[partition]) >= cm.policy.Interval {
        return true
    }

    // Check event-based trigger
    if cm.eventCount[partition] >= cm.policy.EventCount {
        return true
    }

    return false
}

func (cm *CheckpointManager) Checkpoint(ctx context.Context, partition int, token ResumeToken) error {
    if err := cm.store.Save(ctx, partition, token); err != nil {
        return err
    }

    // Reset counters
    cm.lastSaved[partition] = time.Now()
    cm.eventCount[partition] = 0

    return nil
}
```

## 4. Recovery & Restart

### 4.1 Startup Recovery

```go
func (s *Streamer) Start(ctx context.Context) error {
    // Load all partition checkpoints
    checkpoints, err := s.checkpointStore.List(ctx)
    if err != nil {
        return fmt.Errorf("failed to load checkpoints: %w", err)
    }

    // Start each partition consumer
    for partID := 0; partID < s.numPartitions; partID++ {
        checkpoint := checkpoints[partID]

        go s.startPartitionConsumer(ctx, partID, checkpoint)
    }

    return nil
}

func (s *Streamer) startPartitionConsumer(ctx context.Context, partID int, checkpoint *ResumeToken) {
    log.Infof("Starting partition %d consumer (checkpoint: %v)", partID, checkpoint)

    var resumeToken bson.Raw
    if checkpoint != nil {
        resumeToken = checkpoint.Token
    }

    consumer, err := s.createConsumer(partID, resumeToken)
    if err != nil {
        log.Fatalf("Failed to create consumer for partition %d: %v", partID, err)
    }

    stream, err := consumer.Watch(ctx)
    if err != nil {
        log.Fatalf("Failed to start watch for partition %d: %v", partID, err)
    }

    // Process events
    for {
        select {
        case <-ctx.Done():
            return
        case evt, ok := <-stream:
            if !ok {
                log.Warnf("Stream closed for partition %d, restarting", partID)
                // Restart consumer
                time.Sleep(1 * time.Second)
                s.startPartitionConsumer(ctx, partID, checkpoint)
                return
            }

            s.processEvent(ctx, partID, evt)

            // Checkpoint periodically
            if s.checkpointManager.ShouldCheckpoint(partID) {
                s.checkpointManager.Checkpoint(ctx, partID, ResumeToken{Token: evt.ResumeToken})
            }
        }
    }
}
```

### 4.2 Gap Detection

```go
type GapDetector struct {
    lastTimestamp map[int]primitive.Timestamp
    maxGap        time.Duration  // Default: 5 minutes
}

func (gd *GapDetector) Check(partition int, evt *NormalizedEvent) bool {
    last := gd.lastTimestamp[partition]
    current := evt.ClusterTime

    if last.T == 0 {
        // First event
        gd.lastTimestamp[partition] = current
        return false
    }

    // Calculate gap
    gap := time.Duration(current.T-last.T) * time.Second

    if gap > gd.maxGap {
        log.Errorf("Gap detected in partition %d: %v (last=%v, current=%v)",
            partition, gap, last, current)
        return true
    }

    gd.lastTimestamp[partition] = current
    return false
}
```

### 4.3 Gap Recovery

When a gap is detected:

```go
func (s *Streamer) handleGap(partition int, last, current primitive.Timestamp) {
    log.Warnf("Handling gap in partition %d: %v → %v", partition, last, current)

    // Strategy 1: Trigger rescan (if gap is small)
    if s.isSmallGap(last, current) {
        s.rescanPartition(partition, last, current)
        return
    }

    // Strategy 2: Mark partition as stale, require client resync
    s.markPartitionStale(partition)

    // Notify Gateways to resync subscriptions
    s.notifyGatewaysResync(partition)
}

func (s *Streamer) rescanPartition(partition int, start, end primitive.Timestamp) {
    log.Infof("Rescanning partition %d from %v to %v", partition, start, end)

    // Query historical changes
    filter := bson.M{
        "clusterTime": bson.M{
            "$gte": start,
            "$lte": end,
        },
    }

    cursor, err := s.docStore.Find(context.Background(), filter)
    if err != nil {
        log.Errorf("Rescan failed: %v", err)
        return
    }

    // Replay events
    for cursor.Next(context.Background()) {
        var doc storage.Document
        cursor.Decode(&doc)
        s.processDocument(&doc)
    }
}
```

## 5. Deduplication

### 5.1 Why Deduplicate?

Events can be duplicated due to:
- Resume token replays
- Partition rebalancing
- Network retries

### 5.2 Deduplication Cache

```go
type DedupCache struct {
    cache *lru.Cache
    ttl   time.Duration  // Default: 3 minutes
}

type DedupKey struct {
    ClusterTime primitive.Timestamp
    TxnNumber   *int64
    DocumentKey string
}

func (dc *DedupCache) IsDuplicate(key DedupKey) bool {
    cacheKey := dc.makeKey(key)

    if dc.cache.Contains(cacheKey) {
        return true
    }

    dc.cache.Add(cacheKey, time.Now())
    return false
}

func (dc *DedupCache) makeKey(key DedupKey) string {
    if key.TxnNumber != nil {
        return fmt.Sprintf("%d:%d:%s", key.ClusterTime.T, *key.TxnNumber, key.DocumentKey)
    }
    return fmt.Sprintf("%d::%s", key.ClusterTime.T, key.DocumentKey)
}
```

### 5.3 Deduplication in Pipeline

```go
func (s *Streamer) processEvent(ctx context.Context, partition int, evt *storage.Event) {
    // Check for duplicate
    dedupKey := DedupKey{
        ClusterTime: evt.ClusterTime,
        TxnNumber:   evt.TxnNumber,
        DocumentKey: evt.DocumentKey,
    }

    if s.dedupCache.IsDuplicate(dedupKey) {
        s.metrics.DuplicateEvents.Inc()
        log.Debugf("Duplicate event detected: %v", dedupKey)
        return
    }

    // Process normally
    s.matchAndRoute(ctx, partition, evt)
}
```

## 6. Partition Handoff

### 6.1 Handoff Protocol

When a partition moves from Node A to Node B:

```
Node A (Donor)                    Node B (Recipient)
      │                                  │
      │ 1. Receive STOP command          │
      ├──────────────────────────────────►
      │                                  │
      │ 2. Stop consuming new events     │
      │                                  │
      │ 3. Flush pending events          │
      │                                  │
      │ 4. Checkpoint final token        │
      │                                  │
      │ 5. Send token to recipient       │
      ├──────────────────────────────────►
      │                                  │
      │                                  │ 6. Start consuming from token
      │                                  │
      │ 7. Enter broadcast mode (1s)     │
      │◄──────────────────────────────────┤
      │                                  │
      │ 8. Exit after 1s                 │ 9. Normal operation
      │                                  │
```

### 6.2 Handoff Implementation

```go
func (s *Streamer) handoffPartition(partID int, recipient string) error {
    log.Infof("Handing off partition %d to %s", partID, recipient)

    // 1. Stop consuming
    s.stopPartition(partID)

    // 2. Flush pending events
    s.flushPartition(partID)

    // 3. Get final checkpoint
    checkpoint, err := s.checkpointManager.Load(context.Background(), partID)
    if err != nil {
        return fmt.Errorf("failed to load checkpoint: %w", err)
    }

    // 4. Transfer checkpoint to recipient
    if err := s.transferCheckpoint(recipient, partID, checkpoint); err != nil {
        return fmt.Errorf("failed to transfer checkpoint: %w", err)
    }

    // 5. Enter broadcast mode briefly
    s.enterBroadcastMode(partID, 1*time.Second)

    log.Infof("Handoff complete for partition %d", partID)
    return nil
}

func (s *Streamer) enterBroadcastMode(partID int, duration time.Duration) {
    log.Infof("Partition %d entering broadcast mode for %v", partID, duration)

    // Temporarily route all events to both old and new owner
    s.router.SetBroadcast(partID, true)

    time.AfterFunc(duration, func() {
        s.router.SetBroadcast(partID, false)
        log.Infof("Partition %d exiting broadcast mode", partID)
    })
}
```

## 7. Health Monitoring

### 7.1 Health Check

```go
type HealthStatus struct {
    Status     string              `json:"status"`  // healthy, degraded, unhealthy
    Partitions []PartitionHealth   `json:"partitions"`
    Uptime     time.Duration       `json:"uptime"`
    Version    string              `json:"version"`
}

type PartitionHealth struct {
    ID          int               `json:"id"`
    Status      PartitionStatus   `json:"status"`
    Lag         time.Duration     `json:"lag"`
    EventRate   float64           `json:"eventRate"`
    LastEvent   time.Time         `json:"lastEvent"`
}

type PartitionStatus string

const (
    PartitionHealthy  PartitionStatus = "healthy"
    PartitionLagging  PartitionStatus = "lagging"
    PartitionStale    PartitionStatus = "stale"
    PartitionFailed   PartitionStatus = "failed"
)

func (s *Streamer) Health() *HealthStatus {
    status := &HealthStatus{
        Status:     "healthy",
        Partitions: make([]PartitionHealth, s.numPartitions),
        Uptime:     time.Since(s.startTime),
        Version:    s.version,
    }

    unhealthyCount := 0

    for i := 0; i < s.numPartitions; i++ {
        partHealth := s.getPartitionHealth(i)
        status.Partitions[i] = partHealth

        if partHealth.Status != PartitionHealthy {
            unhealthyCount++
        }
    }

    // Overall status
    if unhealthyCount > 0 {
        if unhealthyCount >= s.numPartitions/2 {
            status.Status = "unhealthy"
        } else {
            status.Status = "degraded"
        }
    }

    return status
}
```

### 7.2 Lag Monitoring

```go
type LagMonitor struct {
    partitionLag map[int]time.Duration
    threshold    time.Duration  // Default: 30s
}

func (lm *LagMonitor) Update(partition int, eventTime time.Time) {
    lag := time.Since(eventTime)
    lm.partitionLag[partition] = lag

    if lag > lm.threshold {
        log.Warnf("Partition %d lagging: %v", partition, lag)
    }
}

func (lm *LagMonitor) IsLagging(partition int) bool {
    return lm.partitionLag[partition] > lm.threshold
}
```

## 8. Circuit Breaker

Protect against cascading failures:

```go
type CircuitBreaker struct {
    state         State
    failureCount  int
    threshold     int           // Failures before opening (default: 5)
    timeout       time.Duration // How long to stay open (default: 30s)
    lastFailTime  time.Time
}

type State int

const (
    StateClosed State = iota  // Normal operation
    StateOpen                 // Failing, reject requests
    StateHalfOpen            // Testing recovery
)

func (cb *CircuitBreaker) Call(fn func() error) error {
    if cb.state == StateOpen {
        if time.Since(cb.lastFailTime) > cb.timeout {
            cb.state = StateHalfOpen
        } else {
            return ErrCircuitOpen
        }
    }

    err := fn()

    if err != nil {
        cb.onFailure()
        return err
    }

    cb.onSuccess()
    return nil
}

func (cb *CircuitBreaker) onFailure() {
    cb.failureCount++
    cb.lastFailTime = time.Now()

    if cb.failureCount >= cb.threshold {
        cb.state = StateOpen
        log.Warnf("Circuit breaker opened after %d failures", cb.failureCount)
    }
}

func (cb *CircuitBreaker) onSuccess() {
    cb.failureCount = 0
    if cb.state == StateHalfOpen {
        cb.state = StateClosed
        log.Info("Circuit breaker closed, recovered")
    }
}
```

## 9. Graceful Shutdown

```go
func (s *Streamer) Shutdown(ctx context.Context) error {
    log.Info("Starting graceful shutdown...")

    // 1. Stop accepting new subscriptions
    s.controlPlane.StopAcceptingRequests()

    // 2. Stop consuming new events
    for i := 0; i < s.numPartitions; i++ {
        s.stopPartition(i)
    }

    // 3. Flush pending events (with timeout)
    flushCtx, cancel := context.WithTimeout(ctx, 10*time.Second)
    defer cancel()

    if err := s.flushAll(flushCtx); err != nil {
        log.Errorf("Failed to flush all events: %v", err)
    }

    // 4. Checkpoint all partitions
    for i := 0; i < s.numPartitions; i++ {
        checkpoint := s.getPartitionCheckpoint(i)
        if err := s.checkpointStore.Save(ctx, i, checkpoint); err != nil {
            log.Errorf("Failed to checkpoint partition %d: %v", i, err)
        }
    }

    // 5. Close connections
    s.closeConnections()

    log.Info("Graceful shutdown complete")
    return nil
}
```

## 10. Disaster Recovery

### 10.1 Backup Strategy

```bash
# Backup checkpoints (etcd)
etcdctl snapshot save /backup/syntrix-checkpoints-$(date +%Y%m%d).db

# Backup subscription state (optional, can rebuild from Gateways)
# Subscriptions are ephemeral and will re-register
```

### 10.2 Recovery Scenarios

**Scenario 1: Single Node Failure**
- Other nodes continue operating
- Control plane reassigns partitions
- Lost node's partitions resume from checkpoints

**Scenario 2: Total Cluster Failure**
- Restart all nodes
- Load checkpoints from etcd/DB
- Resume from last known position
- Some duplicates possible (deduplicated by cache)

**Scenario 3: Checkpoint Loss**
- Worst case: lose all checkpoints
- Options:
  - Start from "now" (lose historical events)
  - Trigger full resync from clients
  - Rescan MongoDB oplog (if within retention window)

## 11. Metrics

```go
type ReliabilityMetrics struct {
    // Checkpoints
    CheckpointsSaved      prometheus.Counter
    CheckpointErrors      prometheus.Counter
    CheckpointLatency     prometheus.Histogram

    // Deduplication
    DuplicateEvents       prometheus.Counter
    DedupCacheSize        prometheus.Gauge

    // Recovery
    PartitionRestarts     prometheus.Counter
    GapsDetected          prometheus.Counter
    RescanEvents          prometheus.Counter

    // Health
    PartitionLag          *prometheus.GaugeVec  // Labels: partition
    PartitionHealth       *prometheus.GaugeVec  // Labels: partition, status
}
```

## 12. Configuration

```yaml
reliability:
  # Checkpointing
  checkpoint:
    backend: etcd  # etcd, mongo
    interval: 5s
    event_count: 1000

  # Deduplication
  dedup:
    enabled: true
    cache_size: 10000
    ttl: 3m

  # Gap Detection
  gap_detection:
    enabled: true
    max_gap: 5m

  # Circuit Breaker
  circuit_breaker:
    threshold: 5
    timeout: 30s

  # Shutdown
  shutdown:
    flush_timeout: 10s
    checkpoint_timeout: 5s
```

## 13. Testing

### 13.1 Failure Injection Tests
- Node crash during processing
- Network partition
- MongoDB connection loss
- etcd unavailable

### 13.2 Recovery Tests
- Resume from checkpoint
- Gap recovery
- Partition handoff

### 13.3 Chaos Engineering
- Random node kills
- Network delays
- Resource exhaustion
