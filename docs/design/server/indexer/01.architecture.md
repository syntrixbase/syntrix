# Indexer Service Architecture

Date: December 26, 2025
Status: Draft
Scope: Internal layout and data flows for the Indexer service.

## Why
- Provide accelerated lookup/ordering for queries while keeping storage as the source of truth.
- Decouple index maintenance from Query Engine so they can evolve independently.
- Allow pluggable index backends (in-memory → RocksDB/external KV) without touching Query logic.

## Overview

The Indexer service maintains secondary indexes derived from storage data. It consumes change events from the Puller service via gRPC subscription and provides search capabilities to the Query Engine.

```
                 +-------------------+
                 |   Query Engine    |
                 +--------+----------+
                          |
                   Search(plan)
                          |
                          v
+----------------------------------------------------------+
|                        Indexer                           |
|  +----------------+    +----------------+    +---------+ |
|  | Event Buffer   |<-->|    Manager     |--->| Shards  | |
|  | (PebbleDB)     |    | (routing/match)|    | (btree) | |
|  +----------------+    +----------------+    +---------+ |
+-------------+--------------------------------------------+
              |
   gRPC Subscribe (progress marker)
              |
              v
      +---------------+
      |    Puller     |
      +-------+-------+
              |
              v
      +---------------+
      |    MongoDB    |
      +---------------+
```

**Subscription Model**: Indexer subscribes to Puller using the same gRPC `Subscribe` RPC as Streamer. It maintains a `progress marker` to track its position in the event stream, enabling seamless recovery after restarts.

**Key Design Decision**: Indexer maintains its own local event buffer (PebbleDB) instead of relying on Puller's buffer. This enables:
- Multiple concurrent index rebuilds sharing the same buffered events
- Independent lifecycle control (cleanup after rebuild completes)
- Isolation from Puller's retention policy (default 1 hour may be insufficient for large rebuilds)

## Package Layout

```
internal/indexer/
├── interface.go           - Service interface (Search, Health, Stats)
├── service.go             - Main service implementation
├── internal/
│   ├── manager/           - Index manager (template matching, shard routing)
│   ├── shard/             - Per-collection index shard (btree, ByID map)
│   ├── encoding/          - OrderKey encoding/decoding
│   ├── template/          - Index template loading and matching
│   └── rebuild/           - Rebuild orchestration and throttling
└── config/                - Indexer configuration
```

Note: Indexer directly imports `internal/puller/buffer` for its local event buffer. The buffer package is generic and supports multiple instances with different configurations (path, retention, etc.).

## Service Interface

```go
type Service interface {
    // Search returns document references matching the plan.
    // Indexer internally selects the best matching template based on
    // plan.OrderBy and plan.Filters using the Query-to-Index matching rules.
    Search(ctx context.Context, database string, plan Plan) ([]DocRef, error)

    // Health returns current health status
    Health(ctx context.Context) (Health, error)

    // Stats returns index statistics
    Stats(ctx context.Context) (Stats, error)
}

type Plan struct {
    Collection  string        // concrete collection path (e.g., "users/alice/chats")
    Filters     []Filter      // prefix/range filters
    OrderBy     []OrderField  // ordering specification
    Limit       int
    StartAfter  string        // cursor for pagination
}

type DocRef struct {
    ID       string   // document ID within collection
    OrderKey []byte   // encoded sort key for cursor
}
```

**Template Selection**: Indexer is responsible for selecting the best matching template based on `Plan.Collection`, `Plan.OrderBy`, and `Plan.Filters`. Query Engine does not need to know about template internals. See [Query-to-Index Matching Rules](02.index.md#query-to-index-matching-rules).

## Data Flows

### Puller Subscription

Indexer subscribes to Puller's gRPC `Subscribe` RPC as one of three consumers (alongside Streamer and Trigger). For the overall event distribution architecture, see [Puller Event Distribution](../01.architecture.md#puller-event-distribution).

**Indexer-Specific Subscription Details**:
- **Consumer ID**: `"indexer"` (for logging/monitoring)
- **Progress Marker**: Saved to `data/indexer/progress`; used to resume after restart
- **Local Buffer**: Indexer maintains its own PebbleDB buffer (4h retention) for rebuild catch-up

### Index Update Flow
```
Puller.Subscribe() stream
    │
    ▼
Indexer.processBatch(events)   // batch of events (up to 256 or 1MB)
    │
    ├─> Write batch to local buffer (PebbleDB)
    │       └─> Key: ClusterTime + EventID
    │
    ├─> For each event in batch:
    │       │
    │       ├─> Match templates for collection
    │       │
    │       └─> For each matched template:
    │               │
    │               ▼
    │           Shard.Upsert/Delete()
    │               │
    │               ├─> Encode OrderKey
    │               ├─> Update ByID map
    │               └─> Update btree
    │
    └─> Save progress marker to disk (after batch completes)
```

### Search Flow
```
Query Engine
    │
    ▼
Indexer.Search(database, plan)
    │
    ├─> Match collection path to templates
    │
    ├─> Select best template by Query-to-Index matching rules
    │
    ├─> Lookup shard by (database, collectionPattern, templateIdentity)
    │
    ├─> Build search bounds from plan
    │
    ├─> Iterate btree from startAfter
    │
    └─> Return DocRef list (ID + OrderKey)

    │
    ▼
Query Engine
    │
    └─> Batch fetch documents by ID from Storage
```

### Rebuild Flow
```
Admin trigger / Gap detection / Startup
    │
    ▼
Rebuild.Start(collection, template)
    │
    ├─> Record current buffer position (startKey)
    │
    ├─> Mark shard as "rebuilding" (queries fall back to storage)
    │
    ├─> Clear existing shard data
    │
    ├─> Scan storage with pagination:
    │       │
    │       └─> Upsert each document (throttled)
    │
    ├─> Replay buffered events from startKey:
    │       │
    │       └─> buffer.ScanFrom(startKey) → apply to shard
    │
    ├─> Mark shard as "healthy"
    │
    └─> Optionally delete old buffer entries (retention-based)
```

**Why local buffer instead of memory WAL?**
- Memory WAL has fixed capacity (e.g., 10000 events) - overflow causes data loss
- Large collection rebuilds may take hours; Puller's 1-hour retention is insufficient
- Multiple concurrent rebuilds can share the same buffered events (write once, read many)

### Recovery Scenarios

#### Indexer Restart

```
Indexer.Start()
    │
    ├─> Load saved progress marker from disk
    │
    ├─> Subscribe to Puller with progress marker
    │       │
    │       └─> Puller replays events from that position (catch-up mode)
    │
    ├─> Process events, update shards
    │
    └─> If gap detected (progress too old for Puller buffer):
            │
            └─> Trigger rebuild for affected shards
```

**Progress Marker Storage**: Saved to `data/indexer/progress` file, updated after each batch of events is processed and written to local buffer.

#### Puller Restart

```
Puller restarts
    │
    ▼
Indexer detects gRPC stream closed
    │
    ├─> Auto-reconnect with exponential backoff
    │
    └─> Resume subscription from last saved progress marker
        │
        └─> Puller's catch-up mode replays missed events
```

**No data loss**: Both Indexer and Puller maintain independent buffers; events are not lost as long as the gap is within retention periods.

#### Fallback Behavior During Rebuild

When a shard is in "rebuilding" state:
- Indexer returns `ErrShardRebuilding` with shard status to Query Engine
- Query Engine falls back to storage scan with 500 document limit
- API layer (Gateway) translates this to `X-Index-Status: rebuilding` HTTP header
- Metrics track fallback rate for monitoring

**Responsibility**: Indexer only returns status information; HTTP semantics are handled by the API layer.

## Index Templates

Index templates define which fields to index for which collections. For detailed template definition, pattern matching rules, and conflict handling, see [Index Implementation Details](02.index.md#index-templates).

### Template Definition (YAML)

```yaml
templates:
  - name: chats_by_timestamp
    collectionPattern: users/{uid}/chats
    fields:
      - { field: timestamp, order: desc }
    includeDeleted: true

  - name: messages_by_sender
    collectionPattern: rooms/{rid}/messages
    fields:
      - { field: senderId, order: asc }
      - { field: timestamp, order: desc }
    includeDeleted: true
```

## In-Memory Index Structure

```
Index Manager
    │
    └─> map[string]*Shard  (keyed by database|normalizedPattern|templateIdentity)
            │
            └─> Shard
                  ├── Database: "myapp"
                  ├── Pattern: "users/*/chats"
                  ├── TemplateID: "timestamp:desc"
                  ├── Mu: RWMutex
                  ├── ByID: map[string][]byte  (docID → OrderKey)
                  └── Tree: btree[OrderKey → DocRef]
```

**Multi-Database Isolation**: Each shard is scoped to a specific database. The same collection pattern in different databases results in separate shards, ensuring complete data isolation between databases.

For detailed shard structure, OrderKey encoding, and search execution, see [Index Implementation Details](02.index.md#in-memory-index-shape-default-backend).

## Event Buffer

Indexer maintains a local PebbleDB-based event buffer for rebuild catch-up by directly importing `internal/puller/buffer` with Indexer-specific configuration.

### Why Local Buffer?

| Concern | Shared Puller Buffer | Local Indexer Buffer |
|---------|---------------------|---------------------|
| Rebuild read pattern | 10 indexes = 10 scans | Write once, read many |
| Retention control | Puller's policy (1h default) | Indexer's policy (rebuild duration) |
| Isolation | Puller failure affects Indexer | Independent lifecycle |
| Storage path | `data/puller/events/` | `data/indexer/events/` |

### Buffer Operations

```go
// Write: called for every event from Puller
buffer.Write(evt *StoreChangeEvent, checkpoint bson.Raw)

// Read: used during rebuild catch-up
iter := buffer.ScanFrom(startKey)  // iterate from recorded position
for iter.Next() {
    evt := iter.Event()
    shard.Upsert(evt)
}

// Cleanup: retention-based or on-demand after rebuild
buffer.DeleteBefore(cutoffKey)
```

### Key Format

Same as Puller buffer: `{ClusterTime.T:10d}-{ClusterTime.I:10d}-{EventID}`

This ensures:
- Lexicographic order = temporal order
- Efficient range scans for rebuild catch-up
- Deduplication by EventID

## OrderKey Encoding

OrderKey is a byte sequence that enables lexicographic sorting for indexed fields. For detailed encoding specification, see [Index Implementation Details](02.index.md#orderkey-encoding).

**Summary**:
- Version byte prefix for future compatibility
- Fields encoded in order with direction support (asc/desc)
- Document ID appended as tie-breaker
- Cursor/startAfter uses same encoding (base64-url on wire)

## Configuration

```yaml
indexer:
  templatePath: "config/index/templates.yaml"
  puller:
    address: "localhost:9091"        # Puller gRPC address (empty = in-process)
    reconnectBackoff:
      initial: "1s"
      max: "30s"
      multiplier: 2
  progress:
    path: "data/indexer/progress"    # Progress marker storage
  rebuild:
    batchSize: 500
    qpsLimit: 5000
    maxConcurrent: 2
  lag:
    softWarnSeconds: 30
    hardRebuildSeconds: 120
  buffer:
    path: "data/indexer/events"
    maxSize: "5GiB"              # Max disk usage for event buffer
    batchSize: 100               # Events per PebbleDB batch write
    batchInterval: "100ms"       # Max delay before flush
  cleaner:
    interval: "1m"               # Cleanup check interval
    retention: "4h"              # Keep events for rebuild catch-up
```

## Observability

- **Metrics**: hit/miss counters, search latency, rebuild duration, lag versions
- **Health**: per-shard status (healthy/rebuilding/lagging/failed)
- **Logs**: template load, rebuild start/complete, gap detection

## Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| Index lag | Fall back to storage scan (500 doc limit); schedule rebuilds |
| Event ordering | Idempotent apply; version-based dedup |
| Memory pressure | Configurable shard limits; pluggable backend |
| Rebuild during traffic | Throttling; disk-based event buffer for catch-up |
| Buffer disk usage | Configurable maxSize (5GiB default); retention-based cleanup |
