# Indexer Service Architecture

Date: December 26, 2025
Status: Draft
Scope: Internal layout and data flows for the Indexer service.

## Why
- Provide accelerated lookup/ordering for queries while keeping storage as the source of truth.
- Decouple index maintenance from Query Engine so they can evolve independently.
- Allow pluggable index backends (in-memory → RocksDB/external KV) without touching Query logic.

## Overview

The Indexer service maintains secondary indexes derived from storage data. It consumes change events from the Puller service and provides search capabilities to the Query Engine.

```
                 +-------------------+
                 |   Query Engine    |
                 +--------+----------+
                          |
                   Search(plan)
                          |
                          v
+----------------------------------------------------------+
|                        Indexer                           |
|  +----------------+    +----------------+    +---------+ |
|  | Event Buffer   |<-->|    Manager     |--->| Shards  | |
|  | (PebbleDB)     |    | (routing/match)|    | (btree) | |
|  +----------------+    +----------------+    +---------+ |
+-------------+--------------------------------------------+
              |
   Subscribe (change events)
              |
              v
      +---------------+
      |    Puller     |
      +-------+-------+
              |
              v
      +---------------+
      |    MongoDB    |
      +---------------+
```

**Key Design Decision**: Indexer maintains its own local event buffer (PebbleDB) instead of relying on Puller's buffer. This enables:
- Multiple concurrent index rebuilds sharing the same buffered events
- Independent lifecycle control (cleanup after rebuild completes)
- Isolation from Puller's retention policy (default 1 hour may be insufficient for large rebuilds)

## Package Layout

```
internal/indexer/
├── interface.go           - Service interface (Search, Health, Stats)
├── service.go             - Main service implementation
├── internal/
│   ├── manager/           - Index manager (template matching, shard routing)
│   ├── shard/             - Per-collection index shard (btree, ByID map)
│   ├── buffer/            - Event buffer (reuses puller/internal/buffer)
│   ├── encoding/          - OrderKey encoding/decoding
│   ├── template/          - Index template loading and matching
│   └── rebuild/           - Rebuild orchestration and throttling
└── config/                - Indexer configuration
```

Note: `internal/indexer/internal/buffer/` reuses the PebbleDB-based buffer implementation from `internal/puller/buffer/` with Indexer-specific configuration.

## Service Interface

```go
type Service interface {
    // Search returns document references matching the plan
    Search(ctx context.Context, database, collection string, plan Plan) ([]DocRef, error)

    // Health returns current health status
    Health(ctx context.Context) (Health, error)

    // Stats returns index statistics
    Stats(ctx context.Context) (Stats, error)
}

type Plan struct {
    Collection  string
    Filters     []Filter      // prefix/range filters
    OrderBy     []OrderField  // ordering specification
    Limit       int
    StartAfter  string        // cursor for pagination
}

type DocRef struct {
    ID       string   // document ID within collection
    OrderKey []byte   // encoded sort key for cursor
}
```

## Data Flows

### Index Update Flow
```
Puller (change event)
    │
    ▼
Indexer.processEvent()
    │
    ├─> Write to local buffer (PebbleDB)
    │       └─> Key: ClusterTime + EventID
    │
    ├─> Match templates for collection
    │
    ├─> For each matched template:
    │       │
    │       ▼
    │   Shard.Upsert/Delete()
    │       │
    │       ├─> Encode OrderKey
    │       ├─> Update ByID map
    │       └─> Update btree
    │
    └─> Update checkpoint (last processed ClusterTime)
```

### Search Flow
```
Query Engine
    │
    ▼
Indexer.Search(plan)
    │
    ├─> Lookup shard by (collection, template)
    │
    ├─> Build search bounds from plan
    │
    ├─> Iterate btree from startAfter
    │
    └─> Return DocRef list (ID + OrderKey)

    │
    ▼
Query Engine
    │
    └─> Batch fetch documents by ID from Storage
```

### Rebuild Flow
```
Admin trigger / Gap detection / Startup
    │
    ▼
Rebuild.Start(collection, template)
    │
    ├─> Record current buffer position (startKey)
    │
    ├─> Mark shard as "rebuilding" (queries fall back to storage)
    │
    ├─> Clear existing shard data
    │
    ├─> Scan storage with pagination:
    │       │
    │       └─> Upsert each document (throttled)
    │
    ├─> Replay buffered events from startKey:
    │       │
    │       └─> buffer.ScanFrom(startKey) → apply to shard
    │
    ├─> Mark shard as "healthy"
    │
    └─> Optionally delete old buffer entries (retention-based)
```

**Why local buffer instead of memory WAL?**
- Memory WAL has fixed capacity (e.g., 10000 events) - overflow causes data loss
- Large collection rebuilds may take hours; Puller's 1-hour retention is insufficient
- Multiple concurrent rebuilds can share the same buffered events (write once, read many)

## Index Templates

Index templates define which fields to index for which collections. For detailed template definition, pattern matching rules, and conflict handling, see [Index Implementation Details](02.index.md#index-templates).

### Template Definition (YAML)

```yaml
templates:
  - name: chats_by_timestamp
    collectionPattern: users/{uid}/chats
    fields:
      - { field: timestamp, order: desc }
    includeDeleted: true

  - name: messages_by_sender
    collectionPattern: rooms/{rid}/messages
    fields:
      - { field: senderId, order: asc }
      - { field: timestamp, order: desc }
    includeDeleted: true
```

## In-Memory Index Structure

```
Index Manager
    │
    └─> map[string]*Shard  (keyed by database|normalizedPattern|templateIdentity)
            │
            └─> Shard
                  ├── Database: "myapp"
                  ├── Pattern: "users/*/chats"
                  ├── TemplateID: "timestamp:desc"
                  ├── Mu: RWMutex
                  ├── ByID: map[string][]byte  (docID → OrderKey)
                  └── Tree: btree[OrderKey → DocRef]
```

**Multi-Database Isolation**: Each shard is scoped to a specific database. The same collection pattern in different databases results in separate shards, ensuring complete data isolation between databases.

For detailed shard structure, OrderKey encoding, and search execution, see [Index Implementation Details](02.index.md#in-memory-index-shape-default-backend).

## Event Buffer

Indexer maintains a local PebbleDB-based event buffer for rebuild catch-up. This reuses the buffer implementation from Puller (`internal/puller/buffer/`).

### Why Local Buffer?

| Concern | Shared Puller Buffer | Local Indexer Buffer |
|---------|---------------------|---------------------|
| Rebuild read pattern | 10 indexes = 10 scans | Write once, read many |
| Retention control | Puller's policy (1h default) | Indexer's policy (rebuild duration) |
| Isolation | Puller failure affects Indexer | Independent lifecycle |
| Storage path | `data/puller/events/` | `data/indexer/events/` |

### Buffer Operations

```go
// Write: called for every event from Puller
buffer.Write(evt *StoreChangeEvent, checkpoint bson.Raw)

// Read: used during rebuild catch-up
iter := buffer.ScanFrom(startKey)  // iterate from recorded position
for iter.Next() {
    evt := iter.Event()
    shard.Upsert(evt)
}

// Cleanup: retention-based or on-demand after rebuild
buffer.DeleteBefore(cutoffKey)
```

### Key Format

Same as Puller buffer: `{ClusterTime.T:10d}-{ClusterTime.I:10d}-{EventID}`

This ensures:
- Lexicographic order = temporal order
- Efficient range scans for rebuild catch-up
- Deduplication by EventID

## OrderKey Encoding

OrderKey is a byte sequence that enables lexicographic sorting for indexed fields. For detailed encoding specification, see [Index Implementation Details](02.index.md#orderkey-encoding).

**Summary**:
- Version byte prefix for future compatibility
- Fields encoded in order with direction support (asc/desc)
- Document ID appended as tie-breaker
- Cursor/startAfter uses same encoding (base64-url on wire)

## Configuration

```yaml
indexer:
  templatePath: "config/index/templates.yaml"
  rebuild:
    batchSize: 500
    qpsLimit: 5000
    maxConcurrent: 2
  lag:
    softWarnSeconds: 30
    hardRebuildSeconds: 120
  buffer:
    path: "data/indexer/events"
    maxSize: "5GiB"              # Max disk usage for event buffer
    batchSize: 100               # Events per PebbleDB batch write
    batchInterval: "100ms"       # Max delay before flush
  cleaner:
    interval: "1m"               # Cleanup check interval
    retention: "4h"              # Keep events for rebuild catch-up
```

## Observability

- **Metrics**: hit/miss counters, search latency, rebuild duration, lag versions
- **Health**: per-shard status (healthy/rebuilding/lagging/failed)
- **Logs**: template load, rebuild start/complete, gap detection

## Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| Index lag | Fall back to storage scan (500 doc limit); schedule rebuilds |
| Event ordering | Idempotent apply; version-based dedup |
| Memory pressure | Configurable shard limits; pluggable backend |
| Rebuild during traffic | Throttling; disk-based event buffer for catch-up |
| Buffer disk usage | Configurable maxSize (5GiB default); retention-based cleanup |
