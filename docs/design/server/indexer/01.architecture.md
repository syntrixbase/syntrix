# Indexer Service Architecture

Date: December 26, 2025
Status: Draft
Scope: Internal layout and data flows for the Indexer service.

## Why
- Provide accelerated lookup/ordering for queries while keeping storage as the source of truth.
- Decouple index maintenance from Query Engine so they can evolve independently.
- Allow pluggable index backends (in-memory → RocksDB/external KV) without touching Query logic.

## Overview

The Indexer service maintains secondary indexes derived from storage data. It consumes change events from the Puller service via gRPC subscription and provides search capabilities to the Query Engine.

```
                 +-------------------+
                 |   Query Engine    |
                 +--------+----------+
                          |
                   Search(plan)
                          |
                          v
+----------------------------------------------------------+
|                        Indexer                           |
|  +----------------+    +----------------+    +---------+ |
|  | Event Buffer   |<-->|    Manager     |--->| Shards  | |
|  | (PebbleDB)     |    | (routing/match)|    | (btree) | |
|  +----------------+    +----------------+    +---------+ |
+-------------+--------------------------------------------+
              |
   gRPC Subscribe (progress marker)
              |
              v
      +---------------+
      |    Puller     |
      +-------+-------+
              |
              v
      +---------------+
      |    MongoDB    |
      +---------------+
```

**Subscription Model**: Indexer subscribes to Puller using the same gRPC `Subscribe` RPC as Streamer. It maintains a `progress marker` to track its position in the event stream, enabling seamless recovery after restarts.

**Key Design Decision**: Indexer maintains its own local event buffer (PebbleDB) instead of relying on Puller's buffer. This enables:
- Multiple concurrent index rebuilds sharing the same buffered events
- Independent lifecycle control (cleanup after rebuild completes)
- Isolation from Puller's retention policy (default 1 hour may be insufficient for large rebuilds)

## Package Layout

```
internal/indexer/
├── interface.go           - Service interface (Search, Health, Stats)
├── service.go             - Main service implementation
├── internal/
│   ├── manager/           - Index manager (template matching, shard routing)
│   ├── shard/             - Per-collection index shard (btree, ByID map)
│   ├── buffer/            - Event buffer (reuses puller/internal/buffer)
│   ├── encoding/          - OrderKey encoding/decoding
│   ├── template/          - Index template loading and matching
│   └── rebuild/           - Rebuild orchestration and throttling
└── config/                - Indexer configuration
```

Note: `internal/indexer/internal/buffer/` reuses the PebbleDB-based buffer implementation from `internal/puller/buffer/` with Indexer-specific configuration.

## Service Interface

```go
type Service interface {
    // Search returns document references matching the plan
    Search(ctx context.Context, database, collection string, plan Plan) ([]DocRef, error)

    // Health returns current health status
    Health(ctx context.Context) (Health, error)

    // Stats returns index statistics
    Stats(ctx context.Context) (Stats, error)
}

type Plan struct {
    Collection  string
    Filters     []Filter      // prefix/range filters
    OrderBy     []OrderField  // ordering specification
    Limit       int
    StartAfter  string        // cursor for pagination
}

type DocRef struct {
    ID       string   // document ID within collection
    OrderKey []byte   // encoded sort key for cursor
}
```

## Data Flows

### Puller Subscription

Indexer acts as a consumer of the Puller service, similar to Streamer:

```
┌─────────────────────────────────────────────────────────────────┐
│                         Puller                                   │
│  MongoDB ChangeStream → Buffer (PebbleDB, 1h) → gRPC Subscribe   │
└──────────────────────────────┬──────────────────────────────────┘
                               │
           ┌───────────────────┼───────────────────┐
           │                   │                   │
           ▼                   ▼                   ▼
    ┌────────────┐      ┌────────────┐      ┌────────────┐
    │  Streamer  │      │  Indexer   │      │  Trigger   │
    │ (realtime) │      │  (index)   │      │ (webhooks) │
    └────────────┘      └────────────┘      └────────────┘
```

**Subscription Details**:
- **Protocol**: gRPC bidirectional streaming (`PullerService.Subscribe`)
- **Consumer ID**: `"indexer"` (for logging/monitoring)
- **Progress Marker**: Opaque string saved to disk; used to resume after restart
- **Auto-reconnect**: Exponential backoff (1s to 30s) on connection failure

### Index Update Flow
```
Puller.Subscribe() stream
    │
    ▼
Indexer.processEvent()
    │
    ├─> Write to local buffer (PebbleDB)
    │       └─> Key: ClusterTime + EventID
    │
    ├─> Match templates for collection
    │
    ├─> For each matched template:
    │       │
    │       ▼
    │   Shard.Upsert/Delete()
    │       │
    │       ├─> Encode OrderKey
    │       ├─> Update ByID map
    │       └─> Update btree
    │
    └─> Save progress marker to disk
```

### Search Flow
```
Query Engine
    │
    ▼
Indexer.Search(plan)
    │
    ├─> Lookup shard by (collection, template)
    │
    ├─> Build search bounds from plan
    │
    ├─> Iterate btree from startAfter
    │
    └─> Return DocRef list (ID + OrderKey)

    │
    ▼
Query Engine
    │
    └─> Batch fetch documents by ID from Storage
```

### Rebuild Flow
```
Admin trigger / Gap detection / Startup
    │
    ▼
Rebuild.Start(collection, template)
    │
    ├─> Record current buffer position (startKey)
    │
    ├─> Mark shard as "rebuilding" (queries fall back to storage)
    │
    ├─> Clear existing shard data
    │
    ├─> Scan storage with pagination:
    │       │
    │       └─> Upsert each document (throttled)
    │
    ├─> Replay buffered events from startKey:
    │       │
    │       └─> buffer.ScanFrom(startKey) → apply to shard
    │
    ├─> Mark shard as "healthy"
    │
    └─> Optionally delete old buffer entries (retention-based)
```

**Why local buffer instead of memory WAL?**
- Memory WAL has fixed capacity (e.g., 10000 events) - overflow causes data loss
- Large collection rebuilds may take hours; Puller's 1-hour retention is insufficient
- Multiple concurrent rebuilds can share the same buffered events (write once, read many)

### Recovery Scenarios

#### Indexer Restart

```
Indexer.Start()
    │
    ├─> Load saved progress marker from disk
    │
    ├─> Subscribe to Puller with progress marker
    │       │
    │       └─> Puller replays events from that position (catch-up mode)
    │
    ├─> Process events, update shards
    │
    └─> If gap detected (progress too old for Puller buffer):
            │
            └─> Trigger rebuild for affected shards
```

**Progress Marker Storage**: Saved to `data/indexer/progress` file, updated after each batch of events is processed and written to local buffer.

#### Puller Restart

```
Puller restarts
    │
    ▼
Indexer detects gRPC stream closed
    │
    ├─> Auto-reconnect with exponential backoff
    │
    └─> Resume subscription from last saved progress marker
        │
        └─> Puller's catch-up mode replays missed events
```

**No data loss**: Both Indexer and Puller maintain independent buffers; events are not lost as long as the gap is within retention periods.

#### Fallback Behavior During Rebuild

When a shard is in "rebuilding" state:
- Queries fall back to storage scan with 500 document limit
- Response includes `X-Index-Status: rebuilding` header
- Metrics track fallback rate for monitoring

## Index Templates

Index templates define which fields to index for which collections. For detailed template definition, pattern matching rules, and conflict handling, see [Index Implementation Details](02.index.md#index-templates).

### Template Definition (YAML)

```yaml
templates:
  - name: chats_by_timestamp
    collectionPattern: users/{uid}/chats
    fields:
      - { field: timestamp, order: desc }
    includeDeleted: true

  - name: messages_by_sender
    collectionPattern: rooms/{rid}/messages
    fields:
      - { field: senderId, order: asc }
      - { field: timestamp, order: desc }
    includeDeleted: true
```

## In-Memory Index Structure

```
Index Manager
    │
    └─> map[string]*Shard  (keyed by database|normalizedPattern|templateIdentity)
            │
            └─> Shard
                  ├── Database: "myapp"
                  ├── Pattern: "users/*/chats"
                  ├── TemplateID: "timestamp:desc"
                  ├── Mu: RWMutex
                  ├── ByID: map[string][]byte  (docID → OrderKey)
                  └── Tree: btree[OrderKey → DocRef]
```

**Multi-Database Isolation**: Each shard is scoped to a specific database. The same collection pattern in different databases results in separate shards, ensuring complete data isolation between databases.

For detailed shard structure, OrderKey encoding, and search execution, see [Index Implementation Details](02.index.md#in-memory-index-shape-default-backend).

## Event Buffer

Indexer maintains a local PebbleDB-based event buffer for rebuild catch-up. This reuses the buffer implementation from Puller (`internal/puller/buffer/`).

### Why Local Buffer?

| Concern | Shared Puller Buffer | Local Indexer Buffer |
|---------|---------------------|---------------------|
| Rebuild read pattern | 10 indexes = 10 scans | Write once, read many |
| Retention control | Puller's policy (1h default) | Indexer's policy (rebuild duration) |
| Isolation | Puller failure affects Indexer | Independent lifecycle |
| Storage path | `data/puller/events/` | `data/indexer/events/` |

### Buffer Operations

```go
// Write: called for every event from Puller
buffer.Write(evt *StoreChangeEvent, checkpoint bson.Raw)

// Read: used during rebuild catch-up
iter := buffer.ScanFrom(startKey)  // iterate from recorded position
for iter.Next() {
    evt := iter.Event()
    shard.Upsert(evt)
}

// Cleanup: retention-based or on-demand after rebuild
buffer.DeleteBefore(cutoffKey)
```

### Key Format

Same as Puller buffer: `{ClusterTime.T:10d}-{ClusterTime.I:10d}-{EventID}`

This ensures:
- Lexicographic order = temporal order
- Efficient range scans for rebuild catch-up
- Deduplication by EventID

## OrderKey Encoding

OrderKey is a byte sequence that enables lexicographic sorting for indexed fields. For detailed encoding specification, see [Index Implementation Details](02.index.md#orderkey-encoding).

**Summary**:
- Version byte prefix for future compatibility
- Fields encoded in order with direction support (asc/desc)
- Document ID appended as tie-breaker
- Cursor/startAfter uses same encoding (base64-url on wire)

## Configuration

```yaml
indexer:
  templatePath: "config/index/templates.yaml"
  puller:
    address: "localhost:9091"        # Puller gRPC address (empty = in-process)
    reconnectBackoff:
      initial: "1s"
      max: "30s"
      multiplier: 2
  progress:
    path: "data/indexer/progress"    # Progress marker storage
  rebuild:
    batchSize: 500
    qpsLimit: 5000
    maxConcurrent: 2
  lag:
    softWarnSeconds: 30
    hardRebuildSeconds: 120
  buffer:
    path: "data/indexer/events"
    maxSize: "5GiB"              # Max disk usage for event buffer
    batchSize: 100               # Events per PebbleDB batch write
    batchInterval: "100ms"       # Max delay before flush
  cleaner:
    interval: "1m"               # Cleanup check interval
    retention: "4h"              # Keep events for rebuild catch-up
```

## Observability

- **Metrics**: hit/miss counters, search latency, rebuild duration, lag versions
- **Health**: per-shard status (healthy/rebuilding/lagging/failed)
- **Logs**: template load, rebuild start/complete, gap detection

## Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| Index lag | Fall back to storage scan (500 doc limit); schedule rebuilds |
| Event ordering | Idempotent apply; version-based dedup |
| Memory pressure | Configurable shard limits; pluggable backend |
| Rebuild during traffic | Throttling; disk-based event buffer for catch-up |
| Buffer disk usage | Configurable maxSize (5GiB default); retention-based cleanup |
