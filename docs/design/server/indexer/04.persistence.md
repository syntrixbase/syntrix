# Index Persistence with PebbleDB

Date: 2026-01-13
Status: Draft
Scope: Persistent storage for indexes using PebbleDB

> This document covers persistence implementation. For in-memory index design, see [Index Implementation](02.index.md).

## Motivation

Current indexes are purely in-memory (BTree + HashMap). This has significant limitations:

| Problem | Impact |
|---------|--------|
| Service restart | All indexes lost, full rebuild required |
| Memory growth | Large datasets cause OOM |
| Startup time | Rebuild from storage scan + event replay |
| Buffer overflow | If Puller buffer overflows during downtime, full rebuild required |

## Goals

1. **Durability** - Indexes survive service restarts
2. **Fast recovery** - Resume from last checkpoint, no full rebuild
3. **Bounded memory** - Configurable cache size, not proportional to data
4. **Backward compatible** - Same Manager/Index APIs, transparent to callers

## Non-Goals (v1)

- Distributed/sharded indexes
- Multi-node replication
- Online schema changes

---

## Architecture

### Storage Layout

Single PebbleDB instance with key prefix isolation:

```
data/indexer/
  └── indexes.db/          # Single PebbleDB instance
        ├── MANIFEST-*
        ├── OPTIONS-*
        ├── *.sst
        └── WAL/
```

### Key Design

To reduce key length overhead, we use xxHash64 + hex encoding to create a compact index identifier:

```
hash = hex(xxHash64(pattern + "|" + tmplID))  → 16 chars (full 64-bit, no truncation)
```

**Key structure:**

```
# Index data (ordered by OrderKey for range scans)
idx/{db}/{hash}/{orderKey}              →  {docID}

# Reverse index (for Upsert: find old key to delete)
rev/{db}/{hash}/{docID}                 →  {orderKey}

# Hash mapping (for debugging/introspection)
map/{db}/{hash}                         →  {pattern}|{tmplID}

# Index metadata
meta/state/{db}/{hash}                  →  "healthy"|"rebuilding"|"failed"

# Global checkpoint
meta/progress                           →  {lastEventID}
```

**Key format details:**
- `{db}` is URL-encoded to avoid `/` conflicts
- `{hash}` is hex encoded full xxHash64 (16 chars, charset: `0-9a-f`)
- `{orderKey}` is raw bytes (already lexicographically ordered, includes docID as tie-breaker)
- `{docID}` is stored in idx value for easy extraction during search
- `map/` entries are written once per index for debugging purposes

**Hash collision handling:**
- Full xxHash64 (64-bit) has negligible collision probability (~1 in 2^64)
- If collision occurs, both indexes share the same key space (incorrect behavior)
- Detection: check `map/{db}/{hash}` before creating new index
- Resolution: fail index creation with error (operator must rename pattern/template)

### Operations

#### Upsert(db, pattern, tmplID, docID, orderKey)

```
hash = hex(xxHash64(pattern + "|" + tmplID))

1. Read: rev/{db}/{hash}/{docID} → oldOrderKey
2. Batch write:
   - If oldOrderKey exists: Delete idx/{db}/{hash}/{oldOrderKey}
   - Set idx/{db}/{hash}/{orderKey} → {docID}
   - Set rev/{db}/{hash}/{docID} → orderKey
   - Set map/{db}/{hash} → {pattern}|{tmplID}  (if not exists)
3. Commit batch
```

#### Delete(db, pattern, tmplID, docID)

```
hash = hex(xxHash64(pattern + "|" + tmplID))

1. Read: rev/{db}/{hash}/{docID} → orderKey
2. If not found: return (idempotent)
3. Batch write:
   - Delete idx/{db}/{hash}/{orderKey}
   - Delete rev/{db}/{hash}/{docID}
4. Commit batch
```

#### Search(db, pattern, tmplID, opts)

```
hash = hex(xxHash64(pattern + "|" + tmplID))

1. Snapshot pending operations: memOps = copy(pending ∪ flushing)
2. Build key range:
   - lower = "idx/{db}/{hash}/" + opts.Lower
   - upper = "idx/{db}/{hash}/" + opts.Upper
3. Iterate PebbleDB [lower, upper):
   - Extract orderKey from key, docID from value
   - Check memOps[docID]: if delete, skip; if update, use mem version
   - Collect results as DocRef{ID: docID, OrderKey: orderKey}
4. Add new inserts from memOps not in DB
5. Sort by orderKey, apply limit
6. Return []DocRef
```

#### Get(db, pattern, tmplID, docID)

```
hash = hex(xxHash64(pattern + "|" + tmplID))

1. Check memOps[docID] first
2. If not in mem: Read rev/{db}/{hash}/{docID}
3. Return orderKey or nil
```

---

## Async Batching

### Design

Similar to Puller buffer pattern:

```go
type Store struct {
    db        *pebble.DB

    mu        sync.RWMutex
    pending   map[string]*pendingOp  // key: "{db}|{hash}|{docID}"
    flushing  map[string]*pendingOp

    notifyCh  chan struct{}
    closeCh   chan struct{}
    batcherWG sync.WaitGroup

    cfg       StoreConfig
}

type pendingOp struct {
    db       string
    hash     string  // hex(xxHash64(pattern + "|" + tmplID))
    docID    string
    orderKey []byte  // nil = delete
}
```

### Write Flow

```
Upsert/Delete()
    │
    ▼
pending map (in memory)
    │
    ├──► notifyCh signal
    │
    ▼
batcher goroutine
    │
    ├──► BatchSize reached OR
    ├──► BatchInterval elapsed OR
    └──► Close() called
            │
            ▼
        swap pending → flushing
            │
            ▼
        build pebble.Batch
            │
            ▼
        batch.Commit(pebble.Sync)
            │
            ▼
        clear flushing
```

### Search Consistency

Search must see both persisted and pending data:

```go
func (s *Store) Search(...) ([]DocRef, error) {
    // 1. Snapshot pending ops
    s.mu.RLock()
    memOps := make(map[string]*pendingOp)
    for k, v := range s.flushing {
        memOps[k] = v  // flushing first (older)
    }
    for k, v := range s.pending {
        memOps[k] = v  // pending overwrites flushing (newer)
    }
    s.mu.RUnlock()

    // 2. Scan PebbleDB + merge with memOps
    // 3. Return merged results
}
```

---

## Configuration

```go
type StoreConfig struct {
    // Storage path (required)
    Path string `yaml:"path"`  // default: "data/indexer/indexes.db"

    // Batching
    BatchSize     int           `yaml:"batch_size"`      // default: 100
    BatchInterval time.Duration `yaml:"batch_interval"`  // default: 100ms
    QueueSize     int           `yaml:"queue_size"`      // default: 10000

    // PebbleDB tuning
    BlockCacheSize int64 `yaml:"block_cache_size"`  // default: 64MB
}
```

YAML example:

```yaml
indexer:
  store:
    path: "data/indexer/indexes.db"
    batch_size: 100
    batch_interval: 100ms
    queue_size: 10000
    block_cache_size: 67108864  # 64MB
```

---

## Rebuild Integration

### Index Rebuild Flow

```
hash = hex(xxHash64(pattern + "|" + tmplID))

1. Set meta/state/{db}/{hash} = "rebuilding"
2. Delete all keys with prefix idx/{db}/{hash}/
3. Delete all keys with prefix rev/{db}/{hash}/
4. Storage scan → Upsert each document
5. Event replay from recorded position
6. Set meta/state/{db}/{hash} = "healthy"
7. Flush pending writes
```

### Startup Recovery

```
1. Open PebbleDB
2. Read meta/progress → lastEventID
3. For each index (scan meta/state/ prefix):
   - If state == "healthy": ready to serve
   - If state == "rebuilding": resume or restart rebuild
   - If state == "failed": log error, skip index
4. Subscribe to Puller from lastEventID
5. Resume normal operation
```

---

## Storage Mode Selection

Memory and Persistence are two independent index storage modes. Users choose via configuration:

```yaml
indexer:
  storage_mode: "memory"      # or "pebble"

  # Memory mode settings (when storage_mode: memory)
  # No additional config needed, uses in-memory BTree

  # Pebble mode settings (when storage_mode: pebble)
  store:
    path: "data/indexer/indexes.db"
    batch_size: 100
    batch_interval: 100ms
    queue_size: 10000
    block_cache_size: 67108864
```

### Mode Comparison

| Aspect | Memory | Pebble |
|--------|--------|--------|
| Startup | Rebuild from storage | Load from disk |
| Restart recovery | Full rebuild | Resume from checkpoint |
| Memory usage | Proportional to data | Bounded by cache |
| Write latency | ~1μs | ~10-50μs (async) |
| Read latency | ~10μs | ~50-100μs (cached) |
| Use case | Dev/test, small datasets | Production, large datasets |

### Implementation

Both modes implement the same `Store` interface:

```go
type Store interface {
    Upsert(db, pattern, tmplID, docID string, orderKey []byte) error
    Delete(db, pattern, tmplID, docID string) error
    Get(db, pattern, tmplID, docID string) ([]byte, bool)
    Search(db, pattern, tmplID string, opts SearchOptions) ([]DocRef, error)
    // ...
}

// Factory function
func NewStore(cfg Config) (Store, error) {
    switch cfg.StorageMode {
    case "memory":
        return NewMemoryStore(), nil
    case "pebble":
        return NewPebbleStore(cfg.Store)
    default:
        return nil, fmt.Errorf("unknown storage mode: %s", cfg.StorageMode)
    }
}
```

---

## Performance Considerations

### Expected Latency

| Operation | In-Memory | PebbleDB | Notes |
|-----------|-----------|----------|-------|
| Upsert | ~1μs | ~10-50μs | Async batch amortizes |
| Search (cache hit) | ~10μs | ~50-100μs | Block cache helps |
| Search (cache miss) | N/A | ~1-5ms | SSD read |
| Get by ID | ~100ns | ~10-50μs | Usually cached |

### Memory Usage

| Component | Size |
|-----------|------|
| Block cache | Configurable (default 64MB) |
| Pending ops | ~1KB × QueueSize |
| Iterator buffers | ~10KB per active search |

---

## Interfaces

### Store Interface

```go
// internal/indexer/internal/store/store.go

type Store interface {
    // Index operations
    Upsert(db, pattern, tmplID, docID string, orderKey []byte) error
    Delete(db, pattern, tmplID, docID string) error
    Get(db, pattern, tmplID, docID string) (orderKey []byte, found bool)

    // Search
    Search(db, pattern, tmplID string, opts SearchOptions) ([]DocRef, error)

    // Index management
    DeleteIndex(db, pattern, tmplID string) error
    SetState(db, pattern, tmplID string, state IndexState) error
    GetState(db, pattern, tmplID string) (IndexState, error)

    // Checkpoint
    SaveProgress(eventID string) error
    LoadProgress() (string, error)

    // Lifecycle
    Flush() error
    Close() error
}

type SearchOptions struct {
    Lower      []byte
    Upper      []byte
    StartAfter []byte
    Limit      int
}

type IndexState string

const (
    IndexStateHealthy    IndexState = "healthy"
    IndexStateRebuilding IndexState = "rebuilding"
    IndexStateFailed     IndexState = "failed"
)
```

---

## Error Handling

| Error | Handling |
|-------|----------|
| PebbleDB write failure | Close store, fail all pending ops |
| Batch commit failure | Retry once, then close store |
| Corruption detected | Log, mark index failed, trigger rebuild |
| Disk full | Fail writes, continue reads |

---

## Observability

### Metrics

```
indexer_store_writes_total{status="success|error"}
indexer_store_reads_total{status="success|error"}
indexer_store_batch_size_histogram
indexer_store_batch_latency_seconds
indexer_store_pending_ops_gauge
indexer_store_disk_bytes_gauge
```

### Health Check

```go
func (s *Store) Health() StoreHealth {
    return StoreHealth{
        Status:      "ok",
        DiskUsage:   s.db.Metrics().DiskSpaceUsage(),
        PendingOps:  len(s.pending),
        LastFlush:   s.lastFlushTime,
    }
}
```

---

## References

- [Puller Buffer Implementation](../../puller/01.buffer.md)
- [PebbleDB Documentation](https://github.com/cockroachdb/pebble)
- [Index Architecture](01.architecture.md)
