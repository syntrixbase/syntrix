# Index Implementation Details

Date: December 26, 2025
Status: Draft
Scope: Detailed index implementation for the Indexer service.

> This document covers implementation details. For architecture overview, see [Indexer Architecture](01.architecture.md).

## Responsibilities
- Apply change events (upsert/delete/tombstone) idempotently.
- Serve `Search(plan)` for prefix/range (initial), with room for composite order keys.
- Manage rebuild lifecycle (drop/rebuild, checkpoints, throttling).
- Expose health/stats for hit/miss, lag, rebuild progress.

## Interfaces (sketch)
- `Upsert(ctx, evt Event) error`
- `Delete(ctx, evt Event) error`
- `Search(ctx, plan Plan) ([]DocRef, error)`
- `Rebuild(ctx, iter Iterator) error`
- `Health(ctx) (Health, error)` / `Stats(ctx) (Stats, error)`
- `type Event` aligns with Data change event schema.
- `type Plan` captures collection, filters (prefix/range), order fields, limit, startAfter.
- `type DocRef struct { ID string; OrderKey []byte }` // ID is the document id within the collection; fullpath = collection + ID

## Index Templates
- Definition example:
```json
{
  "collectionPattern": "users/{uid}/chats",
  "fields": [
    {"field": "name", "order": "asc"},
    {"field": "age",  "order": "desc"}
  ],
  "options": {
    "unique": false,
    "sparse": false,
    "includeDeleted": true
  }
}
```
- Storage: templates loaded from a static file `config/index/templates.yaml` (default path; overridable via config/env). No live mutation in v1; changes require config update/restart. Keep interface hooks to allow future backing by metadata collection.
- YAML shape (example):
```
templates:
  - name: chats_by_name_age
    collectionPattern: users/{uid}/chats
    fields:
      - { field: name, order: asc }
      - { field: age,  order: desc }
    includeDeleted: true  # fixed true
```
- Pattern rules: path segments only; `{var}` matches one segment; variable *names* are ignored for matching/priority (normalize `{uid}` and `{user_id}` to the same wildcard); no cross-segment wildcards; validate no empty/invalid segments. **Indexes are collection-scoped**: pattern must end at a collection root, not a specific document (e.g., `users/{uid}/chats` is valid collection, `users/{uid}/chats/{chatid}` is invalid because it targets a document level).
- Matching priority: more concrete wins (more fixed segments > fewer placeholders > longer path). Ties → reject unless explicitly resolved (e.g., naming/weight).
- Multiple indexes per collection allowed if patterns are unambiguous; for the same `collectionPattern`, allow multiple templates distinguished by field list/name. Shards must key by `(normalizedPattern, templateIdentity)` (e.g., name or fields signature), not pattern alone.
- Validation: dedupe fields, check order values, forbid duplicate (pattern+fields) definitions; optional name for human reference; includeDeleted is always true (replication needs tombstones).
- Encoding: key uses fields in order; numeric big-endian, strings length-prefixed; may append `deleted`/`version` for tie-break; cursor/startAfter share the same encoding with a version byte (OrderKey encoding versioning). A version bump means the same logical index is rebuilt with a new encoding; it is **not** a new template/index definition.
 - Encoding (OrderKey):
   - Prefix 1-byte encoding version.
   - For each indexed field (in order):
     - string: uvarint length + raw bytes (UTF-8), asc as-is; desc invert bytes to keep lexicographic order.
     - int/float: big-endian sortable representation; desc invert bytes.
     - bool: 0x00 (false), 0x01 (true); desc invert.
     - null: 0x00 marker if needed; keep type ordering stable.
   - Tie-breaker: append doc `ID` (length-prefixed) to ensure deterministic order for equals; optional `deleted`/`version` bits if required for tombstone ordering.
   - Cursor/startAfter: reuse the same encoding, then base64-url encode for wire.
   - Version bump => rebuild existing index with new encoding (no new template); mismatched cursor/index version returns `IndexNotReady`.

### OrderKey Encoding

OrderKey layout (bytes, left-to-right):
```
[ver:1][field1...][field2...][...][id_len:uvarint][id bytes][optional deleted/version bits]

string field (asc): [len:uvarint][utf8 bytes]
string field (desc): invert each byte after length
number field: big-endian sortable bytes (float via IEEE 754 with sign flip for sorting if needed); desc invert bytes
bool: 0x00/0x01 (desc invert)
```
- Apply: on change event, match collection path to templates and upsert/delete per matched template; on rebuild, scan collections matching the pattern.
- Observability: track template hit rate, rebuild time, failures/disabled state.

### In-Memory Index Shape (default backend)
- Top-level: `map[string]*Shard` keyed by `(database, normalizedPattern, templateIdentity)` to allow multiple indexes per collectionPattern with database isolation. Example key: `myapp|users/*/chats|name:asc,age:desc` (document-level like `users/*/chats/*` is invalid).
- Shard fields (example):
  - `Database string` (e.g., `myapp`).
  - `Pattern string` (normalized, e.g., `users/*/chats`).
  - `TemplateID string` (e.g., provided name, else deterministic fields signature like `name:asc,age:desc`).
  - `RawPattern string` (original, e.g., `users/{uid}/chats`).
  - `Tree` (btree or similar) keyed by `OrderKey []byte`, value `DocRef{ID string, OrderKey []byte}`; supports seek/range by OrderKey.
  - `ByID map[string][]byte` to replace existing OrderKey on upsert/delete without full scan (ID is doc id within the collection).
  - `Mu` (RW lock) guarding the shard.
- Sorted structure: balanced tree (btree) to support prefix/range and ordered iteration; allows seek to `startAfter` and forward scan with limit.
- Duplicate handling: on upsert, if `ByID` has prior key, delete old entry from tree then insert new; on tombstone/delete, remove entry (includeDeleted=true remains encoded in OrderKey for ordering when stored).
- Concurrency: per-shard RW lock; applies batch events under write lock; readers take read lock (or snapshot view if backend supports).
- Persistence: in-memory only for initial cut; rebuild on startup from Data iterator; backend is swappable (e.g., RocksDB) without changing Query code.

ASCII shape (illustrative):
```
                   +---------------------------+
                   |     Index Manager         |
                   +------------+--------------+
                                |
                per (database, pattern, templateIdentity)
                                |
        +-----------------------+-----------------------+
        |                                               |
+----------------------------------------------+    +--------------------------------+
| Shard: myapp|users/*/chats|name:asc,age:desc |    | Shard: other|users/*/chats|... |
+----------------------------------------------+    +--------------------------------+
| Database: myapp                              |    | Database: other                |
| Pattern: users/*/chats                       |    | Pattern: users/*/chats         |
| TemplateID: name:asc,age:desc                |    | TemplateID: name:asc,age:desc  |
| RawPattern: users/{uid}/chats                |    | RawPattern: users/{uid}/chats  |
| Mu (RW lock)                                 |    | Mu (RW lock)                   |
| ByID: map[id] -> OrderKey                    |    | ByID: map[id]->OrderKey        |
| Tree (btree):                                |    | Tree (btree):                  |
|   OrderKey -> DocRef{ID, OrderKey}           |    |   OrderKey -> DocRef           |
+----------------------------------------------+    +--------------------------------+
         ↑ Isolated: same pattern, different databases = separate shards

OrderKey layout:
[version 1B][field1...][field2...][...][id_len:uvarint][id bytes][optional deleted/version bits]
- strings: len + bytes (desc invert bytes)
- nums: big-endian sortable (desc invert)
- bool: 0x00/0x01 (desc invert)
- cursor/startAfter = same bytes, base64-url on wire
```

### Matching & Conflict Handling (Collection Pattern)
- Matching: split `collection` and `collectionPattern` into segments; `{var}` matches exactly one segment. Reject patterns with empty/invalid segments.
- Priority (higher wins): 1) more fixed segments; 2) longer total segments. If still tied → conflict.
- Conflicts: if two templates match a path with equal priority, treat as conflict and fail apply for that doc (or drop the lower-ranked when explicit weight is provided; default: fail-fast). Templates with the same structural pattern (e.g., `user/{uid}/chats` vs `user/{user_id}/chats`) are considered duplicates and should be rejected at definition time. Emit metrics/logs for conflicts.
- Multiple matches allowed when priorities differ (e.g., more concrete beats generic); each non-conflicting match results in its own index entry.

### Query-to-Index Matching Rules

This section defines how Query Engine selects an index based on query's `orderBy` and `filters`.

#### Core Principles

1. **OrderBy Prefix Matching**: Query's `orderBy` must be a **prefix** of template's `fields` (can be shorter, not longer).
2. **Direction Must Match**: Field directions must match exactly; reverse traversal is not supported in v1.
3. **Equality Filter Prefix**: Equality filters on leading index fields allow subsequent fields to be used for ordering.
4. **Range Filter Last**: At most one range filter, positioned after all equality filters; it "consumes" the index.

#### OrderBy Matching Examples

| Query OrderBy | Template Fields | Match? | Reason |
|---------------|-----------------|--------|--------|
| `[name:asc]` | `[name:asc, age:desc]` | ✅ | Prefix match |
| `[name:asc, age:desc]` | `[name:asc, age:desc]` | ✅ | Exact match |
| `[name:asc, age:desc, ts:asc]` | `[name:asc, age:desc]` | ❌ | Extra field not in index |
| `[age:desc]` | `[name:asc, age:desc]` | ❌ | Skips leading field |
| `[name:desc]` | `[name:asc]` | ❌ | Direction mismatch |

#### Equality Filter + OrderBy Examples

When query has equality filters on leading index fields, the orderBy can start from the next field.

| Filters | OrderBy | Template Fields | Match? | Reason |
|---------|---------|-----------------|--------|--------|
| `status = 'active'` | `[createdAt:desc]` | `[status:asc, createdAt:desc]` | ✅ | Equality on prefix, order on next |
| `status = 'active', type = 'msg'` | `[createdAt:desc]` | `[status:asc, type:asc, createdAt:desc]` | ✅ | Multiple equality filters |
| `status > 'a'` | `[createdAt:desc]` | `[status:asc, createdAt:desc]` | ❌ | Range filter blocks subsequent order |
| (none) | `[createdAt:desc]` | `[status:asc, createdAt:desc]` | ❌ | Leading field not covered |
| `status = 'active'` | `[type:asc]` | `[status:asc, createdAt:desc]` | ❌ | orderBy field not in index |

#### Range Filter Placement

- Range filters (`>`, `<`, `>=`, `<=`) can only appear on the **last used** index field.
- After a range filter, no more index fields can be utilized for ordering.

| Filters | OrderBy | Template Fields | Match? | Reason |
|---------|---------|-----------------|--------|--------|
| `ts > 1000` | `[ts:desc]` | `[ts:desc]` | ✅ | Range and order on same field |
| `status = 'active', ts > 1000` | `[ts:desc]` | `[status:asc, ts:desc]` | ✅ | Equality prefix + range on order field |
| `ts > 1000` | `[status:asc]` | `[ts:desc, status:asc]` | ❌ | Range blocks subsequent order |

#### Index Selection Priority

When multiple templates match a collection, select based on:

1. **Coverage**: Template that covers more `orderBy` fields wins.
2. **Filter Coverage**: Template that covers more equality filter fields wins.
3. **Exactness**: Exact match preferred over prefix match.
4. **Conflict**: If tied after above criteria, fail-fast (require explicit disambiguation).

#### Pseudocode (Query-to-Index Matching)

```pseudo
function canServeQuery(query, template):
  // Step 1: Check collection pattern match
  if !matchCollectionPattern(query.collection, template.collectionPattern):
    return false

  // Step 2: Extract equality filters on index prefix
  eqFields = []
  for f in template.fields:
    filter = query.filters.find(f.field, op=eq)
    if filter == nil: break
    eqFields.append(f.field)

  // Step 3: Check range filter (at most one, on next field)
  rangeField = nil
  remainingFilters = query.filters.exclude(eqFields)
  for f in remainingFilters:
    if f.op in [gt, lt, gte, lte]:
      if rangeField != nil: return false  // multiple range filters
      rangeField = f.field

  // Step 4: Determine usable index prefix
  usablePrefix = eqFields
  if rangeField != nil:
    nextField = template.fields[len(eqFields)]
    if nextField == nil or nextField.field != rangeField:
      return false  // range not on next index field
    usablePrefix.append(rangeField)

  // Step 5: Check orderBy is prefix of remaining index fields
  orderStart = len(usablePrefix)
  for i, orderField in enumerate(query.orderBy):
    templateIdx = orderStart + i
    if templateIdx >= len(template.fields):
      return false  // orderBy exceeds index
    if template.fields[templateIdx].field != orderField.field:
      return false  // field mismatch
    if template.fields[templateIdx].order != orderField.direction:
      return false  // direction mismatch

  return true

function selectBestIndex(query, templates):
  candidates = []
  for t in templates:
    if canServeQuery(query, t):
      score = computeScore(query, t)  // based on coverage
      candidates.append((t, score))

  if candidates.empty(): return nil

  best = max(candidates, by=score)
  ties = [c for c in candidates if c.score == best.score]
  if len(ties) > 1:
    return Error("ambiguous index match")

  return best.template
```

### Search Execution (pseudocode)
```pseudo
function search(database, plan):
  shard = lookupShard(database, plan.collectionPattern, plan.templateIdentity)
  if shard == nil: return ErrIndexNotReady

  (lower, upper) = buildBounds(plan) // prefix/range using OrderKey encoding
  cursorStart = lower
  if plan.startAfter != "":
    // decode cursor; version mismatch handled earlier
    cursorStart = decodeOrderKey(plan.startAfter)

  it = shard.Tree.iterFrom(cursorStart)
  results = []
  while it.valid() and len(results) < plan.limit:
    key, ref = it.current()
    if upper != nil and key > upper: break
    if plan.startAfter != "" and key == cursorStart:
      it.next(); continue  // exclusive startAfter
    results.append(ref)
    it.next()

  return results
```
Notes: `buildBounds` encodes prefix/range per plan fields; Query layer post-filters non-indexable predicates and batches Data fetch by returned IDs. Version check for cursor/orderkey happens before `search` (if mismatch → IndexNotReady).

### Pseudocode (conflict-aware matching)
```pseudo
function score(template):
  fixed = count_non_var_segments(template)
  total = segment_count(template)
  return (fixed, total)  // lexicographic

function matchTemplates(path, templates):
  candidates = []
  for t in templates:
    norm = normalize_vars(t) // replace {anything} with * for duplicate detection
    if seen(norm): raise DuplicateTemplateError(t, norm)
    mark_seen(norm)
    if !segments_same_length(path, t): continue
    if !all_segments_match(path, t): continue  // fixed must equal; var matches any single segment
    candidates.append((t, score(t)))

  // group by best score
  if candidates is empty: return []
  best_score = max(candidates.score)
  winners = [t for (t,s) in candidates if s == best_score]

  // conflict detection
  if len(winners) > 1:
    raise ConflictError(winners)

  // allow lower-priority templates to also apply? optional
  // default: only best applies; to allow multiples, keep all candidates sorted by score.
  return winners

function all_segments_match(path, template):
  for i in range(len(path)):
    pseg = path[i]; tseg = template[i]
    if is_var(tseg): continue
    if pseg != tseg: return false
  return true
```

### Defaults / Execution Notes
- OrderKey: tuple-encoding respecting field order and direction (e.g., big-endian number, length-prefixed strings, direction bit); stable lexicographic sort; include `deleted`/`version` if needed for tie-break; prepend encoding-version byte for cursors to detect stale readers.
- Plan: initial support for prefix + single-field range + order by same key; composite order (field1, field2) planned; StartAfter encodes OrderKey.
- Rebuild: consume Data iterator in batches (e.g., 500); per-collection QPS cap (e.g., 5k upserts/s); max concurrent rebuilds (2 global) with priority to hot collections.
- Buffer: Indexer uses local PebbleDB buffer (4h retention) for rebuild catch-up; live events are buffered during rebuild and replayed after storage scan completes. See [Architecture - Event Buffer](01.architecture.md#event-buffer).
- Health/Stats: expose lag (last applied version vs storage), hit/miss counters, rebuild duration, last error.

#### Rebuild/Apply Ordering
- During rebuild: record buffer position (startKey), scan storage with pagination, then replay buffered events from startKey before marking shard healthy.
- Checkpoint rule: write progress marker after each batch of events is processed and written to local buffer.

#### Lag/Backpressure Thresholds (initial defaults)
- Soft warn lag: 30s or 50k versions behind storage.
- Hard rebuild lag: 2m or 200k versions behind storage → pause deliveries, enqueue rebuild.
- Per-consumer channel: buffer 100 events; overflow triggers pause + rebuild flag.
- Batch apply: up to 256 events or 1MB per batch.

#### Change Stream Distribution & Checkpointing
- Single puller (or small pool) subscribes to storage change stream, then fan-out by collection/partition to index consumers to limit storage load.
- Each consumer maintains its own checkpoint (per collection) persisted externally; puller only guarantees ordered delivery per collection shard.
- Semantics: at-least-once to consumers; index apply must be idempotent and tolerate repeats/older versions; on detected gaps/rewind, consumer triggers rebuild.
- Backpressure: if a consumer lags beyond thresholds, pause its deliveries; mark shard as "needs rebuild" rather than endlessly buffering. Resume from checkpoint after rebuild.
- Batching/coalescing: puller/consumer may batch apply and coalesce multiple events for the same doc to reduce write amplification.

## Dependencies
- Data change stream (via Puller gRPC subscription); index storage backend; clock/metrics; local PebbleDB buffer (reuses `internal/puller/buffer`).

## Testing (testify)
- Upsert/Delete idempotency and tombstone handling.
- Ordering correctness for prefix+range; startAfter behavior.
- Rebuild from iterator with pagination and throttling; gap detection triggers rebuild.
- Health/stats reporting.

## Open Points

### Resolved

| Issue | Resolution |
|-------|------------|
| WAL/buffering strategy during rebuild | Local PebbleDB buffer with 4h retention; no memory WAL overflow risk. See [Architecture - Event Buffer](01.architecture.md#event-buffer). |
| Policy for concurrent rebuilds | Max 2 concurrent rebuilds; 5000 QPS limit; priority for hot collections. |
| StartAfter cursor encoding | Same OrderKey encoding, base64-url on wire; version byte for compatibility. |
| Query-to-Index matching rules | Prefix matching for orderBy; equality filters on leading fields; range filter on last used field. See [Query-to-Index Matching Rules](#query-to-index-matching-rules). |
| Multi-database isolation | Shard key includes database: `(database, normalizedPattern, templateIdentity)`. Same collection pattern in different databases results in separate shards. |
| Puller subscription & recovery | Indexer subscribes via gRPC `Subscribe` RPC with progress marker; auto-reconnect on failure; gap triggers rebuild. See [Architecture - Puller Subscription](01.architecture.md#puller-subscription). |

### Pending

| Issue | Notes |
|-------|-------|
| Exact Plan shape for composite sort | Initial: prefix + single-field range; composite order planned. |
| Null/missing field handling | TBD: sort nulls first or last? Encoding for null marker. |
| Unicode string collation | TBD: byte-order (fast) vs locale-aware (correct)? Likely byte-order for v1. |
| Array field indexing | TBD: multi-value index or reject? |
| Nested field paths | TBD: `data.user.name` extraction from document. |
