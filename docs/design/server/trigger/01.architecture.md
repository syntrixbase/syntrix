# Trigger Architecture

## Overview (Why/How)

- Why: decouple trigger orchestration from transport/storage details and align with storage factory pattern for easier swaps and testing.
- How: Split trigger into two independent services: **Evaluator Service** and **Delivery Service**.

## Service Separation

The trigger system is split into two independent services:

| Service | Responsibility | See Details |
|---------|----------------|-------------|
| **Evaluator** | Watch changes, evaluate conditions, publish tasks | [evaluator/](evaluator/) |
| **Delivery** | Consume tasks, execute HTTP webhooks | [delivery/](delivery/) |

This separation enables:
- Independent scaling of each service
- Distributed deployment configurations
- Clearer code organization and ownership

## Component Boundaries

### Evaluator Service
- DocumentWatcher: wraps Puller subscription + checkpoint persistence.
- CEL Evaluator: condition evaluation with cached programs.
- TaskPublisher: NATS JetStream publisher.

### Delivery Service
- TaskConsumer: JetStream consumer + partitioning.
- Worker Pool: parallel task processing.
- DeliveryWorker: HTTP caller with auth + signature.

## Data Flow

1) DocumentWatcher reads a global resume token from storage, starts consuming from Puller, emits events.
2) TriggerEngine invokes Evaluator per event (filtered by database), builds DeliveryTask when matched.
3) TaskPublisher pushes tasks to NATS `<stream_name>.<database>.<collection>.<docKey>` (stream name is configurable, default `TRIGGERS`).
4) TaskConsumer pulls from the configured stream filtered by `<stream_name>.>`, partitions by collection+docKey, dispatches to DeliveryWorker.
5) DeliveryWorker POSTs to target URL with headers, signature, and optional system token.
6) Checkpoint updated after each processed event using a global key (at-least-once semantics). Note: Puller returns a single aggregated progress token across all databases, so checkpoint is global. The `DatabaseID` in events refers to the Syntrix logical database, not MongoDB instance.
7) Watch scope: default watch all collections; filter events by database field in events. Optionally restrict via include/exclude collection prefixes in config to reduce noise.
8) Checkpoint key: `sys/checkpoints/trigger_evaluator` (global, not per-database). The Puller's progress token is aggregated across all databases. See [evaluator/01.checkpoint.md](evaluator/01.checkpoint.md) for details.

Partitioning and hashing notes

- Partition/ordering and idempotency keys must use the decoded docKey from payload, never the hashed subject segment used to enforce NATS length. Reason: hash fallback can map different docKeys to the same subject fragment; using payload docKey preserves correct ordering and dedupe semantics.
- When subject hashing is triggered, record metrics/counters (publish, consume, retry) with `subjectHashed=true` and log collisions if the hashed subject collides with an existing in-flight key. Reason: hash path is rare and operationally risky; observability is needed to detect routing/ordering anomalies.
- Hash collision handling (low priority): define how the consumer reacts to a detected hash collision (e.g., serialize under a single partition, surface alert, and fail tasks) and the alert threshold. Reason: even low-probability collisions need a deterministic response to avoid silent misrouting.
- Hash branch retry/alert tuning (low priority): consider stricter retry caps/timeouts and faster alerting for hash-path traffic since it is exceptional. Reason: hash path risk profile differs; early detection reduces blast radius.

Recovery/audit notes (low priority)

- "Start from now" must be an explicit, authorized action; log audit records (database, actor, reason) and emit a metric when invoked. Reason: skipping backlog is operationally risky and must be traceable.

## ASCII Module Diagram

```text
+----------------------+      +-------------------+      +--------------------+
| storage.DocumentStore|----->| DocumentWatcher   |----->| TriggerEngine      |
|  (watch/checkpoint)  |      | (resume token)    |      | (evaluate+dispatch)|
+----------------------+      +-------------------+      +---------+----------+
                                                                  |
                                                                  v
                                                        +-----------------+
                                                        | TaskPublisher   | -> NATS JetStream (Configurable Stream)
                                                        +-----------------+
                                                                  |
                                                                  v
                                                        +-----------------+
                                                        | TaskConsumer    | -- partitions --> Worker Pool
                                                        +-----------------+             |
                                                                                         v
                                                                                +----------------+
                                                                                | DeliveryWorker |
                                                                                | (HTTP+signing) |
                                                                                +----------------+
```

## Configuration Surfaces

- TriggerFactory accepts trigger-specific config: NATS stream/consumer names, worker pool size, backoff defaults, HTTP timeouts, signature secret source placeholder, database ID for watch scope and checkpoint key prefix, optional collection include/exclude lists, subject-length guard (hashing toggle) and docKey encoding strategy (base64url without padding), database/collection naming and length validation knobs, and hash-flag emission to payload/metrics.
- External services only pass config + dependencies; they do not call NATS/HTTP directly.

## Compatibility Notes

- Subject scheme is `<stream_name>.<database>.>` (default stream is `TRIGGERS`). This allows stream isolation for testing or multi-database sharding.
- docKey inserted into subjects must use a subject-safe encoding (no `.` `*` `>` wildcards); publisher/consumer must apply the same encoding/decoding path.
- CEL condition semantics remain identical; collection matching still supports glob via `path.Match`.
- Single-stream assumption: monitor per-database traffic and retention; if quotas or noisy-neighbor risk are observed, pivot to per-database streams via config.
- Observability: emit per-database metrics for publish/consume/worker success, retry counts, and latency to detect database-specific regressions.
- Subject-length guard: enforce NATS subject length; hash docKey segment when encoded subject would exceed limit, keeping the original docKey in payload.
- Database/collection naming constraints: validate against allowed charset and length (configurable, default <=128 chars each) to prevent subject overflow and routing issues.
- Secret/token failure handling: if secret fetch or token minting fails, treat as task failure and follow retry policy; do not silently drop.
- Recovery semantics: on missing/cleared checkpoint, start from "now" (no historical replay); allow admin override to resume from a provided token for controlled replay. If checkpoint is missing and `StartFromNow` is false, fail fast to prevent silent data loss.
