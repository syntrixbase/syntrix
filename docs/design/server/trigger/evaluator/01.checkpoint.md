# Checkpoint Design

## Overview

The evaluator service uses a checkpoint system to track progress and enable resume capability after restarts.

## Key Design Decision: Global Checkpoint

The checkpoint uses a **global key** rather than per-database keys.

### Checkpoint Key

```
sys/checkpoints/trigger_evaluator
```

### Consumer ID

```
trigger-evaluator
```

## Why Global (Not Per-Database)?

### 1. Puller Returns Aggregated Progress

The Puller service returns a single progress token that represents the position across ALL MongoDB change streams. This token is not database-specific - it's an opaque MongoDB resume token that tracks the position in the unified change stream.

```
MongoDB Change Stream
       │
       ▼
┌─────────────────┐
│     Puller      │ ──► Single aggregated progress token
└─────────────────┘
       │
       ▼ (all events, all databases)
┌─────────────────┐
│ Trigger Watcher │ ──► Filters by Database field
└─────────────────┘
```

### 2. Database is Logical, Not Physical

The `Database` field in events refers to the **Syntrix logical database** (stored in document metadata), NOT the MongoDB database instance.

All Syntrix documents are stored in a **single MongoDB database**, with the logical database ID stored as metadata within each document:

```go
type StoredDoc struct {
    Id         string                 // Document ID
    Database string                 // Syntrix logical database (NOT MongoDB database)
    Collection string
    Data       map[string]interface{}
    // ...
}
```

### 3. Filtering Happens at Event Level

Database filtering is applied in the watcher **after** receiving events from Puller:

```go
func (w *pullerWatcher) Watch(ctx context.Context) (<-chan events.SyntrixChangeEvent, error) {
    // Load global checkpoint
    checkpointKey := "sys/checkpoints/trigger_evaluator"

    // Subscribe to Puller with global consumer ID
    consumerID := "trigger-evaluator"
    pullerCh := w.puller.Subscribe(ctx, consumerID, resumeToken)

    go func() {
        for pEvent := range pullerCh {
            // Filter by logical database
            if w.database != "" && pEvent.Change.Database != w.database {
                continue  // Skip events for other databases
            }
            // Process matching events...
        }
    }()
}
```

## Checkpoint Storage

### Schema

```go
{
    "token":     string,  // Puller progress token (opaque)
    "updatedAt": int64,   // Unix timestamp
}
```

### Storage Location

- **Database**: `default`
- **Collection**: `sys/checkpoints`
- **Document ID**: `trigger_evaluator`
- **Full Path**: `sys/checkpoints/trigger_evaluator`

## Watcher Behavior

### On Startup

1. Load checkpoint from `sys/checkpoints/trigger_evaluator`
2. If checkpoint exists: resume from that token
3. If checkpoint missing:
   - If `StartFromNow=true`: start from "now" with audit log
   - If `StartFromNow=false`: **fail fast** (error)

### During Operation

After processing each event, save the progress token to checkpoint:

```go
func (w *pullerWatcher) SaveCheckpoint(ctx context.Context, token interface{}) error {
    checkpointKey := "sys/checkpoints/trigger_evaluator"
    data := map[string]interface{}{
        "token":     token,
        "updatedAt": time.Now().Unix(),
    }
    // Update or create checkpoint...
}
```

## Recovery Semantics

| Scenario | Behavior |
|----------|----------|
| Checkpoint exists | Resume from stored token |
| Checkpoint missing, StartFromNow=true | Start from "now", log audit message |
| Checkpoint missing, StartFromNow=false | Fail fast with error |
| Checkpoint corrupted | Treat as missing |

## Audit Logging

When starting from "now" (skipping historical events), the watcher logs:

```
AUDIT: Starting watch from NOW (checkpoint missing)
```

This provides traceability for operational debugging.

## Multi-Instance Considerations

If multiple evaluator instances run simultaneously:

1. All instances share the same checkpoint (global key)
2. This could cause duplicate processing
3. **Recommendation**: Run only one evaluator instance, or implement leader election

## Implementation Reference

See: `internal/trigger/watcher/watcher.go`
