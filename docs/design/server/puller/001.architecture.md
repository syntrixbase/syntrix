# Puller Service Architecture

**Date:** December 29, 2025
**Status:** Design Draft
**Component:** Change Stream Puller (Fan-out Service)

## 1. Overview

The Puller is a standalone service that acts as a centralized fan-out layer between MongoDB and multiple event consumers. It watches MongoDB change streams, normalizes events, manages checkpoints, buffers events in PebbleDB, and distributes events to downstream consumers via gRPC streaming.

### 1.1 Design Goals

- **Single MongoDB Connection**: Minimize load on MongoDB by maintaining only one change stream connection per backend
- **Fan-out**: Distribute events to multiple independent consumers (Engine Index, Streamer, Trigger Service)
- **Reliability**: Ensure no event loss through checkpoint management and resume token persistence
- **Scalability**: Support horizontal scaling (see [future/001.shard-scaling.md](future/001.shard-scaling.md) for shard-based consumer scaling)
- **Observability**: Provide metrics and health checks for monitoring

### 1.2 Position in Architecture

```text
┌─────────────────────────────────────────────────┐
│                   MongoDB                        │
│         (Primary + Replicas, Change Streams)     │
└──────────────────┬──────────────────────────────┘
                   │ (1 change stream per backend)
                   ▼
┌─────────────────────────────────────────────────┐
│              Puller Service                      │
│  ┌──────────────────────────────────────────┐   │
│  │ Change Stream Consumer                   │   │
│  │  • Watch MongoDB                         │   │
│  │  • Handle resume tokens                  │   │
│  │  • Detect gaps                           │   │
│  └──────────┬───────────────────────────────┘   │
│             │                                    │
│  ┌──────────▼───────────────────────────────┐   │
│  │ Event Normalizer                         │   │
│  │  • Convert BSON to internal format       │   │
│  │  • Flatten nested documents              │   │
│  │  • Add metadata (tenant, collection)     │   │
│  └──────────┬───────────────────────────────┘   │
│             │                                    │
│  ┌──────────▼───────────────────────────────┐   │
│  │ Checkpoint Manager                       │   │
│  │  • Track single resume token             │   │
│  │  • Persist to etcd/MongoDB               │   │
│  │  • Recovery on restart                   │   │
│  └──────────┬───────────────────────────────┘   │
│             │                                    │
│  ┌──────────▼───────────────────────────────┐   │
│  │ Event Buffer (PebbleDB)                 │   │
│  │  • Persist events for replay            │   │
│  │  • Support multi-consumer positions     │   │
│  │  • Ordered key storage                  │   │
│  └──────────┬───────────────────────────────┘   │
│             │                                    │
│  ┌──────────▼───────────────────────────────┐   │
│  │ gRPC Server                             │   │
│  │  • Server-side streaming to consumers   │   │
│  │  • Merged event stream from all backends│   │
│  │  • Catch-up coalescing                  │   │
│  └──────────┬───────────────────────────────┘   │
└─────────────┼───────────────────────────────────┘
              │ gRPC Streaming
              │
     ┌────────┼────────┬────────────┬──────────┐
     ▼        ▼        ▼            ▼          ▼
┌─────────┐ ┌──────┐ ┌─────────┐ ┌──────┐  ┌────┐
│ Engine  │ │Stream│ │ Trigger │ │Custom│  │... │
│ (Index) │ │  er  │ │ Service │ │Consum│  │    │
└─────────┘ └──────┘ └─────────┘ └──────┘  └────┘
```

## 2. Core Components

### 2.1 Change Stream Consumer

Establishes and maintains MongoDB change stream connection, handles connection failures and automatic reconnection, applies resume tokens on startup and after failures, and detects gaps in event stream.

### 2.2 Event Normalizer

Converts MongoDB change stream events to internal event format, extracts tenant information, flattens nested document structures, and computes event hashes for deduplication.

### 2.3 Checkpoint Manager

Persists a single resume token for the change stream, recovers resume token on startup, and provides atomic checkpoint updates. Supports both etcd and MongoDB backends. In multi-instance deployments, each instance maintains its own resume token.

### 2.4 Event Buffer (PebbleDB)

Persists events locally for durability and replay. Supports multiple consumers at different positions. Uses ordered key storage for efficient range scans.

### 2.5 gRPC Server

Exposes server-side streaming API for consumers. Streams merged events from all backends. Applies catch-up coalescing when consumer is behind threshold. Consumer manages its own progress (Puller does not store it).

## 3. Canonical Event Schema

**IMPORTANT**: This section defines the Puller event schema. For the complete field naming conventions across all system components, see [Field Naming Conventions](../002_field_naming.md).

### 3.1 JSON Wire Format

```json
{
  "eventId": "unique-event-id",
  "tenant": "tenant-id",
  "collection": "collection-name",
  "documentId": "document-id",
  "operationType": "insert",
  "fullDocument": { },
  "updateDescription": {
    "updatedFields": { },
    "removedFields": ["field1"],
    "truncatedArrays": []
  },
  "clusterTime": { "T": 1234567890, "I": 1 },
  "txnNumber": null,
  "timestamp": 1234567890000
}
```

### 3.2 Field Specifications

| Field | JSON Key | Type | Required | Notes |
| ----- | -------- | ---- | -------- | ----- |
| Event ID | `eventId` | string | Yes | Unique identifier for deduplication |
| Tenant ID | `tenant` | string | Yes | NOT "tenantId" - shorter key |
| Collection | `collection` | string | Yes | Collection name |
| Document ID | `documentId` | string | Yes | NOT "documentKey" |
| Operation Type | `operationType` | string | Yes | lowercase: "insert", "update", "replace", "delete" |
| Full Document | `fullDocument` | object | No | Present for insert/update/replace |
| Update Description | `updateDescription` | object | No | Present for update operations |
| Cluster Time | `clusterTime` | object | Yes | MongoDB timestamp {T, I} |
| Transaction Number | `txnNumber` | int64 | No | Present if in transaction |
| Timestamp | `timestamp` | int64 | Yes | Unix milliseconds |

### 3.3 Operation Types

All operation types are **lowercase**:

- `"insert"` - New document created
- `"update"` - Document fields updated
- `"replace"` - Document replaced entirely
- `"delete"` - Document deleted

### 3.4 Go Type Definition

```go
// NormalizedEvent is the canonical event schema published by Puller.
// All consumers MUST use this definition.
type NormalizedEvent struct {
    // Identity
    EventID      string `json:"eventId"`
    TenantID     string `json:"tenant"`      // JSON: "tenant" (NOT "tenantId")
    Collection   string `json:"collection"`
    DocumentID   string `json:"documentId"`  // JSON: "documentId" (NOT "documentKey")

    // Operation
    Type         OperationType          `json:"operationType"` // lowercase
    FullDocument map[string]interface{} `json:"fullDocument,omitempty"`
    UpdateDesc   *UpdateDescription     `json:"updateDescription,omitempty"`

    // Metadata
    ClusterTime  primitive.Timestamp `json:"clusterTime"`
    TxnNumber    *int64              `json:"txnNumber,omitempty"`
    Timestamp    int64               `json:"timestamp"` // Unix milliseconds
}

type OperationType string

const (
    OperationInsert  OperationType = "insert"
    OperationUpdate  OperationType = "update"
    OperationReplace OperationType = "replace"
    OperationDelete  OperationType = "delete"
)

type UpdateDescription struct {
    UpdatedFields   map[string]interface{} `json:"updatedFields,omitempty"`
    RemovedFields   []string               `json:"removedFields,omitempty"`
    TruncatedArrays []TruncatedArray       `json:"truncatedArrays,omitempty"`
}

type TruncatedArray struct {
    Field   string `json:"field"`
    NewSize int    `json:"newSize"`
}
```

## 4. Event Flow

### 4.1 Normal Flow

1. MongoDB emits change event
2. ChangeStreamConsumer receives event
3. EventNormalizer converts to internal format
4. Event persisted to PebbleDB buffer
5. CheckpointManager saves resume token (async)
6. gRPC Server streams event to subscribed consumers
7. Consumer saves progress marker locally

### 4.2 Startup Recovery Flow

1. Puller starts
2. CheckpointManager loads resume token
3. If resume token exists → resume from token
4. If no resume token → start from "now" (or "beginning" based on bootstrap mode)
5. ChangeStreamConsumer establishes connection
6. Begin processing events

### 4.3 Gap Detection and Recovery

1. GapDetector monitors time between consecutive events in the change stream
2. If gap exceeds threshold (default: 5 minutes):
   - Alert operators via metrics/logs
   - Downstream consumers trigger rebuild
   - Resume from checkpoint after rebuild completes

## 5. gRPC API

### 5.1 Service Definition

A single gRPC server handles all backends. Consumers subscribe to a merged event stream without knowing which backend events come from.

```protobuf
service PullerService {
  // Subscribe to merged event stream from all backends
  rpc Subscribe(SubscribeRequest) returns (stream Event);
}

message SubscribeRequest {
  string consumer_id = 1;          // for logging/monitoring only
  string after = 2;                // progress marker: return events after this (exclusive)
                                   // empty = start from current head (no historical events)
  bool coalesce_on_catch_up = 3;   // enable catch-up coalescing
}

message Event {
  string id = 1;                   // unique event ID
  string tenant = 2;
  string collection = 3;
  string document_id = 4;
  string operation_type = 5;       // insert, update, replace, delete
  bytes full_document = 6;
  bytes update_description = 7;
  ClusterTime cluster_time = 8;    // MongoDB cluster timestamp
  int64 timestamp = 9;             // Unix milliseconds
  string progress = 10;            // current progress marker (opaque, save this)
}

message ClusterTime {
  uint32 t = 1;                    // seconds since epoch
  uint32 i = 2;                    // increment within second
}
```

### 5.2 Progress Marker

The `progress` field in each event is an opaque string that encodes the current position across all backends. Consumers should:

1. Save the `progress` field from the last successfully processed event
2. Pass it as `after` when reconnecting to resume from that point
3. Not attempt to parse or understand its internal structure

### 5.3 Consumer States

- **Real-time**: Position near head, events forwarded as-is
- **Catching-up**: Position behind by `catch_up_threshold`, optional coalescing applied

## 6. Key Design Decisions

### 6.1 PebbleDB Event Buffer

We use PebbleDB as a local event buffer because:

- Durable storage survives Puller restart
- Supports multiple consumers at different positions
- Ordered key storage enables efficient range scans
- LSM tree optimized for write-heavy workloads
- Configurable size limit with automatic cleanup

### 6.2 At-Least-Once Delivery

- Puller persists events to PebbleDB before streaming to consumers
- Consumers must be idempotent (handle duplicate events)
- Events may be delivered multiple times on retry, reconnect, etc.

### 6.3 Catch-up Coalescing

When a consumer falls behind by more than `catch_up_threshold` events:

- Multiple updates to the same document are merged
- insert + delete = skip (cancels out)
- Reduces catch-up time for slow consumers
- Consumer can disable via `coalesce_on_catch_up: false`

## 7. Related Documents

- [Field Naming Conventions](../002_field_naming.md) - **Single source of truth** for field names
- [Design Details](002.design-details.md) - Implementation details, configuration, deployment
- [Shard-Based Consumer Scaling](future/001.shard-scaling.md) - Future enhancement for horizontal scaling
- [Engine Integration with Puller](../engine/007.puller.md) - How Engine consumes from Puller
- [Engine Architecture](../engine/001.architecture.md) - Overall Engine architecture
- [Streamer Architecture](../streamer/001.architecture.md) - Streamer service design
