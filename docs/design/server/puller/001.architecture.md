# Puller Service Architecture

**Date:** December 29, 2025
**Status:** Design Draft
**Component:** Change Stream Puller (Fan-out Service)

## 1. Overview

The Puller is a standalone service that acts as a centralized fan-out layer between MongoDB and multiple event consumers. It watches MongoDB change streams, normalizes events, manages checkpoints, and distributes events to downstream consumers via JetStream.

### 1.1 Design Goals

- **Single MongoDB Connection**: Minimize load on MongoDB by maintaining only one change stream connection per backend
- **Fan-out**: Distribute events to multiple independent consumers (Engine Index, Streamer, Trigger Service)
- **Reliability**: Ensure no event loss through checkpoint management and resume token persistence
- **Scalability**: Support horizontal scaling through partitioning
- **Observability**: Provide metrics and health checks for monitoring

### 1.2 Position in Architecture

```
┌─────────────────────────────────────────────────┐
│                   MongoDB                        │
│         (Primary + Replicas, Change Streams)     │
└──────────────────┬──────────────────────────────┘
                   │ (1 change stream per backend)
                   ▼
┌─────────────────────────────────────────────────┐
│              Puller Service                      │
│  ┌──────────────────────────────────────────┐   │
│  │ Change Stream Consumer                   │   │
│  │  • Watch MongoDB                         │   │
│  │  • Handle resume tokens                  │   │
│  │  • Detect gaps                           │   │
│  └──────────┬───────────────────────────────┘   │
│             │                                    │
│  ┌──────────▼───────────────────────────────┐   │
│  │ Event Normalizer                         │   │
│  │  • Convert BSON to internal format       │   │
│  │  • Flatten nested documents              │   │
│  │  • Add metadata (tenant, collection)     │   │
│  └──────────┬───────────────────────────────┘   │
│             │                                    │
│  ┌──────────▼───────────────────────────────┐   │
│  │ Partitioner                              │   │
│  │  • Partition by (collection, docKey)     │   │
│  │  • Assign partition IDs                  │   │
│  └──────────┬───────────────────────────────┘   │
│             │                                    │
│  ┌──────────▼───────────────────────────────┐   │
│  │ Checkpoint Manager                       │   │
│  │  • Track resume tokens per partition     │   │
│  │  • Persist to etcd/MongoDB               │   │
│  │  • Recovery on restart                   │   │
│  └──────────┬───────────────────────────────┘   │
│             │                                    │
│  ┌──────────▼───────────────────────────────┐   │
│  │ JetStream Publisher                      │   │
│  │  • Publish to subjects by partition      │   │
│  │  • Handle backpressure                   │   │
│  │  • Batch writes                          │   │
│  └──────────┬───────────────────────────────┘   │
└─────────────┼───────────────────────────────────┘
              │ NATS JetStream
              │ Subject: puller.events.{collection}.{partition}
              │
     ┌────────┼────────┬────────────┬──────────┐
     ▼        ▼        ▼            ▼          ▼
┌─────────┐ ┌──────┐ ┌─────────┐ ┌──────┐  ┌────┐
│ Engine  │ │Stream│ │ Trigger │ │Custom│  │... │
│ (Index) │ │  er  │ │ Service │ │Consum│  │    │
└─────────┘ └──────┘ └─────────┘ └──────┘  └────┘
```

## 2. Core Components

### 2.1 Change Stream Consumer

**Responsibilities:**
- Establish and maintain MongoDB change stream connection
- Handle connection failures and automatic reconnection
- Apply resume tokens on startup and after failures
- Detect gaps in event stream

**Interface:**
```go
type ChangeStreamConsumer struct {
    client       *mongo.Client
    dbName       string
    resumeTokens map[int]bson.Raw  // partition -> resume token
    checkpoints  CheckpointStore
}

func (c *ChangeStreamConsumer) Start(ctx context.Context) (<-chan RawEvent, error)
func (c *ChangeStreamConsumer) Resume(ctx context.Context, partition int) error
func (c *ChangeStreamConsumer) Health() HealthStatus
```

**Event Schema (from MongoDB):**
```go
type RawEvent struct {
    OperationType string        // "insert", "update", "replace", "delete"
    ClusterTime   primitive.Timestamp
    TxnNumber     *int64
    FullDocument  bson.M
    DocumentKey   bson.M
    Namespace     struct {
        DB   string
        Coll string
    }
    ResumeToken   bson.Raw
}
```

### 2.2 Event Normalizer

**Responsibilities:**
- Convert MongoDB change stream events to internal event format
- Extract tenant information
- Flatten nested document structures
- Compute event hashes for deduplication

**Interface:**
```go
type EventNormalizer struct{}

func (n *EventNormalizer) Normalize(raw RawEvent) (*NormalizedEvent, error)
```

**Normalized Event Schema:**
```go
type NormalizedEvent struct {
    // Identity
    EventID      string    // Unique event ID
    TenantID     string    // Extracted tenant
    Collection   string    // Collection name
    DocumentID   string    // Document _id

    // Operation
    Type         EventType // INSERT, UPDATE, DELETE
    FullDocument map[string]interface{}

    // Metadata
    ClusterTime  primitive.Timestamp
    TxnNumber    *int64
    ResumeToken  bson.Raw
    Timestamp    int64     // Unix milliseconds

    // Routing
    PartitionKey string    // collection + "/" + documentID
    PartitionID  int       // Computed partition
}

type EventType string
const (
    EventTypeInsert  EventType = "INSERT"
    EventTypeUpdate  EventType = "UPDATE"
    EventTypeDelete  EventType = "DELETE"
)
```

### 2.3 Partitioner

**Responsibilities:**
- Assign events to partitions based on collection and document key
- Ensure consistent partition assignment for same document

**Strategy:**
```go
type Partitioner struct {
    numPartitions int  // Default: 4
}

func (p *Partitioner) GetPartition(collection, documentKey string) int {
    if p.numPartitions == 1 {
        return 0
    }

    key := collection + "/" + documentKey
    hash := fnv.New64a()
    hash.Write([]byte(key))
    return int(hash.Sum64() % uint64(p.numPartitions))
}
```

**Partition Count Guidelines:**
```yaml
# MongoDB Load → Partitions
- < 1k events/sec    → 1 partition
- 1k-5k events/sec   → 4 partitions
- 5k-10k events/sec  → 8 partitions
- > 10k events/sec   → 16 partitions
```

### 2.4 Checkpoint Manager

**Responsibilities:**
- Persist resume tokens per partition
- Recover resume tokens on startup
- Provide atomic checkpoint updates

**Interface:**
```go
type CheckpointStore interface {
    Save(ctx context.Context, partition int, token bson.Raw) error
    Load(ctx context.Context, partition int) (bson.Raw, error)
    List(ctx context.Context) (map[int]bson.Raw, error)
    Delete(ctx context.Context, partition int) error
}
```

**etcd Implementation:**
```go
type EtcdCheckpointStore struct {
    client *clientv3.Client
    prefix string  // "/syntrix/puller/checkpoints/"
}

// Key format: /syntrix/puller/checkpoints/{backend}/{partition}
// Value: Base64-encoded resume token
```

**Checkpoint Policy:**
```go
type CheckpointPolicy struct {
    Interval     time.Duration  // Time-based: every 5 seconds
    EventCount   int           // Event-based: every 1000 events
    OnShutdown   bool          // Always checkpoint on graceful shutdown
}
```

### 2.5 JetStream Publisher

**Responsibilities:**
- Publish normalized events to JetStream
- Handle backpressure from JetStream
- Batch events for efficiency
- Retry on transient failures

**Interface:**
```go
type JetStreamPublisher struct {
    js         nats.JetStreamContext
    streamName string  // "PULLER_EVENTS"
    batcher    *EventBatcher
}

func (p *JetStreamPublisher) Publish(ctx context.Context, evt *NormalizedEvent) error
func (p *JetStreamPublisher) PublishBatch(ctx context.Context, events []*NormalizedEvent) error
```

**JetStream Configuration:**
```go
streamConfig := &nats.StreamConfig{
    Name:       "PULLER_EVENTS",
    Subjects:   []string{"puller.events.>"},
    Retention:  nats.InterestPolicy,  // Delete after all consumers ack
    MaxAge:     10 * time.Minute,     // Keep events for 10 minutes
    Storage:    nats.FileStorage,     // Persistent
    Replicas:   3,                    // HA: 3 replicas
    Discard:    nats.DiscardOld,      // Drop old events when full
}
```

**Subject Pattern:**
```
puller.events.{collection}.{partition}

Examples:
- puller.events.messages.0
- puller.events.messages.1
- puller.events.users.0
- puller.events.orders.3
```

**Batching Strategy:**
```go
type EventBatcher struct {
    maxSize   int           // Max events per batch: 256
    maxWait   time.Duration // Max wait time: 50ms
    maxBytes  int           // Max batch size: 1MB
}
```

## 3. Event Flow

### 3.1 Normal Flow

```
1. MongoDB emits change event
2. ChangeStreamConsumer receives event
3. EventNormalizer converts to internal format
4. Partitioner assigns partition ID
5. JetStreamPublisher publishes to subject
6. CheckpointManager saves resume token (async)
7. Consumers receive from JetStream
```

### 3.2 Startup Recovery Flow

```
1. Puller starts
2. CheckpointManager loads resume tokens
3. For each partition:
   a. If resume token exists → resume from token
   b. If no resume token → start from "now"
4. ChangeStreamConsumer establishes connection
5. Begin processing events
```

### 3.3 Gap Detection and Recovery

```go
type GapDetector struct {
    lastClusterTime map[int]primitive.Timestamp
    maxGap          time.Duration  // 5 minutes
}

func (g *GapDetector) CheckGap(partition int, evt *NormalizedEvent) bool {
    last := g.lastClusterTime[partition]
    current := evt.ClusterTime

    if last.T > 0 {
        gap := time.Duration(current.T-last.T) * time.Second
        if gap > g.maxGap {
            // Gap detected - alert and mark for recovery
            return true
        }
    }

    g.lastClusterTime[partition] = current
    return false
}
```

**Gap Recovery Actions:**
1. Alert operators via metrics/logs
2. Mark partition as "stale"
3. Downstream consumers trigger rebuild
4. Resume from checkpoint after rebuild completes

## 4. Multi-Backend Support

**Configuration:**
```yaml
puller:
  backends:
    - name: default_mongo
      connection: "mongodb://primary:27017"
      database: "syntrix"
      partitions: 4

    - name: vip_mongo_a
      connection: "mongodb://vip-a:27017"
      database: "syntrix_vip"
      partitions: 1
```

**Implementation:**
```go
type Puller struct {
    backends []*Backend
}

type Backend struct {
    name       string
    consumer   *ChangeStreamConsumer
    normalizer *EventNormalizer
    publisher  *JetStreamPublisher
}

func (p *Puller) Start(ctx context.Context) error {
    for _, backend := range p.backends {
        go p.runBackend(ctx, backend)
    }
    return nil
}
```

**Subject Pattern (Multi-Backend):**
```
puller.events.{backend}.{collection}.{partition}

Examples:
- puller.events.default.messages.0
- puller.events.vip_a.messages.0
```

## 5. Backpressure and Flow Control

### 5.1 Backpressure Sources

1. **JetStream slow consumers** → JetStream buffer fills up
2. **Network issues** → Publish latency increases
3. **Consumer lag** → JetStream retention window exceeded

### 5.2 Backpressure Handling

```go
type BackpressureMonitor struct {
    publishLatency  *prometheus.HistogramVec
    queueDepth      *prometheus.GaugeVec
    slowThreshold   time.Duration  // 100ms
    criticalThreshold time.Duration  // 500ms
}

func (b *BackpressureMonitor) HandleBackpressure(latency time.Duration) Action {
    switch {
    case latency < b.slowThreshold:
        return ActionNone
    case latency < b.criticalThreshold:
        return ActionSlowDown  // Reduce batch size
    default:
        return ActionPause  // Pause and alert
    }
}
```

**Actions:**
- **ActionNone**: Normal operation
- **ActionSlowDown**: Reduce batch size, increase wait time
- **ActionPause**: Pause publishing, alert operators

### 5.3 Circuit Breaker

```go
type CircuitBreaker struct {
    state         State  // Closed, Open, HalfOpen
    failureCount  int
    threshold     int           // 5 failures
    timeout       time.Duration // 30 seconds
    lastFailTime  time.Time
}
```

## 6. Observability

### 6.1 Metrics

```go
type PullerMetrics struct {
    // Ingestion
    EventsIngested        prometheus.Counter
    IngestionLatency      prometheus.Histogram
    PartitionLag          *prometheus.GaugeVec  // partition

    // Publishing
    EventsPublished       prometheus.Counter
    PublishLatency        prometheus.Histogram
    PublishErrors         prometheus.Counter
    BatchSize             prometheus.Histogram

    // Checkpoints
    CheckpointsSaved      prometheus.Counter
    CheckpointErrors      prometheus.Counter
    CheckpointLatency     prometheus.Histogram

    // Gaps
    GapsDetected          prometheus.Counter

    // Backpressure
    BackpressureEvents    *prometheus.CounterVec  // level
}
```

### 6.2 Health Checks

```go
// GET /health
type HealthStatus struct {
    Status     string              // healthy, degraded, unhealthy
    Uptime     time.Duration
    Version    string
    Backends   []BackendHealth
}

type BackendHealth struct {
    Name          string
    Status        string
    Partitions    []PartitionHealth
    LastEventTime time.Time
}

type PartitionHealth struct {
    ID     int
    Status string  // healthy, lagging, stale
    Lag    time.Duration
}
```

### 6.3 Logging

**Key Log Events:**
- Startup and shutdown
- Resume token loading and saving
- Gap detection
- Backpressure events
- Connection failures and recoveries
- Partition rebalancing

## 7. Configuration

```yaml
puller:
  # MongoDB Backends
  backends:
    - name: default
      connection: ${MONGO_URL}
      database: syntrix
      partitions: 4

  # JetStream
  jetstream:
    url: ${NATS_URL}
    stream_name: PULLER_EVENTS
    retention: 10m
    replicas: 3
    max_age: 10m

  # Checkpointing
  checkpoint:
    backend: etcd  # etcd or mongodb
    interval: 5s
    event_count: 1000
    etcd:
      endpoints: ["etcd-1:2379", "etcd-2:2379"]
      prefix: /syntrix/puller/checkpoints/

  # Batching
  batch:
    max_size: 256
    max_wait: 50ms
    max_bytes: 1048576  # 1MB

  # Backpressure
  backpressure:
    slow_threshold: 100ms
    critical_threshold: 500ms
    circuit_breaker:
      threshold: 5
      timeout: 30s

  # Observability
  metrics:
    port: 9090
    path: /metrics
  health:
    port: 8080
    path: /health
```

## 8. Deployment

### 8.1 Single Instance (Simple)

```yaml
# For development or low-load environments
replicas: 1
resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 1000m
    memory: 1Gi
```

### 8.2 High Availability (Production)

```yaml
# For production with 4 partitions
replicas: 4  # One per partition
resources:
  requests:
    cpu: 1000m
    memory: 1Gi
  limits:
    cpu: 2000m
    memory: 2Gi

# Anti-affinity to spread across nodes
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - puller
        topologyKey: kubernetes.io/hostname
```

## 9. Testing Strategy

### 9.1 Unit Tests
- Event normalization
- Partition assignment
- Checkpoint save/load
- Batch creation

### 9.2 Integration Tests
- End-to-end: MongoDB → Puller → JetStream → Consumer
- Gap detection and recovery
- Resume from checkpoint
- Multi-backend support

### 9.3 Performance Tests
- Throughput: 10k events/sec
- Latency: p99 < 100ms
- Memory usage under sustained load

### 9.4 Chaos Tests
- MongoDB connection loss
- JetStream unavailable
- etcd failures
- Network partitions

## 10. Migration Path

### 10.1 From Current Architecture

**Current:**
```
Engine → storage.Watch() → MongoDB
Realtime → QueryEngine → CSP → MongoDB
Trigger → storage.Watch() → MongoDB
```

**After Puller:**
```
Engine → JetStream (puller.events.*) ← Puller ← MongoDB
Streamer → JetStream (puller.events.*) ←┘
Trigger → JetStream (puller.events.*) ←┘
```

### 10.2 Migration Steps

1. **Phase 1: Deploy Puller alongside existing services**
   - Puller watches MongoDB
   - Publishes to JetStream
   - Existing services continue unchanged

2. **Phase 2: Migrate Engine Index Layer**
   - Update Engine to subscribe from JetStream
   - Test index builds
   - Keep storage.Watch() as fallback

3. **Phase 3: Migrate Streamer (when implemented)**
   - Streamer subscribes from JetStream
   - Remove CSP service

4. **Phase 4: Migrate Trigger**
   - Update Trigger to subscribe from JetStream
   - Remove direct storage.Watch()

5. **Phase 5: Cleanup**
   - Remove fallback paths
   - Remove CSP service completely
   - Document new architecture

## 11. Related Documents

- [Task 016: Change Stream Puller Implementation](../../../tasks/016.2025-12-29-change-stream-puller.md)
- [Engine Architecture](../engine/001.architecture.md)
- [Streamer Architecture](../streamer/001.architecture.md)
- [Overall Server Architecture](../001_architecture.md)
- [Realtime Watching Mechanism](../006_realtime_watching.md)
