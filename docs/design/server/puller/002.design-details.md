# Puller Service Design Details

**Date:** December 29, 2025
**Status:** Design Draft
**Component:** Change Stream Puller (Fan-out Service)

This document contains implementation details, configuration, and operational guidance for the Puller service. For architecture overview and canonical event schema, see [001.architecture.md](001.architecture.md).

## 1. Component Interfaces

### 1.1 Change Stream Consumer

```go
type ChangeStreamConsumer struct {
    client       *mongo.Client
    dbName       string
    resumeToken  bson.Raw       // single resume token for the change stream
    checkpoints  CheckpointStore
}

func (c *ChangeStreamConsumer) Start(ctx context.Context) (<-chan RawEvent, error)
func (c *ChangeStreamConsumer) Resume(ctx context.Context) error
func (c *ChangeStreamConsumer) Health() HealthStatus
```

**Event Schema (from MongoDB):**

```go
type RawEvent struct {
    OperationType     string        // "insert", "update", "replace", "delete"
    ClusterTime       primitive.Timestamp
    TxnNumber         *int64
    FullDocument      bson.M
    UpdateDescription bson.M        // present for update operations
    DocumentKey       bson.M
    Namespace         struct {
        DB   string
        Coll string
    }
    ResumeToken       bson.Raw
}
```

### 1.2 Event Normalizer

```go
type EventNormalizer struct{}

func (n *EventNormalizer) Normalize(raw RawEvent) (*NormalizedEvent, error)
```

### 1.3 Checkpoint Manager

```go
type CheckpointStore interface {
    Save(ctx context.Context, token bson.Raw) error
    Load(ctx context.Context) (bson.Raw, error)
    Delete(ctx context.Context) error
}
```

**etcd Implementation:**

```go
type EtcdCheckpointStore struct {
    client *clientv3.Client
    prefix string  // "/syntrix/puller/checkpoints/"
}

// Key format: /syntrix/puller/checkpoints/{backend}
// Value: Base64-encoded resume token
```

**Checkpoint Policy:**

```go
type CheckpointPolicy struct {
    Interval     time.Duration  // Time-based: every 1 second
    EventCount   int            // Event-based: every 1000 events
    OnShutdown   bool           // Always checkpoint on graceful shutdown
}
```

### 1.3.1 Pebble Batch Checkpointing

**Why:** Persisting the checkpoint token in the same Pebble batch as the event write gives atomicity between "event stored" and "progress advanced". This avoids a checkpoint moving past an event that was not durable yet, while reducing cross-store coordination.

**How:** Use a reserved key prefix in the per-backend Pebble buffer and update it inside the same batch that writes the event.

```text
Checkpoint key: "__checkpoint__/resume_token"
Checkpoint value: base64-encoded resume token
```

**Write flow:**

1. Build a single Pebble batch.
2. `Set(eventKey, eventValue)`
3. If checkpoint policy triggers for this event, `Set(checkpointKey, token)`
4. `Commit(pebble.Sync)`

**Shutdown flow:**

On graceful shutdown, write the latest checkpoint token to the same Pebble database using a single batch commit (even if no event is being written).

**Notes:**

- Each backend already has its own Pebble buffer directory, so checkpoint keys remain isolated per backend.
- If the policy does not trigger, the event is still durable; the checkpoint will advance on the next policy hit or shutdown.

### 1.4 gRPC Server

A single gRPC server handles all backends. Consumers subscribe to a merged event stream without knowing which backend events come from.

```go
type PullerServer struct {
    backends  map[string]*Backend  // name -> backend (internal)
    coalescer *Coalescer
}

func (s *PullerServer) Subscribe(req *SubscribeRequest, stream PullerService_SubscribeServer) error
```

**gRPC Service Definition:**

```protobuf
service PullerService {
  // Subscribe to merged event stream from all backends
  rpc Subscribe(SubscribeRequest) returns (stream Event);
}

message SubscribeRequest {
  string consumer_id = 1;          // for logging/monitoring only
  string after = 2;                // progress marker: return events after this (exclusive)
                                   // empty = start from current head (no historical events)
  bool coalesce_on_catch_up = 3;   // enable catch-up coalescing
}

message Event {
  string id = 1;                   // unique event ID
  string tenant = 2;
  string collection = 3;
  string document_id = 4;
  string operation_type = 5;       // insert, update, replace, delete
  bytes full_document = 6;
  bytes update_description = 7;
  ClusterTime cluster_time = 8;    // MongoDB cluster timestamp
  int64 timestamp = 9;             // Unix milliseconds
  string progress = 10;            // current progress marker (opaque, save this)
}

message ClusterTime {
  uint32 t = 1;                    // seconds since epoch
  uint32 i = 2;                    // increment within second
}
```

### 1.5 Progress Marker

The progress marker is an opaque string that encodes the position in each backend. Consumers don't need to understand its structure - just save the `progress` field from the last received event and pass it back as `after` when reconnecting.

**Internal Structure:**

```go
type ProgressMarker struct {
    Positions map[string]string `json:"p"` // backend -> last event ID
}

func (m *ProgressMarker) Encode() string {
    data, _ := json.Marshal(m)
    return base64.RawURLEncoding.EncodeToString(data)
}

func DecodeProgressMarker(s string) (*ProgressMarker, error) {
    if s == "" {
        return &ProgressMarker{Positions: make(map[string]string)}, nil
    }
    data, err := base64.RawURLEncoding.DecodeString(s)
    if err != nil {
        return nil, err
    }
    var m ProgressMarker
    if err := json.Unmarshal(data, &m); err != nil {
        return nil, err
    }
    return &m, nil
}
```

**Example:**

```json
// Internal structure
{"p":{"default_mongo":"1735567890-1-a1b2","vip_mongo":"1735567885-3-c3d4"}}

// Encoded progress marker (what consumer sees)
"eyJwIjp7ImRlZmF1bHRfbW9uZ28iOiIxNzM1NTY3ODkwLTEtYTFiMiIsInZpcF9tb25nbyI6IjE3MzU1Njc4ODUtMy1jM2Q0In19"
```

### 1.6 Stream Sender

The stream sender maintains a map of each backend's current position and assembles the progress marker on each send:

```go
type StreamSender struct {
    stream   PullerService_SubscribeServer
    progress map[string]string  // backend -> last event ID
}

func NewStreamSender(stream PullerService_SubscribeServer, initial *ProgressMarker) *StreamSender {
    progress := make(map[string]string)
    if initial != nil {
        for k, v := range initial.Positions {
            progress[k] = v
        }
    }
    return &StreamSender{stream: stream, progress: progress}
}

func (s *StreamSender) Send(evt *Event, backend string) error {
    // 1. Update this backend's progress
    s.progress[backend] = evt.Id

    // 2. Encode current complete progress
    marker := ProgressMarker{Positions: s.progress}
    evt.Progress = marker.Encode()

    // 3. Send to consumer
    return s.stream.Send(evt)
}
```

**Consumer workflow:**

```go
// Consumer side
func (c *Consumer) Run(ctx context.Context) error {
    // 1. Load last saved progress marker
    lastProgress := c.loadProgress()  // consumer stores this itself

    // 2. Subscribe
    stream, _ := c.puller.Subscribe(ctx, &SubscribeRequest{
        ConsumerId: c.id,
        After:      lastProgress,
    })

    // 3. Process events
    for {
        evt, err := stream.Recv()
        if err != nil {
            return err
        }

        // 4. Process event
        if err := c.processEvent(evt); err != nil {
            continue
        }

        // 5. Save progress marker (consumer's responsibility)
        c.saveProgress(evt.Progress)
    }
}
```

## 2. Gap Detection

```go
type GapDetector struct {
    lastClusterTime primitive.Timestamp
    maxGap          time.Duration  // 5 minutes
}

func (g *GapDetector) CheckGap(evt *NormalizedEvent) bool {
    current := evt.ClusterTime

    if g.lastClusterTime.T > 0 {
        gap := time.Duration(current.T-g.lastClusterTime.T) * time.Second
        if gap > g.maxGap {
            // Gap detected - alert and mark for recovery
            return true
        }
    }

    g.lastClusterTime = current
    return false
}
```

**Gap Recovery Actions:**

1. Alert operators via metrics/logs
2. Downstream consumers trigger rebuild
3. Resume from checkpoint after rebuild completes

## 3. Event Buffer (PebbleDB)

Puller uses PebbleDB as a local event buffer to support consumers at different progress levels.

### 3.1 Storage Schema

```go
// Key format: {clusterTime.T}-{clusterTime.I}-{eventId}
// Value: Serialized NormalizedEvent

type EventBuffer struct {
    db       *pebble.DB
    path     string
    maxSize  int64  // 10GB default
}

func (b *EventBuffer) Write(evt *NormalizedEvent) error
func (b *EventBuffer) ReadFrom(position string, limit int) ([]NormalizedEvent, error)
func (b *EventBuffer) Head() string  // Latest position
```

### 3.2 Catch-up Detection

Puller detects when a consumer is catching up (behind by many events) to optionally enable coalescing:

```go
func (b *EventBuffer) EventsBehind(position string) (int64, error) {
    // Count events between position and head
    // Used to determine if consumer is in catch-up mode
}
```

**Note**: Consumer progress is NOT stored by Puller. Consumers are responsible for persisting their own progress marker (the `progress` field from the last received event).

### 3.3 Catch-up Coalescing

When consumer is catching up (behind by `catch_up_threshold` events), coalescing is applied:

```go
type Coalescer struct {
    threshold int  // 100,000 events default
}

// Coalesce rules:
// - update + update → keep latest update
// - insert + update → insert with updated data
// - insert + delete → skip (cancel out)
// - update + delete → keep delete

func (c *Coalescer) Coalesce(events []NormalizedEvent) []NormalizedEvent {
    latest := make(map[string]NormalizedEvent)

    for _, e := range events {
        key := e.TenantID + "/" + e.DocumentID
        prev, exists := latest[key]

        if !exists {
            latest[key] = e
            continue
        }

        switch {
        case e.Type == OperationDelete:
            if prev.Type == OperationInsert {
                delete(latest, key)  // cancel out
            } else {
                latest[key] = e
            }
        case prev.Type == OperationInsert:
            prev.FullDocument = e.FullDocument
            latest[key] = prev
        default:
            latest[key] = e
        }
    }

    return sortByClusterTime(latest)
}
```

### 3.4 Event Cleanup

```go
type Cleaner struct {
    interval  time.Duration  // 1 minute
    retention time.Duration  // 1 hour default
}

func (c *Cleaner) Run(ctx context.Context, buffer *EventBuffer)
```

Cleanup removes events that are older than `retention` duration. Since Puller doesn't track consumer progress, the retention policy is time-based only.

## 4. Multi-Backend Support

A single Puller process can watch multiple MongoDB instances. Each backend has its own change stream consumer and event buffer, but shares a single gRPC server. **Consumers don't know about backends** - they receive a merged event stream.

```yaml
puller:
  backends:
    - name: default_mongo
      connection: "mongodb://primary:27017"
      database: "syntrix"

    - name: vip_mongo_a
      connection: "mongodb://vip-a:27017"
      database: "syntrix_vip"
```

**Implementation:**

```go
type Puller struct {
    backends   map[string]*Backend
    grpcServer *PullerServer  // single gRPC server for all backends
}

type Backend struct {
    name       string
    consumer   *ChangeStreamConsumer
    normalizer *EventNormalizer
    buffer     *EventBuffer  // per-backend buffer at {buffer.path}/{name}/
    checkpoint CheckpointStore
}

func (p *Puller) Start(ctx context.Context) error {
    // Start all backend consumers
    for _, backend := range p.backends {
        go p.runBackend(ctx, backend)
    }
    // Start single gRPC server
    return p.grpcServer.Start(ctx)
}
```

**Consumer Subscription:**

Consumers subscribe to a merged stream without specifying backend:

```go
// Subscribe to merged stream from all backends
req := &SubscribeRequest{
    ConsumerId:        "engine-index",
    After:             lastProgress,  // resume from saved progress
    CoalesceOnCatchUp: true,
}

// Consumer receives events from all backends transparently
// The progress marker encodes positions for each backend internally
```

**Buffer Storage:**

Each backend stores events in its own subdirectory:

- `{buffer.path}/default_mongo/`
- `{buffer.path}/vip_mongo_a/`

### Buffer Batching

**Why:** Batch writes reduce fsync overhead while preserving ordering. A small time window bounds latency and allows a replay window on crash, which is acceptable for consumers that can handle duplicates.

**How:** Each backend buffer runs a write queue with N-event batches or a time window (5ms). A batch commit uses `pebble.Sync` and writes the checkpoint key alongside the last checkpoint-triggering event in the same batch.

```yaml
puller:
  buffer:
    batch_size: 100
    batch_interval: 5ms
    queue_size: 1000
```

## 5. Backpressure and Flow Control

### 5.1 Backpressure Sources

1. **Slow gRPC consumers** → Send buffer fills up
2. **Network issues** → Streaming latency increases
3. **Consumer lag** → Buffer retention window exceeded

### 5.2 Backpressure Handling

```go
type BackpressureMonitor struct {
    publishLatency    *prometheus.HistogramVec
    queueDepth        *prometheus.GaugeVec
    slowThreshold     time.Duration  // 100ms
    criticalThreshold time.Duration  // 500ms
}

func (b *BackpressureMonitor) HandleBackpressure(latency time.Duration) Action {
    switch {
    case latency < b.slowThreshold:
        return ActionNone
    case latency < b.criticalThreshold:
        return ActionSlowDown  // Reduce batch size
    default:
        return ActionPause  // Pause and alert
    }
}
```

**Actions:**

- **ActionNone**: Normal operation
- **ActionSlowDown**: Reduce batch size, increase wait time
- **ActionPause**: Pause publishing, alert operators

### 5.3 Circuit Breaker

```go
type CircuitBreaker struct {
    state         State  // Closed, Open, HalfOpen
    failureCount  int
    threshold     int           // 5 failures
    timeout       time.Duration // 30 seconds
    lastFailTime  time.Time
}
```

## 6. Observability

### 6.1 Metrics

```go
type PullerMetrics struct {
    // Ingestion
    EventsIngested        prometheus.Counter
    IngestionLatency      prometheus.Histogram

    // Publishing
    EventsPublished       prometheus.Counter
    PublishLatency        prometheus.Histogram
    PublishErrors         prometheus.Counter
    BatchSize             prometheus.Histogram

    // Checkpoints
    CheckpointsSaved      prometheus.Counter
    CheckpointErrors      prometheus.Counter
    CheckpointLatency     prometheus.Histogram

    // Gaps
    GapsDetected          prometheus.Counter

    // Backpressure
    BackpressureEvents    *prometheus.CounterVec  // level
}
```

### 6.2 Health Checks

```go
// GET /health
type HealthStatus struct {
    Status     string              // healthy, degraded, unhealthy
    Uptime     time.Duration
    Version    string
    Backends   []BackendHealth
}

type BackendHealth struct {
    Name          string
    Status        string
    LastEventTime time.Time
    Lag           time.Duration
}
```

### 6.3 Key Log Events

- Startup and shutdown
- Resume token loading and saving
- Gap detection
- Backpressure events
- Connection failures and recoveries

## 7. Configuration

Puller references backends defined in `storage.backends` instead of duplicating connection info.

```yaml
puller:
  # gRPC Server
  grpc:
    address: ":50051"
    max_connections: 100

  # Which storage backends to watch (references storage.backends by name)
  # Each backend gets its own change stream, buffer, and checkpoint
  backends:
    - name: default_mongo           # references storage.backends["default_mongo"]
      # Collection filtering (applied via MongoDB aggregation pipeline)
      # Only ONE of include/exclude should be specified
      include_collections: []       # Whitelist: only watch these collections
      exclude_collections:          # Blacklist: watch all except these
        - "_system"
        - "logs"

  # Checkpointing
  checkpoint:
    backend: mongodb  # mongodb or etcd
    interval: 1s
    event_count: 1000

  # Event Buffer (PebbleDB)
  buffer:
    path: /var/lib/puller/events    # per-backend subdirs: {path}/{backend_name}/
    max_size: 10GiB  # supports KiB, MiB, GiB, TiB suffixes

  # Consumer Management
  consumer:
    catch_up_threshold: 100000  # events behind to trigger catch-up mode
    coalesce_on_catch_up: true  # enable coalescing when catching up

  # Event Cleanup
  cleaner:
    interval: 1m
    retention: 1h  # keep events for at least 1 hour

  # Observability
  metrics:
    port: 9090
    path: /metrics
  health:
    port: 8080
    path: /health
```

**Note**: Puller backends reference `storage.backends` by name. The MongoDB connection URI and database name come from the storage configuration, ensuring consistency across the system.

### 7.1 Collection Filtering

Collection filtering uses MongoDB aggregation pipeline at the server side, ensuring:

- Single watch connection (no performance impact)
- Reduced network traffic (filtered events never leave MongoDB)

**Pipeline Generation:**

```go
func buildWatchPipeline(cfg *BackendConfig) mongo.Pipeline {
    if len(cfg.IncludeCollections) > 0 {
        return mongo.Pipeline{
            {{Key: "$match", Value: bson.M{
                "ns.coll": bson.M{"$in": cfg.IncludeCollections},
            }}},
        }
    }
    if len(cfg.ExcludeCollections) > 0 {
        return mongo.Pipeline{
            {{Key: "$match", Value: bson.M{
                "ns.coll": bson.M{"$nin": cfg.ExcludeCollections},
            }}},
        }
    }
    return nil // no filter, watch all collections
}
```

**Notes:**

- If both `include_collections` and `exclude_collections` are specified, `include_collections` takes precedence
- Empty lists mean no filtering (watch all collections)
- Changes to filter config require Puller restart

## 8. Deployment

### 8.1 Single Instance (Development)

```yaml
replicas: 1
resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 1000m
    memory: 1Gi
```

### 8.2 High Availability (Production)

```yaml
# For production
replicas: 2
resources:
  requests:
    cpu: 1000m
    memory: 1Gi
  limits:
    cpu: 2000m
    memory: 2Gi

# Anti-affinity to spread across nodes
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - puller
        topologyKey: kubernetes.io/hostname
```

### 8.3 High Availability Notes

In HA deployments, multiple Puller instances run independently. Each instance:

- Maintains its own MongoDB change stream connection
- Has its own local PebbleDB buffer
- Persists its own resume token

**Progress Marker and HA**:

The composite progress marker (see Section 1.5) encodes positions for all backends. When a consumer reconnects to a different Puller instance:

1. Consumer sends `after` with the saved progress marker
2. New Puller instance parses the marker and resumes from the encoded positions
3. Consumer may receive some duplicate events (at-least-once delivery)

**Consumer idempotency requirement**:

Since consumers may receive duplicate events when switching between Puller instances, they MUST implement idempotent processing:

```go
func (c *Consumer) ProcessEvent(evt *Event) error {
    // Use clusterTime to detect duplicates
    lastSeen := c.getLastSeenTime(evt.DocumentId)
    if compareClusterTime(evt.ClusterTime, lastSeen) <= 0 {
        return nil // Already processed, skip
    }

    // Process event
    if err := c.applyEvent(evt); err != nil {
        return err
    }

    c.updateLastSeenTime(evt.DocumentId, evt.ClusterTime)
    return nil
}
```

**Buffer alignment across instances**:

Different Puller instances may have slightly different buffer contents due to:

- Network latency differences
- Startup time differences
- Cleanup timing differences

This is acceptable because:

1. `clusterTime` ordering is authoritative (from MongoDB)
2. Consumers handle duplicates via idempotency
3. Gaps trigger rebuild (same behavior regardless of which instance)

## 9. Testing Strategy

### 9.1 Unit Tests

- Event normalization
- Checkpoint save/load
- Batch creation
- Coalescer rules

### 9.2 Integration Tests

- End-to-end: MongoDB → Puller → gRPC → Consumer
- Gap detection and recovery
- Resume from checkpoint
- Multi-backend support

### 9.3 Performance Tests

- Throughput: 10k events/sec
- Latency: p99 < 100ms
- Memory usage under sustained load

### 9.4 Chaos Tests

- MongoDB connection loss
- gRPC consumer disconnection
- etcd failures
- Network partitions

## 10. Migration Path

### 10.1 Current Architecture

```text
Engine → storage.Watch() → MongoDB
Realtime → QueryEngine → CSP → MongoDB
Trigger → storage.Watch() → MongoDB
```

### 10.2 Target Architecture

```text
Engine → gRPC ← Puller ← MongoDB
Streamer → gRPC ←┘
Trigger → gRPC ←┘
```

### 10.3 Migration Steps

#### Phase 1: Deploy Puller alongside existing services

- Puller watches MongoDB
- Exposes gRPC API
- Existing services continue unchanged

#### Phase 2: Migrate Engine Index Layer

- Update Engine to subscribe via Puller gRPC
- Test index builds
- Keep storage.Watch() as fallback

#### Phase 3: Migrate Streamer (when implemented)

- Streamer subscribes via Puller gRPC
- Remove CSP service

#### Phase 4: Migrate Trigger

- Update Trigger to subscribe via Puller gRPC
- Remove direct storage.Watch()

#### Phase 5: Cleanup

- Remove fallback paths
- Remove CSP service completely
- Document new architecture

## 11. Related Documents

- [Architecture Overview](001.architecture.md) - High-level architecture and canonical event schema
- [Shard-Based Consumer Scaling](future/001.shard-scaling.md) - Future enhancement for horizontal scaling
- [Engine Integration with Puller](../engine/007.puller.md) - How Engine consumes from Puller
- [Engine Architecture](../engine/001.architecture.md) - Overall Engine architecture
