# Puller Service Design Details

**Date:** December 29, 2025
**Status:** Design Draft
**Component:** Change Stream Puller (Fan-out Service)

This document contains implementation details, configuration, and operational guidance for the Puller service. For architecture overview and canonical event schema, see [001.architecture.md](001.architecture.md).

## 1. Component Interfaces

### 1.1 Change Stream Consumer

```go
type ChangeStreamConsumer struct {
    client       *mongo.Client
    dbName       string
    resumeTokens map[int]bson.Raw  // partition -> resume token
    checkpoints  CheckpointStore
}

func (c *ChangeStreamConsumer) Start(ctx context.Context) (<-chan RawEvent, error)
func (c *ChangeStreamConsumer) Resume(ctx context.Context, partition int) error
func (c *ChangeStreamConsumer) Health() HealthStatus
```

**Event Schema (from MongoDB):**

```go
type RawEvent struct {
    OperationType string        // "insert", "update", "replace", "delete"
    ClusterTime   primitive.Timestamp
    TxnNumber     *int64
    FullDocument  bson.M
    DocumentKey   bson.M
    Namespace     struct {
        DB   string
        Coll string
    }
    ResumeToken   bson.Raw
}
```

### 1.2 Event Normalizer

```go
type EventNormalizer struct{}

func (n *EventNormalizer) Normalize(raw RawEvent) (*NormalizedEvent, error)
```

### 1.3 Partitioner

```go
type Partitioner struct {
    numPartitions int  // Default: 4
}

func (p *Partitioner) GetPartition(tenantID, collection, documentKey string) int {
    if p.numPartitions == 1 {
        return 0
    }

    // Include tenant in hash for tenant isolation
    key := tenantID + "/" + collection + "/" + documentKey
    hash := fnv.New64a()
    hash.Write([]byte(key))
    return int(hash.Sum64() % uint64(p.numPartitions))
}
```

**Partition Count Guidelines:**

| MongoDB Load | Recommended Partitions |
| ------------ | ---------------------- |
| < 1k events/sec | 1 partition |
| 1k-5k events/sec | 4 partitions |
| 5k-10k events/sec | 8 partitions |
| > 10k events/sec | 16 partitions |

### 1.4 Checkpoint Manager

```go
type CheckpointStore interface {
    Save(ctx context.Context, partition int, token bson.Raw) error
    Load(ctx context.Context, partition int) (bson.Raw, error)
    List(ctx context.Context) (map[int]bson.Raw, error)
    Delete(ctx context.Context, partition int) error
}
```

**etcd Implementation:**

```go
type EtcdCheckpointStore struct {
    client *clientv3.Client
    prefix string  // "/syntrix/puller/checkpoints/"
}

// Key format: /syntrix/puller/checkpoints/{backend}/{partition}
// Value: Base64-encoded resume token
```

**Checkpoint Policy:**

```go
type CheckpointPolicy struct {
    Interval     time.Duration  // Time-based: every 5 seconds
    EventCount   int            // Event-based: every 1000 events
    OnShutdown   bool           // Always checkpoint on graceful shutdown
}
```

### 1.5 JetStream Publisher

```go
type JetStreamPublisher struct {
    js         nats.JetStreamContext
    streamName string  // "PULLER_EVENTS"
    batcher    *EventBatcher
}

func (p *JetStreamPublisher) Publish(ctx context.Context, evt *NormalizedEvent) error
func (p *JetStreamPublisher) PublishBatch(ctx context.Context, events []*NormalizedEvent) error
```

**JetStream Configuration:**

```go
streamConfig := &nats.StreamConfig{
    Name:       "PULLER_EVENTS",
    Subjects:   []string{"puller.events.>"},
    Retention:  nats.LimitsPolicy,     // Keep messages until MaxAge
    MaxAge:     10 * time.Minute,      // Keep events for 10 minutes
    Storage:    nats.FileStorage,      // Persistent
    Replicas:   3,                     // HA: 3 replicas
    Discard:    nats.DiscardOld,       // Drop old events when full
}
```

**Batching Strategy:**

```go
type EventBatcher struct {
    maxSize   int           // Max events per batch: 256
    maxWait   time.Duration // Max wait time: 50ms
    maxBytes  int           // Max batch size: 1MB
}
```

## 2. Gap Detection

```go
type GapDetector struct {
    lastClusterTime map[int]primitive.Timestamp
    maxGap          time.Duration  // 5 minutes
}

func (g *GapDetector) CheckGap(partition int, evt *NormalizedEvent) bool {
    last := g.lastClusterTime[partition]
    current := evt.ClusterTime

    if last.T > 0 {
        gap := time.Duration(current.T-last.T) * time.Second
        if gap > g.maxGap {
            // Gap detected - alert and mark for recovery
            return true
        }
    }

    g.lastClusterTime[partition] = current
    return false
}
```

**Gap Recovery Actions:**

1. Alert operators via metrics/logs
2. Mark partition as "stale"
3. Downstream consumers trigger rebuild
4. Resume from checkpoint after rebuild completes

## 3. Multi-Backend Support

```yaml
puller:
  backends:
    - name: default_mongo
      connection: "mongodb://primary:27017"
      database: "syntrix"
      partitions: 4

    - name: vip_mongo_a
      connection: "mongodb://vip-a:27017"
      database: "syntrix_vip"
      partitions: 1
```

**Implementation:**

```go
type Puller struct {
    backends []*Backend
}

type Backend struct {
    name       string
    consumer   *ChangeStreamConsumer
    normalizer *EventNormalizer
    publisher  *JetStreamPublisher
}

func (p *Puller) Start(ctx context.Context) error {
    for _, backend := range p.backends {
        go p.runBackend(ctx, backend)
    }
    return nil
}
```

## 4. Backpressure and Flow Control

### 4.1 Backpressure Sources

1. **JetStream slow consumers** → JetStream buffer fills up
2. **Network issues** → Publish latency increases
3. **Consumer lag** → JetStream retention window exceeded

### 4.2 Backpressure Handling

```go
type BackpressureMonitor struct {
    publishLatency    *prometheus.HistogramVec
    queueDepth        *prometheus.GaugeVec
    slowThreshold     time.Duration  // 100ms
    criticalThreshold time.Duration  // 500ms
}

func (b *BackpressureMonitor) HandleBackpressure(latency time.Duration) Action {
    switch {
    case latency < b.slowThreshold:
        return ActionNone
    case latency < b.criticalThreshold:
        return ActionSlowDown  // Reduce batch size
    default:
        return ActionPause  // Pause and alert
    }
}
```

**Actions:**

- **ActionNone**: Normal operation
- **ActionSlowDown**: Reduce batch size, increase wait time
- **ActionPause**: Pause publishing, alert operators

### 4.3 Circuit Breaker

```go
type CircuitBreaker struct {
    state         State  // Closed, Open, HalfOpen
    failureCount  int
    threshold     int           // 5 failures
    timeout       time.Duration // 30 seconds
    lastFailTime  time.Time
}
```

## 5. Observability

### 5.1 Metrics

```go
type PullerMetrics struct {
    // Ingestion
    EventsIngested        prometheus.Counter
    IngestionLatency      prometheus.Histogram
    PartitionLag          *prometheus.GaugeVec  // partition

    // Publishing
    EventsPublished       prometheus.Counter
    PublishLatency        prometheus.Histogram
    PublishErrors         prometheus.Counter
    BatchSize             prometheus.Histogram

    // Checkpoints
    CheckpointsSaved      prometheus.Counter
    CheckpointErrors      prometheus.Counter
    CheckpointLatency     prometheus.Histogram

    // Gaps
    GapsDetected          prometheus.Counter

    // Backpressure
    BackpressureEvents    *prometheus.CounterVec  // level
}
```

### 5.2 Health Checks

```go
// GET /health
type HealthStatus struct {
    Status     string              // healthy, degraded, unhealthy
    Uptime     time.Duration
    Version    string
    Backends   []BackendHealth
}

type BackendHealth struct {
    Name          string
    Status        string
    Partitions    []PartitionHealth
    LastEventTime time.Time
}

type PartitionHealth struct {
    ID     int
    Status string  // healthy, lagging, stale
    Lag    time.Duration
}
```

### 5.3 Key Log Events

- Startup and shutdown
- Resume token loading and saving
- Gap detection
- Backpressure events
- Connection failures and recoveries
- Partition rebalancing

## 6. Configuration

```yaml
puller:
  # MongoDB Backends
  backends:
    - name: default
      connection: ${MONGO_URL}
      database: syntrix
      partitions: 4

  # JetStream
  jetstream:
    url: ${NATS_URL}
    stream_name: PULLER_EVENTS
    retention: 10m
    replicas: 3
    max_age: 10m

  # Checkpointing
  checkpoint:
    backend: etcd  # etcd or mongodb
    interval: 5s
    event_count: 1000
    etcd:
      endpoints: ["etcd-1:2379", "etcd-2:2379"]
      prefix: /syntrix/puller/checkpoints/

  # Batching
  batch:
    max_size: 256
    max_wait: 50ms
    max_bytes: 1048576  # 1MB

  # Backpressure
  backpressure:
    slow_threshold: 100ms
    critical_threshold: 500ms
    circuit_breaker:
      threshold: 5
      timeout: 30s

  # Observability
  metrics:
    port: 9090
    path: /metrics
  health:
    port: 8080
    path: /health
```

## 7. Deployment

### 7.1 Single Instance (Development)

```yaml
replicas: 1
resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 1000m
    memory: 1Gi
```

### 7.2 High Availability (Production)

```yaml
# For production with 4 partitions
replicas: 4  # One per partition
resources:
  requests:
    cpu: 1000m
    memory: 1Gi
  limits:
    cpu: 2000m
    memory: 2Gi

# Anti-affinity to spread across nodes
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - puller
        topologyKey: kubernetes.io/hostname
```

## 8. Testing Strategy

### 8.1 Unit Tests

- Event normalization
- Partition assignment
- Checkpoint save/load
- Batch creation

### 8.2 Integration Tests

- End-to-end: MongoDB → Puller → JetStream → Consumer
- Gap detection and recovery
- Resume from checkpoint
- Multi-backend support

### 8.3 Performance Tests

- Throughput: 10k events/sec
- Latency: p99 < 100ms
- Memory usage under sustained load

### 8.4 Chaos Tests

- MongoDB connection loss
- JetStream unavailable
- etcd failures
- Network partitions

## 9. Migration Path

### 9.1 Current Architecture

```text
Engine → storage.Watch() → MongoDB
Realtime → QueryEngine → CSP → MongoDB
Trigger → storage.Watch() → MongoDB
```

### 9.2 Target Architecture

```text
Engine → JetStream (puller.events.*) ← Puller ← MongoDB
Streamer → JetStream (puller.events.*) ←┘
Trigger → JetStream (puller.events.*) ←┘
```

### 9.3 Migration Steps

**Phase 1: Deploy Puller alongside existing services**

- Puller watches MongoDB
- Publishes to JetStream
- Existing services continue unchanged

**Phase 2: Migrate Engine Index Layer**

- Update Engine to subscribe from JetStream
- Test index builds
- Keep storage.Watch() as fallback

**Phase 3: Migrate Streamer (when implemented)**

- Streamer subscribes from JetStream
- Remove CSP service

**Phase 4: Migrate Trigger**

- Update Trigger to subscribe from JetStream
- Remove direct storage.Watch()

**Phase 5: Cleanup**

- Remove fallback paths
- Remove CSP service completely
- Document new architecture

## 10. Related Documents

- [Architecture Overview](001.architecture.md) - High-level architecture and canonical event schema
- [Engine Integration with Puller](../engine/007.puller.md) - How Engine consumes from Puller
- [Engine Architecture](../engine/001.architecture.md) - Overall Engine architecture
