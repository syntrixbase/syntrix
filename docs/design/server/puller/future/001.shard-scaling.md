# Shard-Based Consumer Scaling

**Date:** December 30, 2025
**Status:** Future Enhancement
**Component:** Puller Service - Consumer Scaling

## Overview

This document describes a future enhancement to support horizontal scaling of downstream consumers (Engine Index, Streamer, Trigger) through dynamic field-based sharding.

## Problem Statement

When downstream consumers (trigger/indexer/streamer) need to scale horizontally across many machines, each machine processing all events is inefficient:

1. **Redundant Network Traffic**: Each consumer instance receives all events, even those it doesn't need
2. **Wasted Processing**: Each instance must filter out irrelevant events
3. **Scaling Bottleneck**: Single-threaded event processing limits throughput

## Solution: Dynamic Field-Based Sharding

### Concept

Sharding is a **dynamic filter** mechanism where:
- Consumer specifies which field(s) to shard by
- Consumer specifies total shard count and which shard(s) it wants
- **Puller calculates the shard and filters events** before streaming to consumer
- This reduces network traffic significantly

```text
Consumer subscribes with:
  - shard_by: ["tenant", "collection"]  # Fields to hash
  - shard_count: 4                       # Total shards
  - shard_ids: [0, 1]                    # Which shards this consumer wants

Puller receives event:
  - Extracts field values: tenant="acme", collection="orders"
  - Computes: hash("acme/orders") % 4 = 2
  - Shard 2 not in [0, 1] → filter out, don't stream to this consumer
```

### Key Design Principles

1. **Dynamic Definition**: Shard configuration is per-subscription, not global
2. **Field-Based**: Shard key is computed from one or more event fields
3. **Ordered Fields**: `shard_by` is an ordered list - field order affects the hash result
4. **Puller-Side Filtering**: Puller computes shard and filters, reducing network traffic
5. **Like a Filter**: Conceptually similar to a WHERE clause, but based on hash distribution

### Comparison with Static Partitioning

| Aspect | Static Partitioning | Dynamic Sharding |
|--------|---------------------|------------------|
| Configuration | Global (Puller config) | Per-subscription |
| Shard Key | Fixed (tenant + collection) | Consumer-defined fields |
| Flexibility | Low | High |
| Use Case | Fixed infrastructure | Dynamic scaling |

## Implementation Guide

### 1. Update gRPC API

Add shard parameters to SubscribeRequest:

```protobuf
message SubscribeRequest {
  string consumer_id = 1;
  string after = 2;                // progress marker (existing field)
  bool coalesce_on_catch_up = 3;

  // Sharding (optional - for horizontal scaling)
  ShardConfig shard = 4;
}

message ShardConfig {
  repeated string shard_by = 1;    // Ordered list of fields to hash: ["tenant", "collection"]
  int32 shard_count = 2;           // Total number of shards
  repeated int32 shard_ids = 3;    // Which shards this consumer wants
}
```

### 2. Add Shard Calculator in Puller

```go
// internal/puller/shard.go

type ShardCalculator struct{}

// Calculate computes the shard ID for an event based on specified fields.
// shardBy is an ordered list - field order affects the hash result.
// Example: ["tenant", "collection"] produces different hash than ["collection", "tenant"]
func (s *ShardCalculator) Calculate(evt *NormalizedEvent, shardBy []string, shardCount int) int {
    if shardCount <= 1 {
        return 0
    }

    // Build key from specified fields
    var keyParts []string
    for _, field := range shardBy {
        value := s.extractField(evt, field)
        keyParts = append(keyParts, value)
    }
    key := strings.Join(keyParts, "/")

    // Hash and mod
    hash := fnv.New64a()
    hash.Write([]byte(key))
    return int(hash.Sum64() % uint64(shardCount))
}

func (s *ShardCalculator) extractField(evt *NormalizedEvent, field string) string {
    switch field {
    case "tenant":
        return evt.TenantID
    case "collection":
        return evt.Collection
    case "documentId":
        return evt.DocumentID
    default:
        // Extract from fullDocument if it's a nested field
        if val, ok := evt.FullDocument[field]; ok {
            return fmt.Sprintf("%v", val)
        }
        return ""
    }
}
```

### 3. Update gRPC Server to Filter by Shard

```go
func (s *PullerServer) Subscribe(req *SubscribeRequest, stream PullerService_SubscribeServer) error {
    shardConfig := req.Shard
    calculator := &ShardCalculator{}

    for evt := range s.eventChannel {
        // If sharding is configured, filter events
        if shardConfig != nil && shardConfig.ShardCount > 1 {
            shardID := calculator.Calculate(evt, shardConfig.ShardBy, int(shardConfig.ShardCount))
            if !containsShard(shardConfig.ShardIds, shardID) {
                continue  // Filter out - not for this consumer
            }
        }

        // Stream event to consumer
        if err := stream.Send(evt); err != nil {
            return err
        }
    }
    return nil
}

func containsShard(shardIds []int32, id int) bool {
    for _, s := range shardIds {
        if int(s) == id {
            return true
        }
    }
    return false
}
```

### 4. Consumer Configuration Examples

**Engine Index - 2 instances handling different shards:**

```yaml
# engine-0
engine:
  index:
    puller:
      address: "puller:50051"
      consumer_id: "engine-index-0"
      shard:
        shard_by: ["tenant", "collection"]
        shard_count: 2
        shard_ids: [0]

# engine-1
engine:
  index:
    puller:
      address: "puller:50051"
      consumer_id: "engine-index-1"
      shard:
        shard_by: ["tenant", "collection"]
        shard_count: 2
        shard_ids: [1]
```

**Streamer - 4 instances:**

```yaml
# streamer-0
streamer:
  puller:
    consumer_id: "streamer-0"
    shard:
      shard_by: ["tenant"]  # Shard by tenant only
      shard_count: 4
      shard_ids: [0]
```

**Single instance (no sharding):**

```yaml
# No shard config = receive all events
engine:
  index:
    puller:
      address: "puller:50051"
      consumer_id: "engine-index-single"
      # No shard config - receives all events
```

### 5. Flexible Shard Keys

Different consumers can use different shard keys:

```yaml
# Shard by tenant only (all collections for a tenant go to same shard)
shard:
  shard_by: ["tenant"]
  shard_count: 4
  shard_ids: [0]

# Shard by tenant + collection (finer granularity)
shard:
  shard_by: ["tenant", "collection"]
  shard_count: 8
  shard_ids: [0, 1]

# Shard by document field (e.g., region)
shard:
  shard_by: ["region"]  # Extracted from fullDocument.region
  shard_count: 3
  shard_ids: [0]
```

## Ordering Guarantees

- Events with same shard key values always go to same shard
- **Per-shard ordering is preserved**
- Cross-shard ordering is NOT guaranteed
- Consumer can rely on ordered delivery within its shard(s)

## Scaling Patterns

### Pattern 1: Horizontal Scaling

```text
Before: 1 Engine instance processing 10k events/sec (bottleneck)

After:
  Engine-0: shard_ids=[0,1] → 2.5k events/sec
  Engine-1: shard_ids=[2,3] → 2.5k events/sec
  Engine-2: shard_ids=[4,5] → 2.5k events/sec
  Engine-3: shard_ids=[6,7] → 2.5k events/sec
```

### Pattern 2: Tenant Isolation

```text
VIP tenants get dedicated instances:

  Engine-VIP: shard_by=["tenant"], filter tenant in ["acme", "bigcorp"]
  Engine-Standard: shard_by=["tenant"], filter tenant not in ["acme", "bigcorp"]
```

### Pattern 3: Gradual Rollout

```text
Testing new consumer version:

  Engine-Stable (v1): shard_ids=[0,1,2]  # 75% of traffic
  Engine-Canary (v2): shard_ids=[3]      # 25% of traffic
```

## Observability

When implemented, add these metrics:

- `puller_events_by_shard` (counter, labels: consumer_id, shard_id)
- `puller_shard_filter_ratio` (gauge) - % of events filtered out per consumer
- `puller_consumers_by_shard_config` (gauge) - consumers grouped by shard config

## References

- [Puller Architecture](../001.architecture.md) - Current Puller design (without sharding)
- [Puller Design Details](../002.design-details.md) - Implementation details
