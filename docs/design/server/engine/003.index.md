# Index Layer Details

Date: December 26, 2025
Status: Draft
Scope: Index adapters/manager for query engine.

## Why
- Provide accelerated lookup/ordering while keeping storage as truth.
- Allow pluggable index backends (in-memory → RocksDB/external KV) without touching Query logic.

## Responsibilities
- Apply Data change events (upsert/delete/tombstone) idempotently.
- Serve `Search(plan)` for prefix/range (initial), with room for composite order keys.
- Manage rebuild lifecycle (drop/rebuild, checkpoints, throttling).
- Expose health/stats for hit/miss, lag, rebuild progress.

## Interfaces (sketch)
- `Upsert(ctx, evt Event) error`
- `Delete(ctx, evt Event) error`
- `Search(ctx, plan Plan) ([]DocRef, error)`
- `Rebuild(ctx, iter Iterator) error`
- `Health(ctx) (Health, error)` / `Stats(ctx) (Stats, error)`
- `type Event` aligns with Data change event schema.
- `type Plan` captures collection, filters (prefix/range), order fields, limit, startAfter.
- `type DocRef struct { ID string; OrderKey []byte }` // ID is the document id within the collection; fullpath = collection + ID

## Index Templates
- Definition example:
```json
{
  "collectionPattern": "users/{uid}/chats",
  "fields": [
    {"field": "name", "order": "asc"},
    {"field": "age",  "order": "desc"}
  ],
  "options": {
    "unique": false,
    "sparse": false,
    "includeDeleted": true
  }
}
```
- Storage: templates loaded from a static file `config/index/templates.yaml` (default path; overridable via config/env). No live mutation in v1; changes require config update/restart. Keep interface hooks to allow future backing by metadata collection.
- YAML shape (example):
```
templates:
  - name: chats_by_name_age
    collectionPattern: users/{uid}/chats
    fields:
      - { field: name, order: asc }
      - { field: age,  order: desc }
    includeDeleted: true  # fixed true
```
- Pattern rules: path segments only; `{var}` matches one segment; variable *names* are ignored for matching/priority (normalize `{uid}` and `{user_id}` to the same wildcard); no cross-segment wildcards; validate no empty/invalid segments. **Indexes are collection-scoped**: pattern must end at a collection root, not a specific document (e.g., `users/{uid}/chats` is valid collection, `users/{uid}/chats/{chatid}` is invalid because it targets a document level).
- Matching priority: more concrete wins (more fixed segments > fewer placeholders > longer path). Ties → reject unless explicitly resolved (e.g., naming/weight).
- Multiple indexes per collection allowed if patterns are unambiguous; for the same `collectionPattern`, allow multiple templates distinguished by field list/name. Shards must key by `(normalizedPattern, templateIdentity)` (e.g., name or fields signature), not pattern alone.
- Validation: dedupe fields, check order values, forbid duplicate (pattern+fields) definitions; optional name for human reference; includeDeleted is always true (replication needs tombstones).
- Encoding: key uses fields in order; numeric big-endian, strings length-prefixed; may append `deleted`/`version` for tie-break; cursor/startAfter share the same encoding with a version byte (OrderKey encoding versioning). A version bump means the same logical index is rebuilt with a new encoding; it is **not** a new template/index definition.
 - Encoding (OrderKey):
   - Prefix 1-byte encoding version.
   - For each indexed field (in order):
     - string: uvarint length + raw bytes (UTF-8), asc as-is; desc invert bytes to keep lexicographic order.
     - int/float: big-endian sortable representation; desc invert bytes.
     - bool: 0x00 (false), 0x01 (true); desc invert.
     - null: 0x00 marker if needed; keep type ordering stable.
   - Tie-breaker: append doc `ID` (length-prefixed) to ensure deterministic order for equals; optional `deleted`/`version` bits if required for tombstone ordering.
   - Cursor/startAfter: reuse the same encoding, then base64-url encode for wire.
   - Version bump => rebuild existing index with new encoding (no new template); mismatched cursor/index version returns `IndexNotReady`.

OrderKey layout (bytes, left-to-right):
```
[ver:1][field1...][field2...][...][id_len:uvarint][id bytes][optional deleted/version bits]

string field (asc): [len:uvarint][utf8 bytes]
string field (desc): invert each byte after length
number field: big-endian sortable bytes (float via IEEE 754 with sign flip for sorting if needed); desc invert bytes
bool: 0x00/0x01 (desc invert)
```
- Apply: on change event, match collection path to templates and upsert/delete per matched template; on rebuild, scan collections matching the pattern.
- Observability: track template hit rate, rebuild time, failures/disabled state.

### In-Memory Index Shape (default backend)
- Top-level: `map[string]*Shard` keyed by `(normalizedPattern, templateIdentity)` to allow multiple indexes per collectionPattern (collection-scoped only). Example key: `users/*/chats|name:asc,age:desc` (document-level like `users/*/chats/*` is invalid).
- Shard fields (example):
  - `Pattern string` (normalized, e.g., `users/*/chats`).
  - `TemplateID string` (e.g., provided name, else deterministic fields signature like `name:asc,age:desc`).
  - `RawPattern string` (original, e.g., `users/{uid}/chats`).
  - `Tree` (btree or similar) keyed by `OrderKey []byte`, value `DocRef{ID string, OrderKey []byte}`; supports seek/range by OrderKey.
  - `ByID map[string][]byte` to replace existing OrderKey on upsert/delete without full scan (ID is doc id within the collection).
  - `Mu` (RW lock) guarding the shard.
- Sorted structure: balanced tree (btree) to support prefix/range and ordered iteration; allows seek to `startAfter` and forward scan with limit.
- Duplicate handling: on upsert, if `ByID` has prior key, delete old entry from tree then insert new; on tombstone/delete, remove entry (includeDeleted=true remains encoded in OrderKey for ordering when stored).
- Concurrency: per-shard RW lock; applies batch events under write lock; readers take read lock (or snapshot view if backend supports).
- Persistence: in-memory only for initial cut; rebuild on startup from Data iterator; backend is swappable (e.g., RocksDB) without changing Query code.

ASCII shape (illustrative):
```
                   +---------------------------+
                   |     Index Manager         |
                   +------------+--------------+
                                |
                per (pattern, templateIdentity)
                                |
        +-----------------------+-----------------------+
        |                                               |
+----------------------------------------+    +--------------------------+
| Shard: users/*/chats|name:asc,age:desc |    | Shard: orders/*|ts       |
+----------------------------------------+    +--------------------------+
| Pattern: users/*/chats                 |    | Pattern: orders/*        |
| TemplateID: name:asc,age:desc          |    | TemplateID: ts:asc       |
| RawPattern: users/{uid}/chats          |    | RawPattern: orders/{oid} |
| Mu (RW lock)                           |    | Mu (RW lock)             |
| ByID: map[id] -> OrderKey              |    | ByID: map[id]->OrderKey  |
| Tree (btree):                          |    | Tree (btree):            |
|   OrderKey -> DocRef{ID, OrderKey}     |    |   OrderKey -> DocRef     |
+----------------------------------------+    +--------------------------+

OrderKey layout:
[version 1B][field1...][field2...][...][id_len:uvarint][id bytes][optional deleted/version bits]
- strings: len + bytes (desc invert bytes)
- nums: big-endian sortable (desc invert)
- bool: 0x00/0x01 (desc invert)
- cursor/startAfter = same bytes, base64-url on wire
```

### Matching & Conflict Handling
- Matching: split `collection` and `collectionPattern` into segments; `{var}` matches exactly one segment. Reject patterns with empty/invalid segments.
- Priority (higher wins): 1) more fixed segments; 2) longer total segments. If still tied → conflict.
- Conflicts: if two templates match a path with equal priority, treat as conflict and fail apply for that doc (or drop the lower-ranked when explicit weight is provided; default: fail-fast). Templates with the same structural pattern (e.g., `user/{uid}/chats` vs `user/{user_id}/chats`) are considered duplicates and should be rejected at definition time. Emit metrics/logs for conflicts.
- Multiple matches allowed when priorities differ (e.g., more concrete beats generic); each non-conflicting match results in its own index entry.

### Search Execution (pseudocode)
```pseudo
function search(plan):
  shard = lookupShard(plan.collectionPattern, plan.templateIdentity)
  if shard == nil: return ErrIndexNotReady

  (lower, upper) = buildBounds(plan) // prefix/range using OrderKey encoding
  cursorStart = lower
  if plan.startAfter != "":
    // decode cursor; version mismatch handled earlier
    cursorStart = decodeOrderKey(plan.startAfter)

  it = shard.Tree.iterFrom(cursorStart)
  results = []
  while it.valid() and len(results) < plan.limit:
    key, ref = it.current()
    if upper != nil and key > upper: break
    if plan.startAfter != "" and key == cursorStart:
      it.next(); continue  // exclusive startAfter
    results.append(ref)
    it.next()

  return results
```
Notes: `buildBounds` encodes prefix/range per plan fields; Query layer post-filters non-indexable predicates and batches Data fetch by returned IDs. Version check for cursor/orderkey happens before `search` (if mismatch → IndexNotReady).

### Pseudocode (conflict-aware matching)
```pseudo
function score(template):
  fixed = count_non_var_segments(template)
  total = segment_count(template)
  return (fixed, total)  // lexicographic

function matchTemplates(path, templates):
  candidates = []
  for t in templates:
    norm = normalize_vars(t) // replace {anything} with * for duplicate detection
    if seen(norm): raise DuplicateTemplateError(t, norm)
    mark_seen(norm)
    if !segments_same_length(path, t): continue
    if !all_segments_match(path, t): continue  // fixed must equal; var matches any single segment
    candidates.append((t, score(t)))

  // group by best score
  if candidates is empty: return []
  best_score = max(candidates.score)
  winners = [t for (t,s) in candidates if s == best_score]

  // conflict detection
  if len(winners) > 1:
    raise ConflictError(winners)

  // allow lower-priority templates to also apply? optional
  // default: only best applies; to allow multiples, keep all candidates sorted by score.
  return winners

function all_segments_match(path, template):
  for i in range(len(path)):
    pseg = path[i]; tseg = template[i]
    if is_var(tseg): continue
    if pseg != tseg: return false
  return true
```

### Defaults / Execution Notes
- OrderKey: tuple-encoding respecting field order and direction (e.g., big-endian number, length-prefixed strings, direction bit); stable lexicographic sort; include `deleted`/`version` if needed for tie-break; prepend encoding-version byte for cursors to detect stale readers.
- Plan: initial support for prefix + single-field range + order by same key; composite order (field1, field2) planned; StartAfter encodes OrderKey.
- Rebuild: consume Data iterator in batches (e.g., 500); per-collection QPS cap (e.g., 5k upserts/s); max concurrent rebuilds (2 global) with priority to hot collections.
- WAL/buffer: during rebuild, either pause live apply or buffer in a small WAL; replay after rebuild before marking healthy.
- Health/Stats: expose lag (last applied version vs storage), hit/miss counters, rebuild duration, last error.

#### Rebuild/Apply Ordering & WAL
- Option A (default): pause live applies during rebuild; after rebuild completes, resume stream from checkpoint at rebuild start and replay buffered/WAL events before marking healthy.
- Option B: allow buffered WAL during rebuild with size cap (e.g., 10k events or 32MB); on overflow, force rebuild restart.
- Checkpoint rule: write checkpoint only after applying events; for rebuild, set checkpoint to the replayed position after WAL flush.

#### Lag/Backpressure Thresholds (initial defaults)
- Soft warn lag: 30s or 50k versions behind storage.
- Hard rebuild lag: 2m or 200k versions behind storage → pause deliveries, enqueue rebuild.
- Per-consumer channel: buffer 100 events; overflow triggers pause + rebuild flag.
- Batch apply: up to 256 events or 1MB per batch.

#### Change Stream Distribution & Checkpointing
- Single puller (or small pool) subscribes to storage change stream, then fan-out by collection/partition to index consumers to limit storage load.
- Each consumer maintains its own checkpoint (per collection) persisted externally; puller only guarantees ordered delivery per collection shard.
- Semantics: at-least-once to consumers; index apply must be idempotent and tolerate repeats/older versions; on detected gaps/rewind, consumer triggers rebuild.
- Backpressure: if a consumer lags beyond thresholds, pause its deliveries; mark shard as "needs rebuild" rather than endlessly buffering. Resume from checkpoint after rebuild.
- Batching/coalescing: puller/consumer may batch apply and coalesce multiple events for the same doc to reduce write amplification.

## Dependencies
- Data change stream; index storage backend; clock/metrics; optional WAL/buffer.

## Testing (testify)
- Upsert/Delete idempotency and tombstone handling.
- Ordering correctness for prefix+range; startAfter behavior.
- Rebuild from iterator with pagination and throttling; gap detection triggers rebuild.
- Health/stats reporting.

## Open Points
- Exact Plan shape for composite sort; encoding of OrderKey (e.g., tuple-encoded, lexicographically sortable, includes direction bits).
- WAL/buffering strategy during rebuild vs live apply; how to pause/change-apply during drop/rebuild and replay buffered events.
- Policy for concurrent rebuilds and backpressure (per-collection limits, queueing, priority for hot collections).
- StartAfter cursor encoding aligned with OrderKey (stable across index/fallback) and versioned; version mismatch → `IndexNotReady` until rebuild on the existing index completes (no new template; version bump == encoding change, not a new index definition).
