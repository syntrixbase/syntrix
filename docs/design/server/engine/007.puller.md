# Engine Integration with Puller Service

Date: December 29, 2025
Status: Updated - References Standalone Puller Service
Scope: How Query Engine consumes change events from the standalone Puller service for index updates.

**IMPORTANT**: The Puller component has been promoted to a standalone service. See [Puller Service Architecture](../puller/001.architecture.md) for the complete design.

This document describes how the Query Engine integrates with the Puller service for index updates.

## Architecture Overview

```
MongoDB Change Stream
    ↓
Puller Service (Standalone)
    ↓
gRPC Streaming
    ↓
Query Engine Index Consumer
    ↓
Index Updates
```

## Engine's Responsibilities
- Connect to Puller via gRPC and subscribe to event stream
- Consume change events for index updates
- Maintain per-collection checkpoints for rebuild decisions
- Save progress marker after processing events (consumer's responsibility)
- Detect lag/gaps and trigger index rebuild when needed

## Canonical Event Schema

**IMPORTANT**: The canonical event schema is defined in [Puller Service Architecture](../puller/001.architecture.md). All field names and formats below match that definition.

### Event Schema from Puller

Events from Puller follow this JSON schema:

```json
{
  "eventId": "unique-event-id",
  "tenant": "tenant-id",
  "collection": "collection-name",
  "documentId": "document-id",
  "operationType": "insert",
  "fullDocument": { ... },
  "updateDescription": {
    "updatedFields": { ... },
    "removedFields": ["field1"],
    "truncatedArrays": []
  },
  "clusterTime": { "T": 1234567890, "I": 1 },
  "txnNumber": null,
  "timestamp": 1234567890000
}
```

### Field Reference

| Field | Type | Description |
|-------|------|-------------|
| `eventId` | string | Unique event identifier for deduplication |
| `tenant` | string | Tenant ID (NOT "tenantId") |
| `collection` | string | Collection name |
| `documentId` | string | Document ID (NOT "documentKey") |
| `operationType` | string | lowercase: "insert", "update", "replace", "delete" |
| `fullDocument` | object | Complete document (for insert/update/replace) |
| `updateDescription` | object | Delta for updates (updatedFields, removedFields, truncatedArrays) |
| `clusterTime` | object | MongoDB cluster timestamp {T, I} |
| `txnNumber` | int64 | Transaction number (if in transaction) |
| `timestamp` | int64 | Unix milliseconds |

## Integration Pattern

### Consuming from Puller via gRPC

```go
type IndexEventConsumer struct {
    client     pullerv1.PullerServiceClient
    consumerID string
    progress   string  // last saved progress marker
}

func (c *IndexEventConsumer) Subscribe(ctx context.Context) error {
    // Load last saved progress
    c.progress = c.loadProgress()

    // Subscribe to Puller events
    req := &pullerv1.SubscribeRequest{
        ConsumerId:        c.consumerID,
        After:             c.progress,  // resume from last position
        CoalesceOnCatchUp: true,
    }

    stream, err := c.client.Subscribe(ctx, req)
    if err != nil {
        return err
    }

    for {
        evt, err := stream.Recv()
        if err == io.EOF {
            return nil
        }
        if err != nil {
            return err
        }

        if err := c.handleIndexUpdate(ctx, evt); err != nil {
            log.Error("Failed to apply index update: %v", err)
            continue
        }

        // Save progress marker (consumer's responsibility)
        c.saveProgress(evt.Progress)
    }
}

func (c *IndexEventConsumer) handleIndexUpdate(ctx context.Context, evt *pullerv1.Event) error {
    // Decode full document
    var fullDoc map[string]interface{}
    if len(evt.FullDocument) > 0 {
        if err := json.Unmarshal(evt.FullDocument, &fullDoc); err != nil {
            return err
        }
    }

    // Apply to index (must be idempotent)
    return c.index.Upsert(PullerEvent{
        EventID:      evt.Id,
        TenantID:     evt.Tenant,
        Collection:   evt.Collection,
        DocumentID:   evt.DocumentId,
        Type:         evt.OperationType,
        ClusterTime:  evt.ClusterTime,
        FullDocument: fullDoc,
    })
}
```

## Engine Consumer Configuration

```yaml
engine:
  index:
    puller:
      # gRPC connection
      address: "puller:50051"

      # Consumer settings
      consumer_id: "engine-index-{instance-id}"
      coalesce_on_catch_up: true

      # Reconnect settings
      reconnect_backoff_min: 100ms
      reconnect_backoff_max: 30s

      # Progress storage (consumer's responsibility)
      progress_store: etcd  # or file
      progress_key: "/syntrix/engine/index/progress"

      # Rebuild triggers
      lag_thresholds:
        soft_warn: 30s       # Log warning
        hard_rebuild: 2m    # Trigger index rebuild
```

## Index Update Semantics

### At-Least-Once Delivery

- Puller persists events to PebbleDB buffer before streaming to consumers
- Engine consumes via gRPC streaming
- Consumer saves progress marker after successful processing
- Events may be delivered multiple times (on retry, reconnect, etc.)
- **Engine index apply must be idempotent**

### Idempotency Requirements

```go
func (idx *Index) Upsert(evt PullerEvent) error {
    // Check if this event is older than current indexed version
    current := idx.Get(evt.DocumentID)
    if current != nil && compareTimestamp(current.ClusterTime, evt.ClusterTime) >= 0 {
        // Skip older event (idempotent)
        return nil
    }

    // Apply update
    return idx.upsertDocument(evt)
}

// compareTimestamp compares MongoDB timestamps (T=seconds, I=increment)
func compareTimestamp(a, b ClusterTime) int {
    if a.T != b.T {
        if a.T < b.T {
            return -1
        }
        return 1
    }
    if a.I < b.I {
        return -1
    }
    if a.I > b.I {
        return 1
    }
    return 0
}
```

### Ordering Guarantees

- Puller streams events in order based on MongoDB cluster time
- Engine consumer receives events in order via gRPC streaming
- For horizontal scaling, see [Shard-Based Consumer Scaling](../puller/future/001.shard-scaling.md)

## Lag Detection and Rebuild Triggers

### Monitoring Lag

Since Puller doesn't store consumer progress, Engine must calculate lag locally by comparing the timestamp of the last processed event with the current time:

```go
type LagMonitor struct {
    lastEventTime time.Time
    lagThreshold  time.Duration
}

func (m *LagMonitor) UpdateFromEvent(evt *Event) {
    m.lastEventTime = time.UnixMilli(evt.Timestamp)
}

func (m *LagMonitor) CheckLag() time.Duration {
    if m.lastEventTime.IsZero() {
        return 0
    }
    return time.Since(m.lastEventTime)
}
```

### Rebuild Triggers

When lag exceeds thresholds:

1. **Soft Lag (30s)**: Log warning, emit metrics
2. **Hard Lag (2m)**: Trigger index rebuild from scratch

Rebuild process:

1. Stop consuming from Puller gRPC stream temporarily
2. Query Engine scans MongoDB directly (via Data layer)
3. Rebuild index from current state
4. Resume gRPC subscription from latest

## Benefits of Using Standalone Puller

1. **Reduced MongoDB Load**: Single change stream connection (in Puller) instead of one per Engine instance
2. **Independent Scaling**: Puller and Engine can scale independently
3. **Shared Infrastructure**: Other services (Streamer, Trigger) also consume from same Puller
4. **Simplified Engine**: Engine no longer manages MongoDB connections or checkpoints at storage level
5. **Better Observability**: Centralized monitoring in Puller service

## Migration from Direct MongoDB Connection

For existing deployments:

1. Deploy Puller service
2. Configure Engine to connect to Puller's gRPC endpoint
3. Trigger initial index rebuild to sync state
4. Remove direct MongoDB change stream code from Engine
5. Verify lag metrics and performance

## References

- [Puller Service Architecture](../puller/001.architecture.md) - Complete Puller design
- [Query Engine Index Design](003.index.md) - Index structure and operations
- [Query Engine Architecture](001.architecture.md) - Overall Engine architecture
