# Change Stream Puller & Fan-out

Date: December 26, 2025
Status: Draft
Scope: Pull change events from primary storage and fan them out to index consumers with checkpoints, backpressure, and lag handling.

## Why
- Reduce load on primary storage by using a small puller pool instead of many direct consumers.
- Provide ordered delivery per collection/partition while allowing each consumer to progress independently.
- Detect lag/gaps and trigger rebuild instead of overwhelming storage or buffering unboundedly.

## Responsibilities
- Subscribe to storage change stream (single or small pool) and shard events by collection/partition.
- Deliver events to consumers with at-least-once semantics; support small batching and coalescing per document.
- Enforce per-consumer backpressure and lag thresholds; cut over to rebuild when lag is excessive.
- Persist puller checkpoints for recovery; surface lag/health metrics.

## Interfaces (sketch)
- `type Puller interface { Run(ctx); Subscribe(collection) (chan Event, error); Checkpoint() error }`
- Storage stream: `StartStream(ctx, fromCheckpoint) (<-chan Event, error)` (from Data layer or storage adapter).
- Event: reuse Data Event schema (`fullpath`, `collection`, `version`, `deleted`, timestamps, `dataHash`, optional payload).
- Checkpoint: per collection/partition offset (version/ts); persisted in shared store.

## Defaults / Execution Notes
- Topology: 1–N pullers (configurable) for HA; each handles a set of collections/partitions.
- Batching/coalescing: batch size (e.g., up to 256 events or 1MB); coalesce multiple events for same doc before delivery when safe.
- Backpressure: bounded channel per consumer (e.g., 100 events); on overflow/lag > threshold, pause deliveries to that consumer and mark shard for rebuild.
- Lag thresholds: soft (warn) vs hard (force rebuild) based on version/ts delta or time (e.g., 30s warn, 2m rebuild trigger).
- Failover: puller persists checkpoints periodically; on restart, resumes from checkpoint; if storage signals gap/invalid checkpoint, mark shard for rebuild.
- Payload handling: trim to hash by default; allow payload pass-through for debugging/testing; enforce size caps.
- Metrics: events/sec, bytes/sec, lag per collection, dropped-to-rebuild counts.

## NATS (JetStream) Option for Change Stream Fan-out
- Transport: JetStream pull consumer per collection/partition; subjects like `query.index.<collection>` (or shard key). Keep single partition per subject to preserve ordering.
- Semantics: at-least-once with idempotent apply; consumer checkpoint (version/ts) is authoritative for rebuild decisions; JetStream ack only ensures delivery.
- Retention: short window (5–10 minutes) is sufficient given rebuild on lag; set stream retention by time (e.g., 10m) and size to cap cost.
- Flow control: set `max_ack_pending` aligned with consumer buffer (e.g., 100); `ack_wait` tuned to expected apply time (e.g., a few seconds). On timeout/overflow, drop and rebuild.
- Payload size: keep messages small (hash/metadata); avoid large payloads—store payload elsewhere if needed.
- Partitioning: hash by collection or configured shard; ensure single consumer per subject for strict order; multiple pullers claim disjoint subjects.
- Metrics: use JetStream lag + consumer checkpoint lag to drive rebuild decisions.

### JetStream vs Kafka (for this use case)
- Latency: JetStream can stay lower (short retention, small batches, pull-based, memory-friendly); Kafka typically higher due to disk log + replication + larger batch defaults.
- Retention/cost: we only need 5–10 minutes and rebuild on lag—JetStream fits better; Kafka is optimized for long retention and higher storage overhead.
- Simplicity: JetStream is lighter to operate for short-window fan-out; Kafka introduces heavier ops (topic/partition planning, brokers/kraft/ZK, storage tuning).
- Ordering model: both can provide per-partition order; JetStream with single subject/consumer per shard is straightforward; Kafka needs strict partition mapping and single consumer per partition.
- When to prefer Kafka: if requirements shift to long retention, high durability/audit, cross-region replication, or very high multi-tenant throughput—plan a separate design and cost model.

### Semantics & Checkpoint Alignment
- Ordering: single subject per collection/partition with one active consumer to preserve in-subject order; if multiple pullers, use disjoint subject shards.
- Idempotency: index apply must check version/ts and tolerate repeats/older events to allow at-least-once delivery.
- Checkpoints: consumer persists its own checkpoint (version/offset) in external store; JetStream ack only confirms delivery. On restart, resume from checkpoint and stream offset; if gaps/invalid offset, trigger rebuild instead of replaying unknown gaps.

## Policy Values (initial)
- Batch size: 256 events or 1MB (whichever first).
- Consumer buffer: 100 events; overflow → pause + rebuild flag.
- Soft lag: 30s or 50k versions behind.
- Hard lag: 2m or 200k versions behind → rebuild.
- WAL buffer (if enabled): 10k events or 32MB; overflow → restart rebuild.
- Checkpoint cadence: every 1s or every 1k events applied (whichever first).
- JetStream (when used): retention 10m; storage=File; replicas=1 (dev) or 3 (prod); max_ack_pending=200; ack_wait=5s; deliver policy=by-subject partition (single consumer per subject).

## Open Points
- Partitioning scheme across pullers (hash on collection name vs configured shards).
- Checkpoint store choice and format (e.g., kv/etcd/db, version vs wall-clock ts).
- Policy for replay window size before forcing rebuild.
- How to signal consumers about rebuild requirement (control channel vs error return).
