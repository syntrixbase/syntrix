# Query Engine Architecture (Data / Index / Query)

Date: December 26, 2025
Status: Draft
Scope: Internal layout and data flows for the refactored query engine.

## Why
- Enforce encapsulation: only `Service` + factories are public; all implementations move under internal packages.
- Decouple responsibilities so Data (truth), Index (acceleration), and Query (planning/execution) can evolve independently and scale (hot/warm/cold, rebuilds).
- Keep wire contracts stable (HTTP/replication/watch) while allowing backend swaps.

## Public Surface
- Interface: `Service` (existing contract) with explicit `tenant string` on every call; empty tenant resolves to `model.DefaultTenantID` for backward compatibility only.
- Factories: `NewService(storage.DocumentStore, cspURL string) Service` (local/in-process), `NewClient(baseURL string) Service` (remote HTTP), `NewHTTPHandler(s Service) http.Handler` (HTTP adapter).
- No exported structs for Engine/Server/Client.

### Tenant Handling
- Edge validation: HTTP/replication/watch request bodies include `tenant`; reject blank tenant in prod, allow default tenant for dev/tests.
- Isolation: Data/Index/Query store and look up documents, indexes, checkpoints, and change events scoped by tenant. No cross-tenant reads or writes.
- Index partitioning: index namespaces and rebuild state are per-tenant-per-collection; change-stream checkpoints include tenant to avoid leakage during resume.

## Internal Package Layout (proposed)
- `internal/engine/internal/data`: data adapters over `storage.DocumentStore`; emits change events for index.
- `internal/engine/internal/index`: index adapters/manager; supports upsert/delete, search (prefix/range), rebuild.
- `internal/engine/internal/query`: planner/executor; uses index for candidates then fetches via data; handles filters/order/limit/startAfter/showDeleted.
- `internal/engine/internal/http`: handler wiring; uses `Service` only.
- `internal/engine/internal/client`: HTTP client implementation of `Service`.

### Module Details
- Data
        - Responsibilities: CAS-aware CRUD/Patch/Delete on storage; flatten/strip protected fields; prefix scan iterators; change stream emission (fullpath, collection, version, deleted, timestamps, payload hash/payload*); backpressure to protect storage during rebuild scans.
        - Interfaces: `Get/BatchGet/ScanPrefix`, `Create/Update/Patch/Delete` with filters, `Changes(ctx, collection) <-chan Event`.
        - Dependencies: `storage.DocumentStore`; clock for timestamps; optional hash util for dataHash.
        - Tests: CAS/conflict cases, tombstone propagation, iterator pagination, change stream ordering/idempotency.

- Index
        - Responsibilities: maintain derived index from Data events; `Upsert/Delete` doc entries; `Search(plan)` for prefix/range/composite order (initial: prefix + single range); rebuild from Data iterator; track health/stats.
        - Interfaces: `Upsert(evt)`, `Delete(evt)`, `Search(plan) ([]DocRef, error)`, `Rebuild(iter)`, `Health/Stats`.
        - Dependencies: Data change events; local index storage (in-mem/RocksDB/other KV); clock for metrics; optional WAL/buffer.
        - Tests: upsert/delete idempotency, tombstone handling, ordering correctness, rebuild from iterator, gap detection → rebuild trigger.

- Query
        - Responsibilities: plan/execute `model.Query`; pick index vs fallback; batch fetch docs via Data; enforce order/limit/startAfter/showDeleted; surface metrics for hits/misses; provide hooks for replication/watch usage.
        - Interfaces: same as `Service` methods for query paths; internal helpers `executeWithIndex`, `fallbackScan`, `batchFetch`.
        - Dependencies: Index, Data; config (batch size, fallback limits); clock/metrics.
        - Tests: index-hit path ordering/limits, fallback scan bounded behavior, startAfter cursors, showDeleted handling, batch fetch size behavior.

- HTTP
        - Responsibilities: expose `Service` over HTTP (existing routes), streaming watch, replication endpoints; no internal type leakage.
        - Interfaces: `NewHTTPHandler(s Service) http.Handler`; route handlers map 1:1 to Service methods.
        - Dependencies: `Service`; stdlib http; JSON codec.
        - Tests: httptest black-box for each route (status codes, bodies, errors), watch streaming happy/error paths.

- Client
        - Responsibilities: HTTP implementation of `Service`; request/response marshaling; streaming watch client.
        - Interfaces: `NewClient(baseURL string) Service`; internal `post` helper; watch reader.
        - Dependencies: stdlib http; JSON; context.
        - Tests: parity with handler (mock server), error mapping (404→ErrNotFound, 412→ErrPreconditionFailed), watch stream decode.

*Payload in change events may be optional if index rebuild can fetch via iterator; keep hash/fingerprint to detect drift.

## Index Store Placement

## Change Event Schema (Data → Index)
- Event fields (minimum): `tenant`, `fullpath`, `collection`, `version`, `deleted`, `updatedAt`, `createdAt`, `dataHash` (or payload fingerprint), optional `payload` for replay.
- Ordering: per-collection monotonic via storage version/ts; consumers must be idempotent. If gaps detected, schedule rebuild from last safe checkpoint.
- Transport: reuse storage change stream where available (e.g., Mongo change stream); otherwise, poll-based tailing with checkpoints.

## Query Execution Notes
- Index-first path returns ordered doc IDs; Query batches Data fetches (configurable batch size) to avoid N+1 when Index yields many IDs.
- Fallback path (missing/unready/index-version-mismatch) is allowed only when the scanned+fetched documents stay under a config cap (default 500 docs) to avoid storage pressure; otherwise return `IndexNotReady` and surface metrics for rebuild.
- Order semantics: prefer index-provided ordering; if fallback scan is used, enforce ordering in-memory with guardrails (limit/size) and abort once the doc cap is hit.

## Rebuild and Throttling
- Triggers: explicit admin call, detected gap in change stream, index corruption/health check fail, or warm→hot promotion.
- Input: Data prefix iterator with pagination; apply backpressure (max concurrent rebuilds, IO/QPS limits per collection).
- Status: track per-collection rebuild state (idle/rebuilding/failed) and last checkpoint; expose health/stats hooks internally.
- Safety: pause change-application during drop/rebuild window or buffer changes with WAL-like queue; on failure, retry with exponential backoff.

## Watch & Streaming
- Query layer can multiplex either local Data/Index streams or CSP remote watch behind the same interface; HTTP handler remains unchanged.
- If remote CSP watch fails, optionally fall back to local watch when available; surface errors via stream termination/logging.

## Open Items

## Layering & Dependencies
```
           +-----------------+
           |   HTTP Client   |
           +-----------------+
                   |
           +-----------------+        +---------------------+
           |    HTTP Handler | <----> |       Service       |
           +-----------------+        +------------------+--+
                                           |             |
                                    +------+-------+     |
                                    |  Query (exec)|     |
                                    +------+-------+     |
                                           |        G   et by id
                                 +---------+-------+     |
                                 |                 |     v
                           +-----+-----+         +-+--------+
                           |   Index   |         |   Data   |
                           +-----+-----+         +---+------+
                                 |                   |
                        [Index Storage]          [Storage backend]
```

- Allowed deps: Query → Index, Query → Data; Index → Data (read via change stream input only); Data owns storage and emits events. No cycles.

## Data Flows
- Writes: Service → Data (CAS/version/tombstone) → change events → Index upsert/delete.
- Reads: Service → Query → Index (candidate IDs + order) → Data batch fetch → assemble documents; fallback to Data prefix scan if index unavailable. All lookups scoped by tenant.
- Watch: Service → Query → Data/Index stream → HTTP streaming (SSE/long poll) unchanged on the wire; streams are tenant-isolated.
- Replication: Service → Query/Data for pull; push uses Data CAS semantics; checkpoints, change tokens, and deleted flags include tenant.
- Rebuild: Index manager triggers scans via Data prefix iterator; throttling required for large collections (protect hot path).

## Index Scope (initial)
- Must: prefix scan, single-field range, stable ordering; handle deleted/tombstone entries.
- Should: composite order keys (field1, field2) for sort-heavy queries; plan if time allows.
- Nice to have later: selectivity stats for hinting, index suggestion surface.

## Observability
- Counters/latency for Query, Index hit/miss, Data ops; rebuild duration and backlog; watch stream errors.
- Minimal logging hooks retained; metrics interface kept internal to avoid public API expansion.

## Risks & Mitigations
- Index lag/failed updates → fallback scans; schedule rebuilds and expose health.
- Double-writes: avoided by writing Data only; Index fed by events. Ensure event ordering and idempotency in Index adapter.
- Hot/warm/cold: dropping indexes on cold collections requires on-demand rebuild; document SLA and throttle.

## Open Items
- Finalize change event schema (hash algo, payload inclusion, size limits) and gap handling.
- Cache/batching strategy for Query fetching from Data to reduce N+1 when Index returns many IDs.
- Policy for index rebuild trigger (time/size/error-driven) and administrative hooks; throttling defaults.
- Hot/Warm/Cold policies: collection metadata, index drop/rebuild rules, archive/restore flow and user-visible behavior during restore.
- Cursor encoding parity across index/fallback; OrderKey encoding for composite sorts.
- Structured error/response schema for HTTP/Client (without breaking existing clients); timeouts/backoff guidance for watch.
- Change stream distribution: single/pool puller fan-out, per-consumer checkpointing, backpressure/lag thresholds, rebuild-on-lag policy, and idempotent apply requirements.
- Cursor/OrderKey versioning: include encoding version; old cursors should get `IndexNotReady` until index rebuild to new encoding completes.
- Rebuild/apply sequencing: explicit steps (pause or WAL with cap), checkpoint ordering, and failure restart policy.
- Fallback doc-cap policy: config default 500 docs; define estimation vs measured counts and metrics when cap is hit.
- Cold collections: interface hook reserved; current behavior unchanged until implemented.
