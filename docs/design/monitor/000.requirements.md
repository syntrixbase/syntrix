# Monitoring & Observability - Requirements

- Date: 2025-12-27
- Status: Draft
- Scope: Syntrix control-plane and data-plane services (api, query, trigger, storage, identity, console, sdk)
- Owner: TBD (Observability)

## Why
- Minimize mean time to detect and resolve service incidents through consistent visibility across layers.
- Protect availability and latency SLOs for tenant-facing APIs by catching regressions before users notice.
- Provide product and engineering teams with feedback loops on feature usage, performance, and capacity.

## How (at the requirement level)
- Adopt end-to-end OpenTelemetry instrumentation for metrics, traces, and structured logs.
- Standardize resource naming, labels, and correlation identifiers to enable joinable telemetry across signals.
- Use a common collector pipeline to manage sampling, enrichment, routing, and egress to storage/alerting backends.

## Goals
- Every public-facing request and background job is observable via metrics, traces, and logs with shared IDs.
- SLOs defined for availability and latency per key surface (REST, replication, triggers, realtime, identity).
- Alerting is actionable with clear runbooks and paging thresholds tied to SLO burn-rate policies.
- Dashboards exist per service and per tenant slice with at-a-glance health and drill-down paths.
- Telemetry pipelines are resilient: backpressure, retry, and graceful degradation without breaking prod traffic.

## Non-Goals
- Replacing vendor-managed monitoring stacks used by customers; we focus on Syntrix-operated services.
- Building a custom tracing protocol; we rely on OpenTelemetry and supported exporters.
- Deep APM auto-instrumentation for every language; we prioritize Go services and the JS/TS SDK first.

## Functional Requirements

### Metrics
- Capture RED (rate, errors, duration) for HTTP/gRPC/WebSocket endpoints.
- Capture USE (utilization, saturation, errors) for critical resources: CPU, memory, goroutines, DB connections, queue depths.
- Domain metrics: replication lag, trigger dispatch latency, query planner duration, storage read/write latency, identity auth success/failure rate.
- Include `tenant_id`, `service`, `endpoint`, `region`, `version`, and `instance` labels; enforce low-cardinality guardrails.

### Traces
- Propagate W3C Trace Context across HTTP/gRPC/WebSocket and to SDK clients; record spans for network, storage, query planning/execution, trigger dispatch, and replication steps.
- Add span attributes for `tenant_id`, `request_id`, `user_id` (when allowed), and `plan_id` for queries.
- Tail-sample high-volume surfaces with policy: default 10% for healthy traffic, 100% for errors, 100% for latency p95>target.

### Logs
- Structured JSON logs with `timestamp`, `severity`, `service`, `tenant_id`, `trace_id`, `span_id`, `request_id`, and message; no plain-text free-form logs.
- PII controls: redact secrets/tokens; hash identifiers when not required in raw form.
- Log levels: `INFO` for lifecycle, `WARN` for recoverable issues, `ERROR` for user-impacting failures; sampling on noisy categories.

### Alerting and SLOs
- Define SLOs per surface: availability (99.9% api, 99.5% realtime), latency (p95 under target per operation), replication freshness (lag < threshold), trigger dispatch success rate.
- Burn-rate multi-window policies (e.g., 2h/6h, 1h/24h) to page on fast leaks and warn on slow leaks.
- Alerts include runbook links, recent deploy info, and top correlated signals (error class, tenant, region).

### Dashboards and Reporting
- Service owner dashboards: golden signals, saturation, dependency health, deploy markers.
- Tenant views: per-tenant error/latency, throttling, and resource usage.
- Executive/ops rollups: uptime, incident counts, MTTR/MTTD, cost-of-telemetry reporting.

### Runbooks and On-call
- Every paging alert links to a runbook with mitigation steps and ownership.
- Post-incident review captures gaps and feeds back into instrumentation and alert tuning.

### Multi-tenant and Isolation
- Mandatory `tenant_id` label on metrics/logs/traces; validate in collectors to reject unlabeled data.
- Card limits per tenant on log volume and trace sampling to prevent noisy-neighbor impact.

### Sampling and Retention
- Default retention: metrics 30-90 days (high-res 15d, downsampled beyond), traces 7-14 days (indexed spans for 30d on errors), logs 14-30 days with archive to cold storage.
- Sampling policies configurable per environment (dev/stage/prod) and per tenant tier.

## Non-Functional Requirements
- Overhead: instrumentation adds <5% CPU and <10% latency overhead on critical paths; fail-open if collector unreachable.
- Telemetry pipeline SLO: collectors and storage targets at least 99.9% ingestion availability.
- Security: TLS in transit; authZ for pushing telemetry; scoped API tokens for dashboards and alert webhooks.
- Privacy: field-level redaction and schema validation for logs; audit trail for access to telemetry data.

## Integrations and Tooling
- OpenTelemetry SDKs for Go services and JS/TS SDK; OTel Collector as the central router.
- Storage backends: Prometheus (metrics), Tempo/Jaeger (traces), Loki or ClickHouse (logs), Alertmanager + ChatOps/Pager.
- Visualization: Grafana (dashboards) with recording rules and exemplars linking to traces.

## Open Questions
- Finalize logging backend choice (Loki vs ClickHouse) based on retention/cost benchmarks.
- Do we need per-tenant custom SLOs exposed in the console UI?
- Should we index select span attributes (tenant, query plan id) for faster trace search?
