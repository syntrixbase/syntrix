# Monitoring & Observability - Architecture

- Date: 2025-12-27
- Status: Draft
- Scope: Syntrix control-plane and data-plane services
- Owner: TBD (Observability)

## Why
- Provide a consistent, low-friction path to capture and correlate metrics, traces, and logs across all services.
- Enable fast triage (reduce MTTD/MTTR) and capacity planning through a shared, resilient telemetry pipeline.

## Architecture Overview
The design uses OpenTelemetry everywhere with a central collector tier that fans out to purpose-built backends. Fail-open behavior keeps production traffic flowing if telemetry degrades.

### High-level flow
```
[Clients/SDK] --traceparent--> [API/Query/Trigger/Storage/Identity Services]
      | metrics/logs/traces (OTLP)
      v
+--------------------+
| OTel Collectors    |  <- HA pair per region
| - receivers: otlp  |
| - processors:      |
|   batch, attrs,    |
|   tail-sampling    |
| - exporters:       |
|   prometheus,      |
|   loki/clickhouse, |
|   tempo/jaeger     |
+---------+----------+
          | (metrics)
          v
   +--------------+        +---------------+
   | Prometheus   |<-------+ Recording     |
   | (scrape/OTLP)|        | Rules         |
   +------+-------+        +-------+-------+
          | (alerts)               |
          v                        v
    +-----------+           +--------------+
    | Alertmgr  |---------> | Pager/ChatOps|
    +-----------+           +--------------+

          | (traces)                 | (logs)
          v                          v
   +--------------+            +--------------+
   | Tempo/Jaeger |            | Loki/ClickH. |
   +------+-------+            +------+-------+
          \                        /
           \______ Grafana _______/
```

## Data Flow and Components
- **Instrumentation (How)**: OpenTelemetry SDKs in Go services and JS/TS SDK; standard attributes: `service`, `version`, `tenant_id`, `region`, `request_id`, `trace_id`, `plan_id` (queries), `user_id` when allowed.
- **Context propagation (How)**: W3C Trace Context across HTTP/gRPC/WebSocket; outbound calls (storage, identity, trigger, query, replication) inject headers; console and SDK propagate `traceparent`.
- **Collectors (How)**: HA pair per region; batch processor to reduce export chatter; attribute processor to enforce required labels and drop high-cardinality fields; tail-sampling for error and high-latency traces; rate limits per tenant to protect pipeline.
- **Metrics path (How)**: OTLP to collectors -> Prometheus remote-write; recording rules for SLO windows; exemplars attached to latency histograms to enable trace jumps in Grafana.
- **Traces path (How)**: OTLP -> tail-sampling -> Tempo/Jaeger; store traces 7-14 days; index `tenant_id`, `error`, and `plan_id` fields for search.
- **Logs path (How)**: OTLP logs -> Loki or ClickHouse; schema validation ensures JSON structure; redaction middleware strips secrets; retention 14-30 days with cold storage archive.
- **Alerting (How/Why)**: Alertmanager receives Prometheus alert rules; paging (P1/P2) for SLO burn-rate breaches; notification templates include runbook link, recent deploy, top offenders (tenant/endpoint), and trace exemplars.
- **Dashboards (How)**: Grafana with service owner views (RED/USE + dependencies), tenant slices, and executive rollups; deployment markers from CI/CD for correlation.
- **Runbooks and ownership (How)**: Each alert rule references a runbook in docs/operations (to be added) and an owning team; incident timeline auto-links to traces/logs.

## SLOs and Alert Policies
- API availability 99.9%, realtime 99.5%; latency targets per operation (e.g., CRUD p95 < 200ms, query p95 < 400ms).
- Replication freshness: lag < 5s p95; trigger dispatch success > 99%.
- Multi-window burn-rate examples: page when 2h/6h burn-rate > 14x budget; warn when 1h/24h > 2x.

## Multi-tenant Handling
- Mandatory `tenant_id` label; collectors reject or rewrite unlabeled data.
- Per-tenant sampling and log volume caps; per-tenant dashboards and SLO views.

## Deployment and Operations
- Environments: dev/stage/prod with progressively stricter sampling and alerting; feature flags for instrumentation modules.
- Collectors deployed with autoscaling and pod disruption budgets; at least two replicas per region/zone.
- Backpressure: drop telemetry (fail-open) when exporters unavailable; emit self-metrics on queue depth and dropped spans.

## Failure Modes and Mitigations
- Collector outage: traffic continues, spans sampled locally and retried; alerts on ingestion drop and queue backlog.
- Backend outage (Prom/Tempo/Loki): degrade to minimal sampling; store-and-forward buffer (bounded) on collectors; notify owners.
- High-cardinality explosion: attribute processor drops offending labels and emits warning metric; playbook to fix source.
- Misconfigured sampling: detection via mismatch between request rate and span rate; default to safer 100% error sampling.

## Rollout Plan (How)
- Phase 1: Instrument core API/Query services and JS/TS SDK; ship collector stack in stage.
- Phase 2: Add storage, trigger, replication paths; enable dashboards and exemplars; tune sampling and recording rules.
- Phase 3: Harden alerting (burn-rate), add runbooks, enable per-tenant caps; production rollout per region.

## Open Questions
- Choose final log backend (Loki vs ClickHouse) and archive strategy.
- Do we need custom dashboards in the console for tenants or is Grafana embedding sufficient?
- Should we support remote write to customer-owned telemetry stacks for dedicated tenants?
