# Monitoring & Observability - Pipeline and Controls

- Date: 2025-12-27
- Status: Draft
- Scope: Collector pipeline, instrumentation controls, telemetry schemas, alert/runbook plumbing
- Owner: TBD (Observability)

## Why
- Turn the high-level architecture into actionable, configurable components with guardrails for multi-tenant operation.
- Reduce noise and cardinality risk while preserving the ability to debug tenant-facing incidents quickly.
- Ensure every signal (metrics, traces, logs) is correlated via shared identifiers and consistent schemas.

## Core Decisions (How/Why)
- **Telemetry stack**: OpenTelemetry SDKs + OTel Collector (HA) + Prometheus + Tempo/Jaeger + Loki/ClickHouse + Alertmanager + Grafana. Chosen for ecosystem maturity and open standards.
- **Fail-open**: Services must never block request paths on telemetry export; collectors buffer with bounded queues and drop oldest with counters if backends fail.
- **Label discipline**: Enforce mandatory attributes (`service`, `tenant_id`, `region`, `version`, `instance`, `request_id`, `trace_id`) in collectors; reject or rewrite non-compliant telemetry.
- **Sampling**: Tail sampling based on errors and latency; head sampling only for extremely high-volume tenants/endpoints.
- **Schema-first logging**: Only structured JSON logs that pass schema validation and redaction middleware.

## Pipeline Topology
```
[Service Pods] -- OTLP (grpc/http) --> [OTel Collector DaemonSet]
    | batch + attr + tail-sample
    | exporters: promremotewrite, loki/clickhouse, tempo/jaeger
    v
[Backends]
  - Metrics: Prometheus (+ recording rules)
  - Traces: Tempo/Jaeger
  - Logs: Loki or ClickHouse
  - Alerts: Alertmanager -> Pager/ChatOps
  - Viz: Grafana (exemplars enabled)
```

## Collector Configuration (How)
- **Receivers**: `otlp` (grpc/http); `prometheus` only for self-metrics.
- **Processors**:
  - `batch` (200ms/5k spans) to reduce export overhead.
  - `attributes` to inject required labels, drop high-cardinality fields, and normalize naming.
  - `tail_sampling` policies:
    - Error sample: 100% when `status_code != OK`.
    - Slow sample: 100% when span latency > target p95 for the operation.
    - Baseline sample: 10% default; per-tenant overrides allowed.
    - Max per-tenant rate guardrail (e.g., 200 spans/sec) to prevent noisy-neighbor.
  - `memory_limiter` sized to 30% of pod memory with safety margin.
- **Exporters**:
  - `prometheusremotewrite` to Prom; histograms use native OTLP buckets with exemplars.
  - `otlp` traces to Tempo/Jaeger (TLS).
  - `loki` or `clickhouse` for logs with `tenant_id` as partition key.
- **Extensions**: health_check, pprof, zpages for debugging; `bearertokenauth`/mTLS between services and collectors.

## Instrumentation Contracts (How)
- **Context propagation**: W3C Trace Context; include `baggage` for `tenant_id` and `plan_id` (queries) when safe.
- **Span attributes (must)**: `service.name`, `service.version`, `deployment.environment`, `tenant_id`, `region`, `request_id`, `trace_id`, `span.kind`, `http.method`, `http.route`, `db.system`, `db.operation`, `messaging.system` as applicable.
- **Metrics conventions**:
  - RED for APIs: `http_server_duration_seconds` (histogram, exemplars), `http_requests_total`, `http_request_errors_total`.
  - USE for infra: `cpu_usage`, `memory_usage`, `goroutines`, `db_pool_in_use`, `queue_depth`.
  - Domain: `replication_lag_seconds`, `trigger_dispatch_latency_seconds`, `query_planner_duration_seconds`, `storage_read_latency_seconds`, `identity_auth_success_total`.
  - Labels kept low-cardinality: `tenant_id`, `service`, `endpoint`, `region`, `version`, `status_code_class`.
- **Logs schema** (JSON): `timestamp`, `severity`, `service`, `tenant_id`, `trace_id`, `span_id`, `request_id`, `category`, `message`, `error.class`, `error.stack` (optional), `fields` (map). Redact secrets; hash sensitive IDs when not required.

## Alert Policies (Why/How)
- **SLO burn-rate** (Prometheus):
  - Page: 2h/6h > 14x budget; Warn: 1h/24h > 2x budget.
  - Surfaces: API availability 99.9%, realtime 99.5%, CRUD p95 < 200ms, query p95 < 400ms, replication lag p95 < 5s, trigger success > 99%.
- **Guardrail alerts**: collector queue depth, dropped spans/logs, Prom/Tempo/Loki scrape failures, exemplar missing ratio, sampling mismatch (request rate vs span rate).
- **Templates**: include runbook link, recent deploy SHA, top tenants/endpoints, exemplar trace link.

## Dashboards (How)
- **Service owner**: RED/USE, dependency health, deploy markers, error budget burn, exemplar jump-to-trace.
- **Tenant slice**: latency/error by tenant, throttling counts, sampled trace rate, log volume by tenant.
- **Ops rollup**: uptime, incident counts, MTTR/MTTD, telemetry cost per GB/day.

## Runbooks and Operations (How)
- Runbooks stored in docs/operations (to add) and linked from alerts.
- Standard steps: check deploys, inspect burn-rate panel, jump to exemplar trace, pivot to logs, mitigate (rollback/feature-flag/shed load), record timeline.
- Post-incident: update sampling/alerts and add missing telemetry attributes.

## Security and Privacy Controls
- mTLS between services and collectors; auth tokens rotated.
- Field-level redaction in log processors; collectors validate schema and reject non-JSON logs.
- Access control on Grafana/Tempo/Loki with tenant-aware RBAC where possible; audit log of queries.

## Rollout Steps
1) Stage: enable OTel SDK for api/query/trigger/storage/identity; deploy collector with conservative sampling; validate schemas.
2) Add exemplars + dashboards; tune buckets and recording rules; add burn-rate alerts and runbooks.
3) Prod canary: one region with per-tenant rate caps; monitor collector drops; then expand.
4) Backfill docs: console integration story (embed Grafana or link-outs), tenant SLO exposure decision.

## Open Questions
- Finalize log backend choice (Loki vs ClickHouse) and archive strategy.
- Need per-tenant custom SLOs surfaced in console?
- Should we allow tenant-provided OTLP endpoints for dedicated tiers?
