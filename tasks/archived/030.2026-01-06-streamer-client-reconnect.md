# Task 030: Streamer Client Auto-Reconnect Mechanism

**Date**: 2026-01-06
**Status**: Completed
**Priority**: High

## Overview

Implement automatic reconnection with subscription state recovery for the Streamer gRPC client (`internal/streamer/client.go`). Currently, the client has reconnect configuration fields (`ReconnectInterval`, `HeartbeatInterval`) but no actual implementation.

## Implementation Summary

### Completed Features

1. **Connection State Management**: Added `ConnectionState` enum with `Disconnected`, `Connecting`, `Connected`, `Reconnecting` states
2. **State Change Callback**: `ClientConfig.OnStateChange` callback notifies callers of state transitions
3. **Exponential Backoff with Jitter**: `reconnect()` uses configurable backoff (1s→30s) with ±20% jitter
4. **Subscription Recovery**: `resubscribeAll()` re-establishes all active subscriptions after reconnect
5. **Client Heartbeat**: `heartbeatLoop()` sends periodic heartbeats to keep connection alive
6. **Activity Monitoring**: `activityMonitor()` detects stale connections and triggers reconnect
7. **Max Retries Support**: Configurable limit (0=unlimited) for reconnection attempts

### Configuration Fields (Updated)

```go
type ClientConfig struct {
    StreamerAddr      string         // gRPC server address
    InitialBackoff    time.Duration  // Initial wait before reconnect (default: 1s)
    MaxBackoff        time.Duration  // Maximum wait between attempts (default: 30s)
    BackoffMultiplier float64        // Backoff growth factor (default: 2.0)
    MaxRetries        int            // Max reconnect attempts, 0=unlimited
    HeartbeatInterval time.Duration  // Heartbeat send interval (default: 30s)
    ActivityTimeout   time.Duration  // Stale connection timeout (default: 90s)
    OnStateChange     StateChangeCallback // State change notification
}
```

### Key Implementation Details

- `subscriptionInfo` struct stores tenant, collection, filters for each subscription
- On reconnect, subscriptions are automatically restored with original IDs
- Thread-safe access to all shared state using `sync.RWMutex`
- All nil-safety checks for unit testing without real gRPC connections

### Test Coverage

- All existing tests pass
- Added 8 new unit tests for connection state, heartbeat, and reconnect logic
- Total coverage: 94.0%

## Background & Analysis

### Current State

The `streamerClient` in `client.go`:
1. **Configuration**: Has `ReconnectInterval` (default 5s) and `HeartbeatInterval` (default 30s) but **neither is used**
2. **Connection**: Creates gRPC connection once in `NewClient()`, no reconnect on failure
3. **Stream**: `remoteStream.recvLoop()` exits on error without attempting to reconnect
4. **Subscriptions**: Lost on disconnect, no recovery mechanism

### Comparison with Similar Components

| Component | Transport | Reconnect | State Recovery | Heartbeat |
|-----------|-----------|-----------|----------------|-----------|
| Puller Client (Go) | gRPC Streaming | ✅ Exponential backoff | Progress marker | Server-side |
| Realtime Client (TS) | WebSocket | ✅ Exponential backoff + jitter | Subscription map | Activity timeout |
| **Streamer Client (Go)** | gRPC Bidirectional | ❌ None | ❌ None | ❌ Config only |

### Key Differences from Puller Client

| Aspect | Puller Client | Streamer Client |
|--------|---------------|-----------------|
| Stream Type | Server streaming (single direction) | Bidirectional |
| Subscriber | Single consumer per connection | Multiple subscriptions |
| State to Restore | Progress marker (string) | Map of active subscriptions |
| Reconnect Trigger | `stream.Recv()` error | `recvLoop` error |
| User | Internal (Streamer service) | External (Gateway, SDK) |

## Requirements

### Functional Requirements

1. **Automatic Reconnection**: When the gRPC stream disconnects, automatically attempt to reconnect
2. **Exponential Backoff**: Use exponential backoff with optional jitter to avoid thundering herd
3. **Subscription Recovery**: After reconnecting, re-establish all active subscriptions
4. **State Notification**: Notify callers of connection state changes
5. **Graceful Degradation**: Respect `MaxRetries` limit, stop attempting after limit reached

### Non-Functional Requirements

1. **Thread Safety**: All state management must be thread-safe
2. **Resource Cleanup**: Proper cleanup on disconnect and context cancellation
3. **Observability**: Log reconnection attempts and state changes
4. **Backward Compatibility**: Existing API should remain compatible

## Design

### 1. Enhanced Client Configuration

```go
// ClientConfig configures the Streamer client.
type ClientConfig struct {
    // StreamerAddr is the address of the Streamer gRPC server.
    StreamerAddr string

    // InitialBackoff is the initial wait time before first reconnect attempt.
    // Defaults to 1 second.
    InitialBackoff time.Duration

    // MaxBackoff is the maximum wait time between reconnect attempts.
    // Defaults to 30 seconds.
    MaxBackoff time.Duration

    // BackoffMultiplier is the factor by which backoff increases after each failure.
    // Defaults to 2.0.
    BackoffMultiplier float64

    // MaxRetries is the maximum number of consecutive reconnect attempts.
    // Set to 0 for unlimited retries. Defaults to 0 (unlimited).
    MaxRetries int

    // HeartbeatInterval is the time between heartbeat messages.
    // If > 0, client sends periodic heartbeats to detect stale connections.
    // Defaults to 30 seconds.
    HeartbeatInterval time.Duration

    // ActivityTimeout is the maximum time to wait for any message before
    // considering the connection stale. Defaults to 90 seconds.
    ActivityTimeout time.Duration

    // OnStateChange is called when connection state changes. Optional.
    OnStateChange StateChangeCallback
}
```

### 2. Connection State Management

```go
// ConnectionState represents the current connection state.
type ConnectionState int

const (
    StateDisconnected ConnectionState = iota
    StateConnecting
    StateConnected
    StateReconnecting
)

// StateChangeCallback is called when connection state changes.
type StateChangeCallback func(state ConnectionState, err error)
```

### 3. Subscription State Tracking

```go
// subscriptionInfo stores subscription details for recovery
type subscriptionInfo struct {
    tenant     string
    collection string
    filters    []Filter
}

type remoteStream struct {
    // ... existing fields ...

    // activeSubscriptions tracks subscriptions for recovery on reconnect
    activeSubscriptions   map[string]*subscriptionInfo  // subID -> info
    activeSubscriptionsMu sync.RWMutex
}
```

### 4. Reconnection Flow

```
[Connected] ---(stream error)---> [Reconnecting]
     ^                                  |
     |                                  v
     +--- (success) --- [Re-subscribe all] <--- (backoff wait)
                                  |
                                  v
                           [MaxRetries?]
                                  |
                            (yes) v
                          [Disconnected]
```

### 5. Key Implementation Points

#### a) Store subscription info on Subscribe
```go
func (rs *remoteStream) Subscribe(tenant, collection string, filters []Filter) (string, error) {
    // ... existing logic ...

    // On success, store for potential recovery
    rs.activeSubscriptionsMu.Lock()
    rs.activeSubscriptions[subID] = &subscriptionInfo{
        tenant:     tenant,
        collection: collection,
        filters:    filters,
    }
    rs.activeSubscriptionsMu.Unlock()

    return subID, nil
}
```

#### b) Remove subscription info on Unsubscribe
```go
func (rs *remoteStream) Unsubscribe(subscriptionID string) error {
    // ... send unsubscribe ...

    rs.activeSubscriptionsMu.Lock()
    delete(rs.activeSubscriptions, subscriptionID)
    rs.activeSubscriptionsMu.Unlock()

    return nil
}
```

#### c) Reconnect loop in recvLoop
```go
func (rs *remoteStream) recvLoop() {
    defer close(rs.recvChan)

    for {
        protoMsg, err := rs.grpcStream.Recv()
        if err != nil {
            if rs.ctx.Err() != nil {
                return // Context canceled, stop
            }

            // Attempt reconnect
            if !rs.reconnect() {
                rs.recvErr = err
                return
            }
            continue
        }

        rs.lastMessageTime = time.Now()
        // ... handle message ...
    }
}

func (rs *remoteStream) reconnect() bool {
    backoff := rs.client.config.InitialBackoff
    attempts := 0

    for {
        select {
        case <-rs.ctx.Done():
            return false
        case <-time.After(backoff):
        }

        attempts++
        rs.notifyState(StateReconnecting, nil)

        // Try to establish new stream
        newStream, err := rs.client.client.Stream(rs.ctx)
        if err != nil {
            if rs.client.config.MaxRetries > 0 && attempts >= rs.client.config.MaxRetries {
                rs.notifyState(StateDisconnected, err)
                return false
            }

            backoff = min(backoff * time.Duration(rs.client.config.BackoffMultiplier),
                          rs.client.config.MaxBackoff)
            continue
        }

        rs.grpcStream = newStream

        // Re-subscribe all active subscriptions
        if err := rs.resubscribeAll(); err != nil {
            rs.logger.Error("failed to resubscribe", "error", err)
            // Continue trying
            continue
        }

        rs.notifyState(StateConnected, nil)
        return true
    }
}
```

#### d) Activity timeout (optional heartbeat)
```go
func (rs *remoteStream) startActivityMonitor() {
    ticker := time.NewTicker(rs.client.config.ActivityTimeout / 3)
    defer ticker.Stop()

    for {
        select {
        case <-rs.ctx.Done():
            return
        case <-ticker.C:
            if time.Since(rs.lastMessageTime) > rs.client.config.ActivityTimeout {
                rs.logger.Warn("connection appears stale, triggering reconnect")
                rs.grpcStream.CloseSend()
                return
            }
        }
    }
}
```

## Implementation Tasks

### Phase 1: Configuration & State Management
- [ ] 1.1 Update `ClientConfig` with new fields (align with Puller client)
- [ ] 1.2 Add `ConnectionState` enum and callback
- [ ] 1.3 Update `DefaultClientConfig()` with sensible defaults
- [ ] 1.4 Add state tracking to `remoteStream`

### Phase 2: Subscription State Tracking
- [ ] 2.1 Add `subscriptionInfo` struct and map to `remoteStream`
- [ ] 2.2 Store subscription info in `Subscribe()`
- [ ] 2.3 Remove subscription info in `Unsubscribe()`
- [ ] 2.4 Implement `resubscribeAll()` helper

### Phase 3: Reconnection Logic
- [ ] 3.1 Implement `reconnect()` with exponential backoff
- [ ] 3.2 Modify `recvLoop()` to trigger reconnect on error
- [ ] 3.3 Add jitter to backoff calculation (optional)
- [ ] 3.4 Implement `notifyState()` for callback invocation

### Phase 4: Activity Monitoring (Optional)
- [ ] 4.1 Implement client-side heartbeat sending
- [ ] 4.2 Implement activity timeout detection
- [ ] 4.3 Add `lastMessageTime` tracking

### Phase 5: Testing
- [ ] 5.1 Unit tests for reconnection logic
- [ ] 5.2 Unit tests for subscription recovery
- [ ] 5.3 Unit tests for max retries limit
- [ ] 5.4 Unit tests for context cancellation
- [ ] 5.5 Integration tests with mock server

### Phase 6: Documentation & Cleanup
- [ ] 6.1 Update code comments
- [ ] 6.2 Remove unused `ReconnectInterval` field (replaced by `InitialBackoff`)
- [ ] 6.3 Update any related documentation

## Test Cases

1. **Basic Reconnect**: Stream error triggers reconnect and recovery
2. **Subscription Recovery**: All subscriptions restored after reconnect
3. **Max Retries**: Stop reconnecting after limit reached
4. **Context Cancellation**: Clean shutdown without reconnect attempts
5. **Backoff Calculation**: Verify exponential increase with cap
6. **State Callbacks**: Verify callbacks fired at correct times
7. **Activity Timeout**: Stale connection detection and reconnect

## Questions to Confirm

1. **Subscription ID Handling**:
   - Option A: Keep same subscription IDs after reconnect (requires server support)
   - Option B: Generate new IDs, notify caller of mapping change
   - **Decision**: ✅ Option A - Keep same IDs (server handles idempotent subscribe)

2. **Heartbeat Direction**:
   - Proto already has `Heartbeat` message from Gateway to Streamer
   - Should implement client-side heartbeat sending for activity monitoring
   - **Decision**: ✅ Yes, use existing proto message

3. **Jitter Implementation**:
   - TS client uses jitter to avoid thundering herd
   - Go puller client does not use jitter
   - **Decision**: ✅ Add jitter (±20%) for robustness

## Dependencies

- None (uses existing proto messages)

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Race conditions in state management | Medium | Use proper mutex protection, test with race detector |
| Memory leak from orphaned goroutines | Medium | Ensure proper cleanup on context cancellation |
| Thundering herd on server restart | Low | Implement jitter in backoff |

## Success Criteria

1. Client automatically reconnects on stream failure
2. All active subscriptions are restored after reconnect
3. State changes are properly notified to callers
4. No goroutine leaks in tests
5. Test coverage ≥ 80% for new code
