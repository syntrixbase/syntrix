# Change Stream Puller Implementation

Date: December 29, 2025
Status: Completed
Scope: Implement change stream puller with fan-out, checkpointing, event buffer, and gRPC consumer API.
Depends On: [014.2025-12-26-query-surface-encapsulation.md](014.2025-12-26-query-surface-encapsulation.md) (for storage interface patterns only)

## Design Documents

| Doc | Path | Description |
| --- | ---- | ----------- |
| Field Naming | [002_field_naming.md](../docs/design/server/002_field_naming.md) | **Single source of truth** for field names |
| Puller Architecture | [001.architecture.md](../docs/design/server/puller/001.architecture.md) | High-level architecture, event schema |
| Puller Design Details | [002.design-details.md](../docs/design/server/puller/002.design-details.md) | Implementation details, configuration, deployment |
| Engine Integration | [007.puller.md](../docs/design/server/engine/007.puller.md) | How Engine consumes from Puller |

## Goals

- Implement change stream puller to reduce load on primary storage
- Fan-out events to multiple consumers (Engine Index, Streamer, Trigger)
- Catch-up coalescing for slow consumers
- PebbleDB-based event buffer for durability and replay
- gRPC API for consumer subscription (consumer manages own progress)

## Prerequisites

- Task 014 completed (for reference patterns only)
- Puller connects directly to MongoDB change stream (NOT via storage.Watch)
- gRPC for consumer connections

---

## Phase 1: Shared Packages

Create shared packages for event types.

### Phase 1 Deliverables

| File | Description |
| ---- | ----------- |
| `internal/events/types.go` | `NormalizedEvent`, `OperationType`, `UpdateDescription` types |

### Phase 1 Implementation Notes

- Field names MUST match [Field Naming Conventions](../docs/design/server/002_field_naming.md)
- Event schema MUST match [Puller Architecture Section 3](../docs/design/server/puller/001.architecture.md#3-canonical-event-schema)
- Puller uses shorter JSON keys: `tenant` (not `tenantId`), `documentId` (not `id`)
- Operation types are lowercase: `insert`, `update`, `replace`, `delete`

### Phase 1 Acceptance Criteria

- [ ] `NormalizedEvent` struct matches canonical schema
- [ ] Types compile with no errors
- [ ] Unit tests for type serialization

---

## Phase 2: Configuration

Add puller configuration block to config system.

### Deliverables

| File | Description |
| ---- | ----------- |
| `internal/config/puller.go` | `PullerConfig` struct with YAML tags |
| `configs/puller.example.yaml` | Example configuration file |

### Phase 2 Configuration Structure

Puller references backends defined in `storage.backends` instead of duplicating connection info.

```yaml
puller:
  # gRPC Server
  grpc:
    address: ":50051"
    max_connections: 100

  # Which storage backends to watch (references storage.backends)
  # Each backend gets its own change stream, buffer, and checkpoint
  backends:
    - name: default_mongo           # references storage.backends["default_mongo"]
      collections:                 # whitelist (empty = watch `documents` only)
        - "documents"

  # Checkpoint (resume token persistence)
  checkpoint:
    backend: mongodb                # mongodb or etcd
    interval: 1s
    event_count: 1000               # also checkpoint every N events

  # Event Buffer (PebbleDB)
  buffer:
    path: /var/lib/puller/events    # per-backend subdirs created automatically
    max_size: 10GiB

  # Consumer Management
  consumer:
    catch_up_threshold: 100000
    coalesce_on_catch_up: true

  # Event Cleanup
  cleaner:
    interval: 1m
    retention: 1h

  # Bootstrap
  bootstrap:
    mode: from_now
```

**Note**: Puller backends reference `storage.backends` by name. The MongoDB connection URI and database name come from the storage configuration, ensuring consistency across the system.

### Acceptance Criteria

- [ ] Config loads from YAML without error
- [ ] Defaults applied for missing fields
- [ ] Validation fails for invalid values (negative durations, etc.)
- [ ] Unit tests for config parsing

---

## Phase 3: Puller Core

Implement the core puller service.

### Phase 3 Deliverables

| File | Description |
| ---- | ----------- |
| `internal/puller/puller.go` | Main `Puller` struct and `Run()` method |
| `internal/puller/normalizer.go` | Convert MongoDB events to `NormalizedEvent` |
| `internal/puller/checkpoint.go` | `CheckpointStore` interface and MongoDB implementation |

### Phase 3 Implementation Notes

- Use database-level watch (not per-tenant) - see [Design Details Section 3](../docs/design/server/puller/002.design-details.md)
- Single resume token for entire change stream, checkpointed periodically

### Phase 3 Acceptance Criteria

- [ ] Puller connects to MongoDB and receives change events
- [ ] Events normalized to `NormalizedEvent` with correct field mapping
- [ ] Checkpoint saved on interval and graceful shutdown
- [ ] Checkpoint loaded on restart, stream resumes correctly
- [ ] Unit tests for normalizer, checkpoint store
- [ ] Integration test: MongoDB insert → Puller receives event

---

## Phase 4: gRPC Consumer API

Expose gRPC API for consumers to subscribe to events.

### Phase 4 Deliverables

| File | Description |
| ---- | ----------- |
| `api/proto/puller.proto` | gRPC service definition |
| `internal/puller/server.go` | gRPC server implementation |
| `internal/puller/subscriber.go` | Subscription management |

### Phase 4 gRPC API

A single gRPC server handles all backends. Consumers subscribe to a merged event stream without knowing which backend events come from. Consumer progress is managed by consumers themselves (not stored by Puller).

```protobuf
service PullerService {
  // Subscribe to merged event stream from all backends
  rpc Subscribe(SubscribeRequest) returns (stream Event);
}

message SubscribeRequest {
  string consumer_id = 1;          // for logging/monitoring only
  string after = 2;                // progress marker: return events after this (exclusive)
                                   // empty = start from current head (no historical events)
  bool coalesce_on_catch_up = 3;   // enable catch-up coalescing
}

message Event {
  string id = 1;                   // unique event ID
  string tenant = 2;
  string collection = 3;
  string document_id = 4;
  string operation_type = 5;       // insert, update, replace, delete
  bytes full_document = 6;
  bytes update_description = 7;
  ClusterTime cluster_time = 8;    // MongoDB cluster timestamp
  int64 timestamp = 9;             // Unix milliseconds
  string progress = 10;            // current progress marker (opaque, save this)
}

message ClusterTime {
  uint32 t = 1;                    // seconds since epoch
  uint32 i = 2;                    // increment within second
}
```

### Phase 4 Progress Marker

The `progress` field is an opaque string encoding positions across all backends. Consumers should:

1. Save `progress` from the last successfully processed event
2. Pass it as `after` when reconnecting
3. Not parse or understand its internal structure

**Internal structure (Puller implementation detail):**

```go
type ProgressMarker struct {
    Positions map[string]string `json:"p"` // backend -> last event ID
}
// Encoded as base64 JSON
```

### Phase 4 Implementation Notes

- Server-side streaming for event delivery
- Each event includes `progress` field for consumer to save
- Consumer manages its own progress (Puller does not store it)
- Graceful handling of consumer disconnect/reconnect

### Phase 4 Acceptance Criteria

- [ ] gRPC server starts and accepts connections
- [ ] Consumer can subscribe and receive events
- [ ] Each event contains valid `progress` marker
- [ ] Consumer reconnects with `after` and resumes correctly
- [ ] Multiple consumers can subscribe independently
- [ ] Integration test: Puller → gRPC → test consumer receives event

---

## Phase 5: Event Buffer & Catch-up Coalescing

PebbleDB-based event buffer with catch-up coalescing.

### Phase 5 Deliverables

| File | Description |
| ---- | ----------- |
| `internal/puller/buffer.go` | PebbleDB wrapper for event storage |
| `internal/puller/coalescer.go` | Catch-up coalescing logic |
| `internal/puller/cleaner.go` | Old event cleanup |
| `internal/puller/sender.go` | StreamSender with progress marker assembly |

### Phase 5 Implementation Notes

**Event Buffer (PebbleDB):**

- Key: `{clusterTime.T}-{clusterTime.I}-{eventId}`
- Value: Serialized `NormalizedEvent`
- Ordered storage, supports range scan
- Per-backend subdirectory: `{buffer.path}/{backend_name}/`

**Catch-up Detection:**

- Puller counts events between consumer's position and head
- If behind by `catch_up_threshold`, enable coalescing (if requested)

**Coalesce Rules (catching-up only):**

- `update + update` → keep latest update
- `insert + update` → insert with updated data
- `insert + delete` → skip (cancel out)
- `update + delete` → keep delete
- `delete` always wins

**Note**: Consumer progress is NOT stored by Puller. Consumers save the `progress` field themselves.

### Phase 5 Configuration

```yaml
puller:
  buffer:
    path: /var/lib/puller/events
    max_size: 10GiB  # supports KiB, MiB, GiB, TiB suffixes

  consumer:
    catch_up_threshold: 100000  # events behind to trigger catch-up mode
    coalesce_on_catch_up: true

  cleaner:
    interval: 1m
    retention: 1h  # keep events for at least 1 hour
```

### Phase 5 Acceptance Criteria

- [ ] Events persisted to PebbleDB with correct ordering
- [ ] Real-time consumer receives all events as-is
- [ ] Catching-up consumer receives coalesced events (when enabled)
- [ ] Coalesce rules correctly applied (insert+delete cancels, etc.)
- [ ] Old events cleaned up based on retention policy
- [ ] Unit tests for buffer, coalescer
- [ ] Integration test: slow consumer catches up with coalescing

---

## Phase 6: Error Recovery

Handle gaps and resume token failures.

### Phase 6 Deliverables

| File | Description |
| ---- | ----------- |
| `internal/puller/gap_detector.go` | Detect time gaps in event stream |
| `internal/puller/recovery.go` | Resume token error handling |

### Phase 6 Implementation Notes

- Gap threshold: 5 minutes between events
- On gap: emit `GAP_DETECTED` metric and log warning
- On invalid resume token: delete checkpoint, start fresh, alert

### Phase 6 Acceptance Criteria

- [ ] Gap > 5 minutes detected and logged
- [ ] `puller_gaps_detected` metric incremented on gap
- [ ] Invalid resume token handled gracefully (no crash)
- [ ] Metrics emitted for gaps and token failures
- [ ] Integration test: corrupt checkpoint → Puller recovers

---

## Phase 7: Bootstrap & Health

First-run handling and operational endpoints.

### Phase 7 Deliverables

| File | Description |
| ---- | ----------- |
| `internal/puller/bootstrap.go` | First-run detection and initialization |
| `internal/puller/health.go` | Health check endpoint |
| `cmd/puller/main.go` | Standalone puller binary |

### Phase 7 Implementation Notes

- Bootstrap modes: `from_now` (default), `from_beginning` (use with caution)
- `from_beginning` warning: requires extended buffer retention for large databases
- Health endpoint: `/health` returns backend status and lag

### Phase 7 Acceptance Criteria

- [ ] First run detected when no checkpoint exists
- [ ] Bootstrap completion logged and metric emitted
- [ ] Health endpoint returns correct status
- [ ] Standalone binary starts and runs correctly
- [ ] Graceful shutdown saves checkpoint

---

## Future: Shard-Based Consumer Scaling

Shard-based horizontal scaling for consumers is documented as a future enhancement.
See [Shard-Based Consumer Scaling](../docs/design/server/puller/future/001.shard-scaling.md) for details.

---

## Key Defaults

| Parameter | Value |
| --------- | ----- |
| gRPC address | :50051 |
| gRPC max connections | 100 |
| Checkpoint interval | 1s or 1k events |
| Gap detection threshold | 5m |
| Buffer max size | 10GiB |
| Catch-up threshold | 100,000 events |
| Cleaner interval | 1m |
| Cleaner retention | 1h |

---

## Testing Summary

### Unit Tests

- [ ] Event normalization (MongoDB → NormalizedEvent)
- [ ] Buffer read/write (PebbleDB operations)
- [ ] Progress marker encoding/decoding
- [ ] Coalescer (merge rules, catch-up mode)
- [ ] Gap detector (time gap calculation)

### Integration Tests

- [ ] End-to-end: MongoDB insert → Puller → gRPC → consumer
- [ ] Checkpoint: restart Puller → resumes from checkpoint
- [ ] Gap recovery: simulate gap → metric emitted and logged
- [ ] Resume token failure: corrupt token → graceful recovery
- [ ] Consumer catch-up with coalescing

### Manual Verification

- [ ] Standalone binary runs with example config
- [ ] Health endpoint accessible and returns valid JSON
- [ ] Metrics exposed on `/metrics`

---

## Risks

| Risk | Mitigation |
| ---- | ---------- |
| Checkpoint loss → duplicates | Consumers must be idempotent |
| Resume token corruption | Graceful fallback, alert operators |
| PebbleDB disk usage | Bounded by max_size, cleaner removes old events |
| Catch-up coalescing loses details | Only enabled when behind threshold, consumer can disable |
| gRPC connection management | Graceful disconnect handling, reconnect logic in consumers |

---

## Status Log

- 2025-12-29: Task created from 014 dry-run analysis
- 2025-12-30: Comprehensive review, issues identified and resolved
- 2025-12-30: Design docs split into architecture and details
- 2025-12-30: Task simplified, removed duplicate content, added acceptance criteria
- 2025-12-30: Added collection filtering via MongoDB pipeline
- 2025-12-30: Redesigned Phase 5: PebbleDB buffer + catch-up coalescing (threshold: 100k events)
- 2025-12-30: Replaced JetStream with direct gRPC consumer API
- 2025-12-30: Updated buffer retention to 1 hour (configurable)
- 2025-12-30: Fixed resumeToken to single token (not per-partition)
- 2025-12-30: Updated Streamer docs to use Puller gRPC instead of JetStream
- 2025-12-30: Moved shard-based consumer scaling to future enhancement
- 2025-12-30: Multi-Backend redesigned: single gRPC server, merged event stream
- 2025-12-30: Puller backends now reference storage.backends (shared config)
- 2025-12-30: Added clusterTime and timestamp to gRPC Event message
- 2025-12-30: **FINAL API**: Consumer-transparent design - no backend field exposed
- 2025-12-30: Consumer manages own progress (Puller does not store it)
- 2025-12-30: Progress marker as composite opaque string (base64 encoded JSON)
- 2025-12-30: Added StreamSender with per-send progress marker assembly
- 2025-12-30: Global review - fixed outdated docs and markdown warnings
- 2025-12-30: Second global review - fixed remaining inconsistencies (acknowledge, LagMonitor, cleanup logic)
- 2025-12-30: Third global review - fixed streamer/001.architecture.md (removed Ack API, updated to progress marker)
- 2025-12-30: Fourth global review - fixed future/001.shard-scaling.md (start_position → after)
- 2025-12-30: Fifth global review - unified config structure (catch_up → consumer.catch_up_threshold, added event_count)

---

## References

- [Field Naming Conventions](../docs/design/server/002_field_naming.md) - **Single source of truth** for field names
- [Puller Architecture](../docs/design/server/puller/001.architecture.md) - Event schema, design decisions
- [Puller Design Details](../docs/design/server/puller/002.design-details.md) - Implementation code, configuration, deployment
- [Engine Integration](../docs/design/server/engine/007.puller.md) - Consumer integration patterns
