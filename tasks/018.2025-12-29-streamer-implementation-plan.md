# Streamer Implementation Plan

**Date:** December 29, 2025
**Status:** Planning Phase
**Est. Timeline:** 7-10 weeks for MVP

## Design Documentation

This implementation plan is based on the comprehensive Streamer design documents:

- [000.overview.md](../docs/design/server/streamer/000.overview.md) - Service overview and high-level architecture
- [001.architecture.md](../docs/design/server/streamer/001.architecture.md) - Component architecture and interfaces (JetStream-based)
- [002.subscription.md](../docs/design/server/streamer/002.subscription.md) - Subscription management and lifecycle
- [003.matching.md](../docs/design/server/streamer/003.matching.md) - Two-phase event matching (Bloom + CEL)
- [004.routing.md](../docs/design/server/streamer/004.routing.md) - Event routing and partitioning strategies
- [005.reliability.md](../docs/design/server/streamer/005.reliability.md) - Reliability and fault tolerance mechanisms
- [006.observability.md](../docs/design/server/streamer/006.observability.md) - Metrics, logging, and monitoring

This plan also depends on the [Puller service design](../docs/design/server/puller/001.architecture.md) being implemented first.

## Overview

This document provides a detailed, step-by-step implementation plan for building the Streamer service, migrating from the current minimal CSP (Change Stream Processor) implementation to a production-ready, scalable event streaming system.

**IMPORTANT ARCHITECTURAL CHANGE:** The Streamer service has been redesigned to use NATS JetStream for event transport instead of direct MongoDB connections and HTTP/2 delivery. This significantly simplifies the implementation and improves scalability.

**Key Changes from Original Design:**
- Streamer consumes events from Puller service via JetStream (no direct MongoDB connection)
- CEL evaluation executes in Streamer (not Gateway) to reduce network traffic
- Events are published to Gateway via JetStream subjects (not HTTP/2 push)
- Checkpoint management moved to Puller service
- Simpler architecture with fewer components

## Current State Analysis

### What We Have (internal/csp/)
- **97 lines** of basic HTTP server code
- 2 endpoints: `/health` and `/internal/v1/watch`
- Direct MongoDB change stream → single client SSE
- No subscription management
- No event matching logic
- No multi-tenant awareness

### What We Can Reuse (~30%)
- ✅ `storage.Event` type and interfaces
- ✅ `storage.Watch()` MongoDB integration
- ✅ CEL filter matching logic (`internal/api/realtime/cel.go`)
- ✅ Event serialization patterns
- ✅ Context handling patterns
- ✅ Testing infrastructure

### What We Need to Build (~60% - Simplified with JetStream)
- ❌ Subscription Index Manager
- ❌ Two-Phase Matching Engine (Bloom + CEL)
- ❌ JetStream Event Consumer (from Puller)
- ❌ JetStream Event Publisher (to Gateway)
- ❌ Control Plane API
- ❌ Observability (metrics, logging, tracing)

### What We DON'T Need to Build (Simplified Architecture)
- ✅ MongoDB Connection Manager (moved to Puller)
- ✅ Checkpoint Manager (moved to Puller)
- ✅ HTTP/2 Delivery Manager (replaced by JetStream)
- ✅ Retry Logic / DLQ (handled by NATS JetStream)
- ✅ Backpressure Management (handled by NATS JetStream)
- ✅ Event Batching (NATS handles efficiently)

## Target Architecture

```
MongoDB (1 change stream)
    ↓
┌─────────────────────────┐
│   Puller Service        │
│   (Task 016)            │
│   • Change Stream       │
│   • Normalization       │
│   • Checkpointing       │
└────────┬────────────────┘
         │
         ▼
    NATS JetStream
    puller.events.{coll}.{partition}
         │
         ▼
┌─────────────────────────────────┐
│       Streamer Node             │
│  ┌──────────────────────────┐   │
│  │ JetStream Consumer       │   │
│  │ • Subscribe to Puller    │   │
│  │ • Deduplication          │   │
│  └──────────┬───────────────┘   │
│             │                   │
│  ┌──────────▼───────────────┐   │
│  │ Subscription Index       │   │
│  │ • In-memory registry     │   │
│  │ • Bloom filters          │   │
│  │ • CEL cache              │   │
│  │ • TTL expiry             │   │
│  └──────────┬───────────────┘   │
│             │                   │
│  ┌──────────▼───────────────┐   │
│  │ Matching Engine          │   │
│  │ • Bloom filter phase     │   │
│  │ • CEL evaluation phase   │   │
│  │ *** CEL EXECUTES HERE ***│   │
│  └──────────┬───────────────┘   │
│             │                   │
│  ┌──────────▼───────────────┐   │
│  │ JetStream Publisher      │   │
│  │ • Publish to Gateway     │   │
│  │ • Subject routing        │   │
│  └──────────┬───────────────┘   │
└─────────────┼───────────────────┘
              │
              ▼
    NATS JetStream
    events.{gatewayId}.{subscriptionId}
              │
              ▼
    API Gateway Realtime
    (WebSocket connections)
```

**Key Benefits of JetStream Architecture:**
- **Simpler**: No MongoDB connection management, no HTTP/2 delivery code
- **More Reliable**: NATS handles persistence, retry, and backpressure automatically
- **More Scalable**: Horizontal scaling via NATS consumer groups
- **Lower Latency**: ~90% reduction in NATS traffic by executing CEL in Streamer

## Implementation Phases

### Phase 0: Setup and Foundation (Week 1)

**Goal:** Create package structure and basic interfaces

**Tasks:**
1. Create `internal/streamer/` package
2. Define core interfaces:
   ```go
   // internal/streamer/types.go
   type Subscription struct {
       ID          string
       GatewayID   string
       Tenant      string
       Collection  string
       Filters     []model.Filter
       CreatedAt   time.Time
       ExpiresAt   time.Time
   }

   type SubscriptionIndex interface {
       Register(sub *Subscription) error
       Renew(id string, ttl time.Duration) error
       Remove(id string) error
       GetByCollection(tenant, collection string) []*Subscription
   }

   type Matcher interface {
       Match(evt *storage.Event) ([]*MatchResult, error)
   }

   type Router interface {
       Route(matches []*MatchResult) (map[string][]*DeliveryBatch, error)
   }

   type Deliverer interface {
       Deliver(ctx context.Context, batch *DeliveryBatch) error
   }
   ```

3. Create subpackages:
   - `internal/streamer/index/` - Subscription index
   - `internal/streamer/matcher/` - Event matching
   - `internal/streamer/router/` - Event routing
   - `internal/streamer/delivery/` - Event delivery
   - `internal/streamer/checkpoint/` - Resume token management

4. Setup basic test infrastructure

**Deliverable:** Package skeleton with interfaces and TODO markers

**Testing:** Interface compilation check

---

### Phase 1: Subscription Management (Week 2-3)

#### Phase 1.1: Subscription Index (3 days)

**Goal:** In-memory subscription registry with basic CRUD

**Tasks:**
1. Implement `MemorySubscriptionIndex`:
   ```go
   type MemorySubscriptionIndex struct {
       mu            sync.RWMutex
       byID          map[string]*Subscription
       byCollection  map[string]map[string]*Subscription  // tenant:collection → id → sub
       byGateway     map[string]map[string]*Subscription  // gatewayID → id → sub
   }
   ```

2. Implement CRUD operations:
   - `Register(sub)` - Add subscription with ID generation
   - `Renew(id, ttl)` - Update expiration time
   - `Remove(id)` - Delete subscription
   - `GetByCollection(tenant, collection)` - Query subscriptions

3. Add read-only queries:
   - `GetByID(id) *Subscription`
   - `GetByGateway(gatewayID) []*Subscription`
   - `Count() int`
   - `CountByTenant(tenant) int`

**Files:**
- `internal/streamer/index/memory.go`
- `internal/streamer/index/memory_test.go`

**Testing:**
- Unit tests for all CRUD operations
- Concurrent access tests (race detector)
- Benchmark for 100k subscriptions

---

#### Phase 1.2: TTL and Expiry (2 days)

**Goal:** Automatic subscription expiration

**Tasks:**
1. Add expiry loop to `MemorySubscriptionIndex`:
   ```go
   func (idx *MemorySubscriptionIndex) StartExpiryLoop(ctx context.Context) {
       ticker := time.NewTicker(10 * time.Second)
       defer ticker.Stop()

       for {
           select {
           case <-ctx.Done():
               return
           case <-ticker.C:
               idx.expireStale()
           }
       }
   }
   ```

2. Implement `expireStale()`:
   - Find subscriptions with `ExpiresAt < time.Now()`
   - Remove expired subscriptions
   - Log expiration events
   - Update metrics

3. Add metrics:
   - `subscriptions_expired_total` (Counter)
   - `subscriptions_active` (Gauge)

**Testing:**
- Test expiry with fake time
- Test renewal prevents expiry
- Test metrics are updated

---

#### Phase 1.3: Control Plane API (3 days)

**Goal:** HTTP endpoints for subscription management

**Tasks:**
1. Extend `internal/streamer/server.go` with new endpoints:
   ```go
   POST   /internal/v1/subscriptions           - Register
   PUT    /internal/v1/subscriptions/:id/heartbeat - Renew
   DELETE /internal/v1/subscriptions/:id       - Remove
   GET    /internal/v1/subscriptions           - List (for debugging)
   GET    /health                              - Health check
   ```

2. Implement handlers with validation:
   - Validate required fields (gatewayID, tenant, collection)
   - Generate subscription IDs (UUID)
   - Set default TTL (5 minutes)
   - Return JSON responses

3. Add quota enforcement:
   ```go
   type QuotaChecker struct {
       MaxPerGateway int  // 10000
       MaxPerTenant  int  // 50000
       MaxGlobal     int  // 100000
   }
   ```

4. Add error handling:
   - `400 Bad Request` - Invalid JSON
   - `409 Conflict` - Duplicate subscription
   - `429 Too Many Requests` - Quota exceeded
   - `500 Internal Server Error` - Server errors

**Files:**
- `internal/streamer/server.go` (extend existing)
- `internal/streamer/handlers.go`
- `internal/streamer/quota.go`

**Testing:**
- Integration tests for all endpoints
- Quota enforcement tests
- Error handling tests

---

#### Phase 1.4: Subscription Registration Protocol (3 days) **[P0 - CRITICAL]**

**Goal:** Define and implement the complete subscription registration protocol between Gateway and Streamer

**Critical Questions to Answer:**
1. **Who initiates registration?** Gateway initiates when a client WebSocket connects
2. **How is GatewayID managed?** Each Gateway instance generates a unique ID on startup (UUID)
3. **What is the heartbeat mechanism?** Gateway sends heartbeat every 60s, Streamer expects it every 90s (30s grace period)
4. **Who generates SubscriptionID?** Gateway generates SubscriptionID when client subscribes
5. **What happens on Gateway restart?** Gateway re-registers all active subscriptions on startup

**Sequence Diagram:**

```
Client                Gateway                 Streamer               JetStream
  │                      │                       │                      │
  │──WebSocket Connect──►│                       │                      │
  │                      │──POST /subscriptions──►│                      │
  │                      │   {gatewayId: "gw-1", │                      │
  │                      │    subscriptionId,    │                      │
  │                      │    filters}           │                      │
  │                      │◄─────201 Created──────│                      │
  │                      │   {subscriptionId}    │                      │
  │                      │                       │──Build Bloom Filter──│
  │                      │                       │──Subscribe to Puller─►│
  │                      │                       │                      │
  │                      │─────Heartbeat────────►│                      │
  │                      │  (every 60s)          │                      │
  │                      │                       │                      │
  │                      │                       │◄──Event from Puller──│
  │                      │                       │──Match & Publish────►│
  │                      │◄──Event via JetStream────────────────────────│
  │◄─────WebSocket───────│   Subject:            │                      │
  │      Message         │   events.gw-1.sub-123 │                      │
  │                      │                       │                      │
  │──WebSocket Close────►│                       │                      │
  │                      │──DELETE /subscriptions/:id────►              │
  │                      │                       │──Remove from Index───│
```

**API Specification:**

```go
// Subscription Registration Request
POST /internal/v1/subscriptions
{
    "gatewayId": "gw-550e8400-e29b-41d4-a716-446655440000",
    "subscriptionId": "sub-123e4567-e89b-12d3-a456-426614174000",
    "tenant": "default",
    "collection": "messages",
    "filters": [
        {"field": "status", "op": "==", "value": "active"}
    ],
    "ttl": 300  // seconds, optional (default: 300)
}

// Response
201 Created
{
    "subscriptionId": "sub-123e4567-e89b-12d3-a456-426614174000",
    "expiresAt": "2025-12-29T12:05:00Z",
    "jetStreamSubject": "events.gw-550e8400-e29b-41d4-a716-446655440000.sub-123e4567-e89b-12d3-a456-426614174000"
}

// Heartbeat Request
PUT /internal/v1/subscriptions/:id/heartbeat
{
    "ttl": 300  // optional, extends TTL
}

// Response
200 OK
{
    "expiresAt": "2025-12-29T12:10:00Z"
}

// Unregister Request
DELETE /internal/v1/subscriptions/:id

// Response
204 No Content
```

**GatewayID Management:**

```go
// In API Gateway Realtime startup
type Gateway struct {
    id            string  // Generated once on startup
    streamerURL   string
    subscriptions map[string]*Subscription
}

func NewGateway() *Gateway {
    return &Gateway{
        id: fmt.Sprintf("gw-%s", uuid.New().String()),
        subscriptions: make(map[string]*Subscription),
    }
}

func (g *Gateway) Start(ctx context.Context) error {
    // 1. Connect to NATS JetStream
    if err := g.connectJetStream(); err != nil {
        return err
    }

    // 2. Subscribe to events for this gateway
    subject := fmt.Sprintf("events.%s.>", g.id)
    g.js.Subscribe(subject, g.handleStreamerEvent, nats.Durable(g.id))

    // 3. Start heartbeat loop
    go g.startHeartbeatLoop(ctx)

    // 4. Start HTTP server for client connections
    return g.server.ListenAndServe()
}
```

**Heartbeat Implementation:**

```go
// In Gateway
func (g *Gateway) startHeartbeatLoop(ctx context.Context) {
    ticker := time.NewTicker(60 * time.Second)
    defer ticker.Stop()

    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            g.sendHeartbeats()
        }
    }
}

func (g *Gateway) sendHeartbeats() {
    for subID := range g.subscriptions {
        url := fmt.Sprintf("%s/internal/v1/subscriptions/%s/heartbeat", g.streamerURL, subID)
        resp, err := http.Put(url, "application/json", nil)
        if err != nil {
            log.Warn("Heartbeat failed for subscription %s: %v", subID, err)
        }
        resp.Body.Close()
    }
}

// In Streamer
func (s *Streamer) checkExpiredSubscriptions() {
    now := time.Now()
    for id, sub := range s.index.byID {
        if now.After(sub.ExpiresAt) {
            log.Info("Subscription expired: %s (gateway: %s)", id, sub.GatewayID)
            s.index.Remove(id)
            s.metrics.SubscriptionsExpired.Inc()
        }
    }
}
```

**Gateway Restart Recovery:**

```go
// In Gateway - recovery on restart
func (g *Gateway) RecoverSubscriptions(ctx context.Context) error {
    // Load active client connections (if persisted, or rebuild from active WebSockets)
    activeClients := g.hub.GetAllClients()

    for _, client := range activeClients {
        for _, sub := range client.subscriptions {
            // Re-register with Streamer
            req := &RegisterRequest{
                GatewayID:      g.id,
                SubscriptionID: sub.ID,
                Tenant:         sub.Tenant,
                Collection:     sub.Collection,
                Filters:        sub.Filters,
                TTL:            300,
            }

            if err := g.registerWithStreamer(ctx, req); err != nil {
                log.Error("Failed to recover subscription %s: %v", sub.ID, err)
                // Continue with other subscriptions
            }
        }
    }

    return nil
}
```

**Error Handling:**

```go
// In Gateway
func (g *Gateway) registerWithStreamer(ctx context.Context, req *RegisterRequest) error {
    payload, _ := json.Marshal(req)
    resp, err := http.Post(
        fmt.Sprintf("%s/internal/v1/subscriptions", g.streamerURL),
        "application/json",
        bytes.NewReader(payload))

    if err != nil {
        return fmt.Errorf("network error: %w", err)
    }
    defer resp.Body.Close()

    switch resp.StatusCode {
    case 201:
        return nil
    case 400:
        return fmt.Errorf("invalid request: %s", readBody(resp.Body))
    case 409:
        // Subscription already exists, consider this success
        log.Warn("Subscription already registered: %s", req.SubscriptionID)
        return nil
    case 429:
        return fmt.Errorf("quota exceeded")
    default:
        return fmt.Errorf("unexpected status: %d", resp.StatusCode)
    }
}
```

**Files:**
- `internal/streamer/protocol.go` (new) - Protocol documentation and types
- `internal/streamer/handlers.go` (extend) - Registration endpoint handlers
- `internal/api/realtime/streamer_client.go` (new) - Gateway client for Streamer API
- `internal/api/realtime/gateway.go` (extend) - GatewayID management and heartbeat

**Testing:**
- Test subscription registration flow
- Test heartbeat prevents expiry
- Test Gateway restart recovery
- Test TTL expiration when heartbeat stops
- Test duplicate subscription handling
- Test quota enforcement
- Integration test: Gateway + Streamer + JetStream

---

### Phase 2: JetStream Infrastructure Setup (Week 3) **[P1 - HIGH PRIORITY]**

#### Phase 2.0: JetStream Streams Configuration (2 days)

**Goal:** Set up NATS JetStream infrastructure with proper streams and subjects

**Critical:** This must be completed before any consumer/publisher work begins.

**Tasks:**

1. **Create PULLER_EVENTS Stream** (consumed by Streamer):
   ```bash
   # Using nats CLI
   nats stream add PULLER_EVENTS \
     --subjects "puller.events.>" \
     --storage file \
     --retention limits \
     --max-age 24h \
     --max-msgs 10000000 \
     --max-bytes 10GB \
     --max-msg-size 1MB \
     --replicas 3
   ```

2. **Create GATEWAY_EVENTS Stream** (consumed by Gateway):
   ```bash
   nats stream add GATEWAY_EVENTS \
     --subjects "events.>" \
     --storage file \
     --retention limits \
     --max-age 1h \
     --max-msgs 5000000 \
     --max-bytes 5GB \
     --max-msg-size 512KB \
     --replicas 3
   ```

3. **Subject Naming Conventions:**
   ```
   # Puller → Streamer
   puller.events.{collection}.{partition}

   # Multi-backend mode
   puller.events.{backend}.{collection}.{partition}

   # Streamer → Gateway
   events.{gatewayId}.{subscriptionId}

   Examples:
   - puller.events.users.0
   - puller.events.default.orders.1
   - events.gw-550e8400.sub-123e4567
   ```

4. **Stream Configuration in Code:**
   ```go
   // internal/streamer/jetstream/setup.go
   type StreamConfig struct {
       Name       string
       Subjects   []string
       Storage    nats.StorageType
       Retention  nats.RetentionPolicy
       MaxAge     time.Duration
       MaxMsgs    int64
       MaxBytes   int64
       Replicas   int
   }

   func (s *JetStreamSetup) CreateStreams(ctx context.Context) error {
       js := s.nc.JetStream()

       configs := []StreamConfig{
           {
               Name:      "PULLER_EVENTS",
               Subjects:  []string{"puller.events.>"},
               Storage:   nats.FileStorage,
               Retention: nats.LimitsPolicy,
               MaxAge:    24 * time.Hour,
               MaxMsgs:   10000000,
               MaxBytes:  10 * 1024 * 1024 * 1024, // 10GB
               Replicas:  3,
           },
           {
               Name:      "GATEWAY_EVENTS",
               Subjects:  []string{"events.>"},
               Storage:   nats.FileStorage,
               Retention: nats.LimitsPolicy,
               MaxAge:    1 * time.Hour,
               MaxMsgs:   5000000,
               MaxBytes:  5 * 1024 * 1024 * 1024, // 5GB
               Replicas:  3,
           },
       }

       for _, cfg := range configs {
           _, err := js.AddStream(&nats.StreamConfig{
               Name:      cfg.Name,
               Subjects:  cfg.Subjects,
               Storage:   cfg.Storage,
               Retention: cfg.Retention,
               MaxAge:    cfg.MaxAge,
               MaxMsgs:   cfg.MaxMsgs,
               MaxBytes:  cfg.MaxBytes,
               Replicas:  cfg.Replicas,
           })
           if err != nil {
               return fmt.Errorf("failed to create stream %s: %w", cfg.Name, err)
           }
       }

       return nil
   }
   ```

5. **Consumer Configuration:**
   ```go
   // Streamer consumer config (consuming from Puller)
   type ConsumerConfig struct {
       StreamName    string
       ConsumerName  string
       FilterSubject string
       AckWait       time.Duration
       MaxAckPending int
       DeliverPolicy nats.DeliverPolicy
   }

   func (s *Streamer) createConsumer() error {
       _, err := s.js.AddConsumer("PULLER_EVENTS", &nats.ConsumerConfig{
           Durable:       "streamer-consumer",
           FilterSubject: "puller.events.>",
           AckWait:       5 * time.Second,
           MaxAckPending: 1000,
           DeliverPolicy: nats.DeliverAllPolicy,
           AckPolicy:     nats.AckExplicitPolicy,
       })
       return err
   }

   // Gateway consumer config (consuming from Streamer)
   func (g *Gateway) createConsumer(gatewayID string) error {
       _, err := g.js.AddConsumer("GATEWAY_EVENTS", &nats.ConsumerConfig{
           Durable:       fmt.Sprintf("gateway-%s", gatewayID),
           FilterSubject: fmt.Sprintf("events.%s.>", gatewayID),
           AckWait:       10 * time.Second,
           MaxAckPending: 500,
           DeliverPolicy: nats.DeliverAllPolicy,
           AckPolicy:     nats.AckExplicitPolicy,
       })
       return err
   }
   ```

6. **Conflict Resolution:**
   - If streams already exist, check configuration matches
   - If configuration differs, log warning and use existing
   - Allow manual override via config flag `--force-stream-update`

7. **Health Checks:**
   ```go
   func (s *Streamer) checkJetStreamHealth() error {
       js := s.nc.JetStream()

       // Check PULLER_EVENTS stream exists
       pullerStream, err := js.StreamInfo("PULLER_EVENTS")
       if err != nil {
           return fmt.Errorf("PULLER_EVENTS stream not found: %w", err)
       }

       // Check GATEWAY_EVENTS stream exists
       gatewayStream, err := js.StreamInfo("GATEWAY_EVENTS")
       if err != nil {
           return fmt.Errorf("GATEWAY_EVENTS stream not found: %w", err)
       }

       // Check stream lag
       if pullerStream.State.Msgs > 100000 {
           log.Warn("PULLER_EVENTS stream has high lag: %d messages", pullerStream.State.Msgs)
       }

       return nil
   }
   ```

**Files:**
- `internal/streamer/jetstream/setup.go` (new)
- `internal/streamer/jetstream/health.go` (new)
- `deployments/nats/streams.yaml` (new) - Stream configuration for operators
- `scripts/setup-jetstream.sh` (new) - Setup script

**Testing:**
- Test stream creation with nats CLI
- Test stream creation idempotency (run twice)
- Test consumer creation
- Test subject filtering works correctly
- Test stream limits (max age, max msgs)

**Configuration:**
```yaml
# config/streamer.yaml
jetstream:
  nats_urls:
    - nats://nats-1:4222
    - nats://nats-2:4222
    - nats://nats-3:4222

  # Input stream (from Puller)
  input_stream: "PULLER_EVENTS"
  input_subjects:
    - "puller.events.>"

  # Output stream (to Gateway)
  output_stream: "GATEWAY_EVENTS"

  # Consumer settings
  consumer:
    name: "streamer-consumer"
    ack_wait: 5s
    max_ack_pending: 1000

  # Stream setup on startup
  setup_streams: true  # Create streams if not exist
  force_update: false  # Force update existing streams
```

---

#### Phase 2.1: JetStream Consumer (3 days)

**Goal:** Consume events from Puller service via NATS JetStream

**Prerequisites:** Puller service must be implemented and running (Task 016)

**Tasks:**
1. Add NATS JetStream dependencies:
   ```go
   import "github.com/nats-io/nats.go"
   ```

2. Create `JetStreamConsumer`:
   ```go
   type JetStreamConsumer struct {
       nc           *nats.Conn
       js           nats.JetStreamContext
       consumerName string
       streamName   string
       subs         map[string]*nats.Subscription
       output       chan<- *NormalizedEvent
   }
   ```

3. Implement `Start()` method:
   - Connect to NATS server
   - Create JetStream context
   - Subscribe to `puller.events.>` (all collections)
   - Use durable consumer with manual ack
   - Decode events and send to processing pipeline

4. Implement message handler:
   - Decode JSON payload from Puller
   - Validate event structure
   - Send to deduplication
   - Acknowledge message after processing

**Files:**
- `internal/streamer/jetstream/consumer.go`
- `internal/streamer/jetstream/consumer_test.go`

**Testing:**
- Test with mock NATS server
- Test subscription to wildcard subjects
- Test message acknowledgment
- Test reconnection handling

---

#### Phase 2.2: Deduplication Cache with Extended Window (3 days) **[P2 - UPDATED]**

**Goal:** Prevent duplicate event processing with adequate window for redelivery scenarios

**Critical Issue:** Original design used 3-minute TTL which may be insufficient if Streamer crashes and restarts, or JetStream redelivery takes longer than expected.

**Updated Strategy:**

Use **MongoDB clusterTime-based idempotency** instead of time-based cache expiry:

**Tasks:**

1. **Implement ClusterTime-based Deduplicator:**

   ```go
   type Deduplicator struct {
       // Per-document tracking: stores highest processed clusterTime
       processed map[string]primitive.Timestamp  // Key: tenant:collection:docID
       mu        sync.RWMutex

       // LRU eviction for memory management
       lru       *lru.Cache
       maxSize   int
   }

   type DedupKey struct {
       Tenant      string
       Collection  string
       DocumentKey string
   }

   func (d *Deduplicator) IsDuplicate(evt *NormalizedEvent) bool {
       key := d.makeKey(evt.Tenant, evt.Collection, evt.DocumentID)

       d.mu.Lock()
       defer d.mu.Unlock()

       // Check if we've seen a later or equal clusterTime for this document
       if lastTime, exists := d.processed[key]; exists {
           if evt.ClusterTime.T <= lastTime.T && evt.ClusterTime.I <= lastTime.I {
               return true  // Duplicate or out-of-order event
           }
       }

       // Record this clusterTime as latest
       d.processed[key] = evt.ClusterTime
       d.lru.Add(key, evt.ClusterTime)  // For LRU eviction

       return false
   }

   func (d *Deduplicator) makeKey(tenant, collection, docID string) string {
       return fmt.Sprintf("%s:%s:%s", tenant, collection, docID)
   }
   ```

2. **Add LRU Eviction for Memory Management:**

   ```go
   func NewDeduplicator(maxSize int) *Deduplicator {
       cache, _ := lru.NewWithEvict(maxSize, func(key, value interface{}) {
           // Called when item is evicted
           log.Debug("Evicting dedup entry: %s", key)
       })

       return &Deduplicator{
           processed: make(map[string]primitive.Timestamp),
           lru:       cache,
           maxSize:   maxSize,
       }
   }
   ```

3. **Add Metrics:**

   ```go
   type DedupMetrics struct {
       DuplicatesDetected prometheus.Counter
       CacheSize          prometheus.Gauge
       CacheEvictions     prometheus.Counter
   }
   ```

4. **Configuration:**

   ```yaml
   streamer:
     deduplication:
       # Use clusterTime-based deduplication (no TTL needed)
       max_entries: 100000  # Max unique documents to track
       eviction_policy: lru
   ```

**Advantages over TTL-based approach:**

- Works correctly regardless of Streamer restart timing
- Handles JetStream redelivery delays (minutes, hours, or days)
- More memory efficient (only tracks active documents)
- Naturally handles out-of-order delivery
- No risk of processing duplicates after cache expiry

**Files:**
- `internal/streamer/dedupe/dedupe.go`
- `internal/streamer/dedupe/dedupe_test.go`

**Testing:**
- Test duplicate detection with same clusterTime
- Test out-of-order events (older clusterTime arrives later)
- Test LRU eviction
- Test cache size limits
- Test Streamer restart scenario
- Test long JetStream redelivery delay (simulate with Nak + delay)

---

### Phase 3: Event Matching (Week 4-5)

#### Phase 3.1: Bloom Filter with Key Extraction (4 days) **[P1 - HIGH PRIORITY]**

**Goal:** Fast probabilistic filtering with well-defined key extraction algorithm

**Tasks:**
1. Choose Bloom filter library:
   - Option 1: `github.com/bits-and-blooms/bloom/v3` (Recommended)
   - Option 2: `github.com/willf/bloom`

2. **Define Bloom Filter Key Extraction Algorithm:**

   The Bloom filter is used for fast candidate filtering before CEL evaluation. Keys are extracted from subscription filters based on equality operators only.

   ```go
   // Key Extraction Algorithm
   type BloomKeyExtractor struct{}

   func (e *BloomKeyExtractor) ExtractKeys(sub *Subscription) ([]string, error) {
       keys := make([]string, 0)

       // Rule 1: Always include collection name (all events match collection)
       keys = append(keys, fmt.Sprintf("collection:%s", sub.Collection))

       // Rule 2: Extract keys from equality filters
       for _, filter := range sub.Filters {
           switch filter.Op {
           case "==", "eq":
               // Build key: "field:value"
               key := fmt.Sprintf("%s:%v", filter.Field, filter.Value)
               keys = append(keys, key)

           case "in":
               // For "in" operator, create key for each value
               values := filter.Value.([]interface{})
               for _, val := range values {
                   key := fmt.Sprintf("%s:%v", filter.Field, val)
                   keys = append(keys, key)
               }

           // Skip non-equality operators (>, <, >=, <=, !=, contains, etc.)
           // These cannot be optimized with Bloom filter
           default:
               continue
           }
       }

       return keys, nil
   }

   // Event Key Extraction
   func (e *BloomKeyExtractor) ExtractEventKeys(evt *NormalizedEvent) []string {
       keys := make([]string, 0)

       // Always check collection
       keys = append(keys, fmt.Sprintf("collection:%s", evt.Collection))

       // Extract keys from event document fields
       for field, value := range evt.FullDoc {
           key := fmt.Sprintf("%s:%v", field, value)
           keys = append(keys, key)
       }

       return keys
   }
   ```

   **Example:**
   ```go
   // Subscription filter: status == "active" && priority == "high"
   // Extracted keys:
   // - "collection:orders"
   // - "status:active"
   // - "priority:high"

   // Event: {collection: "orders", data: {status: "active", priority: "high", amount: 100}}
   // Extracted keys:
   // - "collection:orders"
   // - "status:active"
   // - "priority:high"
   // - "amount:100"

   // Bloom filter check:
   // If ALL subscription keys are present in event keys → Candidate (proceed to CEL)
   // If ANY subscription key is missing → Definitely no match (skip CEL)
   ```

   **Limitations:**
   ```go
   // Case 1: Range queries - Cannot be optimized with Bloom filter
   // Filter: amount > 100
   // Behavior: All subscriptions with range queries on this field are checked

   // Case 2: Partial matches - Cannot be optimized
   // Filter: name contains "test"
   // Behavior: All subscriptions with "contains" are checked

   // Case 3: Negation - Cannot be optimized
   // Filter: status != "inactive"
   // Behavior: All subscriptions with "!=" are checked
   ```

3. Implement `BloomFilterIndex`:
   ```go
   type BloomFilterIndex struct {
       filters   map[string]*bloom.BloomFilter  // collection → filter
       config    BloomConfig
       extractor *BloomKeyExtractor

       // Track subscriptions without equality filters (cannot be optimized)
       noOptimize map[string][]*Subscription  // collection → subs
   }

   type BloomConfig struct {
       FalsePositiveRate float64  // 0.01 (1%)
       ExpectedItems     uint     // 100000
   }

   func (idx *BloomFilterIndex) Build(subs []*Subscription) error {
       // Group by collection
       byCollection := make(map[string][]*Subscription)
       for _, sub := range subs {
           byCollection[sub.Collection] = append(byCollection[sub.Collection], sub)
       }

       // Build Bloom filter per collection
       for collection, collSubs := range byCollection {
           filter := bloom.NewWithEstimates(
               uint(len(collSubs)),
               idx.config.FalsePositiveRate)

           optimizable := make([]*Subscription, 0)
           noOptimize := make([]*Subscription, 0)

           for _, sub := range collSubs {
               keys, err := idx.extractor.ExtractKeys(sub)
               if err != nil {
                   return err
               }

               if len(keys) > 1 {  // More than just collection key
                   // Add all keys to filter
                   for _, key := range keys {
                       filter.AddString(key)
                   }
                   optimizable = append(optimizable, sub)
               } else {
                   // Cannot optimize, must check every event
                   noOptimize = append(noOptimize, sub)
               }
           }

           idx.filters[collection] = filter
           idx.noOptimize[collection] = noOptimize

           log.Info("Built Bloom filter for collection %s: %d optimizable, %d non-optimizable",
               collection, len(optimizable), len(noOptimize))
       }

       return nil
   }

   func (idx *BloomFilterIndex) GetCandidates(evt *NormalizedEvent) []*Subscription {
       candidates := make([]*Subscription, 0)

       // Always include non-optimizable subscriptions
       candidates = append(candidates, idx.noOptimize[evt.Collection]...)

       // Check Bloom filter for optimizable subscriptions
       filter := idx.filters[evt.Collection]
       if filter == nil {
           return candidates
       }

       eventKeys := idx.extractor.ExtractEventKeys(evt)

       // For each subscription, check if event might match
       for _, sub := range idx.getOptimizableSubscriptions(evt.Collection) {
           subKeys, _ := idx.extractor.ExtractKeys(sub)

           // Check if ALL subscription keys are present in event
           allMatch := true
           for _, subKey := range subKeys {
               if !filter.TestString(subKey) {
                   allMatch = false
                   break
               }
           }

           if allMatch {
               candidates = append(candidates, sub)
           }
       }

       return candidates
   }
   ```

4. Add to subscription index:
   - Rebuild Bloom filter when subscriptions change
   - Update filter on add/remove operations
   - Periodic rebuild every 24h to handle false positive rate drift

**Files:**
- `internal/streamer/matcher/bloom.go`
- `internal/streamer/matcher/bloom_keys.go` (new) - Key extraction logic
- `internal/streamer/matcher/bloom_test.go`
- `docs/design/server/streamer/003.matching.md` (update) - Document algorithm

**Testing:**
- Test key extraction for all operator types
- Test false positive rate with real data
- Test with 100k subscriptions
- Test non-optimizable subscriptions still work
- Benchmark lookup performance
- Test periodic rebuild

---

#### Phase 3.2: CEL Matching and Compilation Ownership (4 days) **[P1 - HIGH PRIORITY]**

**Goal:** Exact filter evaluation with clear compilation ownership

**CRITICAL DECISION: CEL Compilation Ownership**

**Decision: Gateway compiles, Streamer caches and executes**

**Rationale:**
- Gateway receives filter expressions from clients and can validate them immediately
- Gateway can return compilation errors to client in real-time
- Streamer receives pre-validated expressions, reducing error surface
- Compilation happens once per subscription lifetime, not on every match

**Architecture:**

```
Client → Gateway → Streamer
         │           │
         ├─ Compile CEL expression
         ├─ Validate syntax
         ├─ Return error to client if invalid
         │
         └─ Send compiled expression → Store in index
                                      └─ Execute on each event
```

**Tasks:**

1. **Gateway: CEL Compilation** (in `internal/api/realtime/`)

   Extract and enhance existing CEL logic from `internal/api/realtime/cel.go`:

   ```go
   // internal/api/realtime/cel_compiler.go
   type CELCompiler struct {
       env *cel.Env
   }

   func NewCELCompiler() (*CELCompiler, error) {
       env, err := cel.NewEnv(
           cel.Variable("tenant", cel.StringType),
           cel.Variable("collection", cel.StringType),
           cel.Variable("data", cel.MapType(cel.StringType, cel.DynType)),
           cel.Variable("op", cel.StringType),
           cel.Variable("before", cel.MapType(cel.StringType, cel.DynType)),
       )
       if err != nil {
           return nil, err
       }
       return &CELCompiler{env: env}, nil
   }

   func (c *CELCompiler) CompileFilters(filters []model.Filter) (string, error) {
       // Build CEL expression from filters
       expr := c.buildExpression(filters)

       // Validate by compiling
       ast, issues := c.env.Compile(expr)
       if issues != nil && issues.Err() != nil {
           return "", fmt.Errorf("CEL compilation error: %w", issues.Err())
       }

       // Type check
       if ast.OutputType() != cel.BoolType {
           return "", fmt.Errorf("CEL expression must return boolean")
       }

       return expr, nil
   }

   func (c *CELCompiler) buildExpression(filters []model.Filter) string {
       expressions := make([]string, 0, len(filters))

       for _, filter := range filters {
           expr := c.filterToExpression(filter)
           expressions = append(expressions, expr)
       }

       return strings.Join(expressions, " && ")
   }

   func (c *CELCompiler) filterToExpression(filter model.Filter) string {
       field := fmt.Sprintf("data.%s", filter.Field)

       switch filter.Op {
       case "==", "eq":
           return fmt.Sprintf("%s == %s", field, c.valueToString(filter.Value))
       case "!=", "ne":
           return fmt.Sprintf("%s != %s", field, c.valueToString(filter.Value))
       case ">", "gt":
           return fmt.Sprintf("%s > %s", field, c.valueToString(filter.Value))
       case ">=", "gte":
           return fmt.Sprintf("%s >= %s", field, c.valueToString(filter.Value))
       case "<", "lt":
           return fmt.Sprintf("%s < %s", field, c.valueToString(filter.Value))
       case "<=", "lte":
           return fmt.Sprintf("%s <= %s", field, c.valueToString(filter.Value))
       case "in":
           return fmt.Sprintf("%s in %s", field, c.valueToString(filter.Value))
       case "contains":
           return fmt.Sprintf("%s.contains(%s)", field, c.valueToString(filter.Value))
       default:
           return "true"  // Unknown operator, allow all
       }
   }
   ```

   **Gateway Subscription Flow:**
   ```go
   // In Gateway - when client subscribes
   func (h *Hub) HandleSubscribe(client *Client, req *SubscribeRequest) error {
       // 1. Compile CEL expression
       celExpr, err := h.celCompiler.CompileFilters(req.Filters)
       if err != nil {
           return &SubscribeError{
               Code:    "CEL_COMPILATION_ERROR",
               Message: fmt.Sprintf("Invalid filter expression: %v", err),
           }
       }

       // 2. Register with Streamer (send compiled expression)
       streamerReq := &RegisterRequest{
           GatewayID:      h.gateway.ID,
           SubscriptionID: req.SubscriptionID,
           Tenant:         req.Tenant,
           Collection:     req.Collection,
           Filters:        req.Filters,      // Original filters
           CELExpression:  celExpr,          // Compiled CEL expression
           TTL:            300,
       }

       if err := h.streamerClient.Register(ctx, streamerReq); err != nil {
           return fmt.Errorf("failed to register with Streamer: %w", err)
       }

       // 3. Store subscription locally
       client.subscriptions[req.SubscriptionID] = &Subscription{
           ID:         req.SubscriptionID,
           Filters:    req.Filters,
           CELExpr:    celExpr,
       }

       return nil
   }
   ```

2. **Streamer: CEL Caching and Execution** (in `internal/streamer/matcher/`)

   ```go
   // internal/streamer/matcher/cel.go
   type CELMatcher struct {
       env   *cel.Env
       cache *lru.Cache  // subscriptionID → cel.Program
       mu    sync.RWMutex
   }

   func NewCELMatcher(cacheSize int) (*CELMatcher, error) {
       env, err := cel.NewEnv(
           cel.Variable("tenant", cel.StringType),
           cel.Variable("collection", cel.StringType),
           cel.Variable("data", cel.MapType(cel.StringType, cel.DynType)),
           cel.Variable("op", cel.StringType),
           cel.Variable("before", cel.MapType(cel.StringType, cel.DynType)),
       )
       if err != nil {
           return nil, err
       }

       cache, _ := lru.New(cacheSize)
       return &CELMatcher{env: env, cache: cache}, nil
   }

   func (m *CELMatcher) Evaluate(sub *Subscription, evt *NormalizedEvent) (bool, error) {
       // Get or compile program
       program, err := m.getProgram(sub)
       if err != nil {
           return false, fmt.Errorf("failed to get CEL program: %w", err)
       }

       // Build evaluation context
       vars := map[string]interface{}{
           "tenant":     evt.Tenant,
           "collection": evt.Collection,
           "data":       evt.FullDoc,
           "op":         evt.Type,
       }
       if evt.UpdateDesc != nil && evt.UpdateDesc.Before != nil {
           vars["before"] = evt.UpdateDesc.Before
       }

       // Execute
       result, _, err := program.Eval(vars)
       if err != nil {
           return false, fmt.Errorf("CEL evaluation error: %w", err)
       }

       boolResult, ok := result.Value().(bool)
       if !ok {
           return false, fmt.Errorf("CEL expression did not return boolean")
       }

       return boolResult, nil
   }

   func (m *CELMatcher) getProgram(sub *Subscription) (cel.Program, error) {
       // Check cache
       m.mu.RLock()
       if prog, ok := m.cache.Get(sub.ID); ok {
           m.mu.RUnlock()
           return prog.(cel.Program), nil
       }
       m.mu.RUnlock()

       // Compile (should not fail since Gateway already validated)
       m.mu.Lock()
       defer m.mu.Unlock()

       // Double-check after acquiring write lock
       if prog, ok := m.cache.Get(sub.ID); ok {
           return prog.(cel.Program), nil
       }

       ast, issues := m.env.Compile(sub.CELExpression)
       if issues != nil && issues.Err() != nil {
           // This should never happen if Gateway validated correctly
           log.Error("CEL compilation failed in Streamer (Gateway validation bug?): %v", issues.Err())
           return nil, fmt.Errorf("CEL compilation error: %w", issues.Err())
       }

       program, err := m.env.Program(ast)
       if err != nil {
           return nil, fmt.Errorf("CEL program creation error: %w", err)
       }

       m.cache.Add(sub.ID, program)
       return program, nil
   }

   // Called when subscription is removed
   func (m *CELMatcher) Evict(subscriptionID string) {
       m.mu.Lock()
       defer m.mu.Unlock()
       m.cache.Remove(subscriptionID)
   }
   ```

3. **Error Handling Strategy:**

   ```go
   // Gateway: Return errors to client
   type SubscribeError struct {
       Code    string `json:"code"`
       Message string `json:"message"`
   }

   // Possible error codes:
   // - CEL_COMPILATION_ERROR: Invalid CEL syntax
   // - CEL_TYPE_ERROR: Expression doesn't return boolean
   // - UNSUPPORTED_OPERATOR: Operator not supported

   // Streamer: Log errors but continue processing
   func (m *MatchingEngine) Match(evt *NormalizedEvent) ([]*MatchResult, error) {
       matches := make([]*MatchResult, 0)

       for _, sub := range candidates {
           matched, err := m.celMatcher.Evaluate(sub, evt)
           if err != nil {
               // Log error but don't fail entire match operation
               log.Error("CEL evaluation failed for subscription %s: %v", sub.ID, err)
               m.metrics.CELEvaluationErrors.Inc()
               continue  // Skip this subscription, continue with others
           }

           if matched {
               matches = append(matches, &MatchResult{...})
           }
       }

       return matches, nil
   }
   ```

4. **Persistence and Recovery:**

   Compiled expressions are stored in etcd with subscriptions:

   ```go
   type Subscription struct {
       ID            string
       GatewayID     string
       Tenant        string
       Collection    string
       Filters       []Filter       // Original filters
       CELExpression string         // Compiled CEL expression (from Gateway)
       ExpiresAt     time.Time
   }

   // On Streamer restart:
   // 1. Load subscriptions from etcd
   // 2. CEL programs are re-compiled from stored expressions on first use
   // 3. Cache is rebuilt lazily
   ```

**Files:**
- `internal/api/realtime/cel_compiler.go` (new) - CEL compilation in Gateway
- `internal/streamer/matcher/cel.go` - CEL execution in Streamer
- `internal/streamer/matcher/cel_test.go`
- `docs/design/server/streamer/003.matching.md` (update) - Document ownership

**Testing:**
- Gateway: Test CEL compilation for all operators
- Gateway: Test error messages returned to client
- Gateway: Test invalid expressions rejected
- Streamer: Test cache hit/miss
- Streamer: Test graceful handling of compilation errors
- Integration: Test end-to-end flow with valid/invalid filters

---

#### Phase 3.3: Two-Phase Matcher with Multi-Tenant Isolation (3 days) **[P2 - ENHANCED]**

**Goal:** Combine Bloom filter + CEL evaluation

**Tasks:**
1. Implement `TwoPhaseMatchEngine`:
   ```go
   type TwoPhaseMatchEngine struct {
       index      SubscriptionIndex
       bloom      *BloomFilterIndex
       cel        *CELMatcher
       metrics    *MatcherMetrics
   }
   ```

2. Implement `Match(event)`:
   ```go
   func (m *TwoPhaseMatchEngine) Match(evt *storage.Event) ([]*MatchResult, error) {
       // Phase 1: Bloom filter (fast path)
       candidates := m.bloom.GetCandidates(evt.TenantID, evt.Collection, evt.Id)

       // Phase 2: CEL evaluation (slow path)
       matches := make([]*MatchResult, 0, len(candidates))
       for _, sub := range candidates {
           if m.cel.Evaluate(sub, evt) {
               matches = append(matches, &MatchResult{
                   SubscriptionID: sub.ID,
                   GatewayID:      sub.GatewayID,
                   Event:          evt,
               })
           }
       }

       return matches, nil
   }
   ```

3. Add metrics:
   - `bloom_filter_candidates` (Histogram)
   - `cel_evaluations_total` (Counter)
   - `cel_evaluation_latency_seconds` (Histogram)
   - `matches_found` (Histogram)

**Files:**
- `internal/streamer/matcher/engine.go`
- `internal/streamer/matcher/engine_test.go`

**Testing:**
- End-to-end matching tests
- Performance tests (target: <10ms p99)
- Test with 10k subscriptions

---

### Phase 4: Gateway Integration and JetStream Publishing (Week 5-6) **[P0 - CRITICAL]**

#### Phase 4.0: Gateway Integration with Streamer (4 days) **[NEW - CRITICAL]**

**Goal:** Modify API Gateway to integrate with Streamer service via JetStream

**Context:** Currently Gateway uses `queryService.WatchCollection()` to get events directly from Engine. This needs to be replaced with JetStream consumer pattern to receive pre-filtered events from Streamer.

**Current Implementation (to be replaced):**

From `internal/api/realtime/server.go:95`:
```go
stream, err := s.queryService.WatchCollection(ctx, "", "")
```

From `internal/api/realtime/hub.go:62-131`:
```go
case message := <-h.broadcast:
    for client := range h.clients {
        for subID, sub := range client.subscriptions {
            if sub.CelProgram != nil {
                out, _, err := sub.CelProgram.Eval(...)
            }
        }
    }
```

**New Architecture:**

```
Client WebSocket → Gateway → Streamer Registration API
                     ↓
              JetStream Subscribe (events.{gatewayId}.>)
                     ↓
              Receive Pre-Filtered Events
                     ↓
              Forward to Client WebSocket
```

**Tasks:**

1. **Modify Gateway Startup Sequence:**

   ```go
   // internal/api/realtime/gateway.go (new file or extend server.go)
   type Gateway struct {
       id              string  // UUID generated on startup
       nc              *nats.Conn
       js              nats.JetStreamContext
       streamerURL     string
       streamerClient  *StreamerClient
       hub             *Hub
       subscriptions   map[string]*ActiveSubscription  // subscriptionID → subscription
       mu              sync.RWMutex
   }

   func NewGateway(config *Config) (*Gateway, error) {
       // Generate unique Gateway ID
       gatewayID := fmt.Sprintf("gw-%s", uuid.New().String())

       // Connect to NATS
       nc, err := nats.Connect(config.NatsURL)
       if err != nil {
           return nil, fmt.Errorf("failed to connect to NATS: %w", err)
       }

       js, err := nc.JetStream()
       if err != nil {
           return nil, fmt.Errorf("failed to create JetStream context: %w", err)
       }

       return &Gateway{
           id:             gatewayID,
           nc:             nc,
           js:             js,
           streamerURL:    config.StreamerURL,
           streamerClient: NewStreamerClient(config.StreamerURL),
           subscriptions:  make(map[string]*ActiveSubscription),
       }, nil
   }

   func (g *Gateway) Start(ctx context.Context) error {
       // 1. Subscribe to JetStream events for this gateway
       subject := fmt.Sprintf("events.%s.>", g.id)

       _, err := g.js.Subscribe(subject, g.handleStreamerEvent,
           nats.ManualAck(),
           nats.Durable(fmt.Sprintf("gateway-%s", g.id)),
           nats.DeliverAll(),
           nats.AckExplicit())
       if err != nil {
           return fmt.Errorf("failed to subscribe to JetStream: %w", err)
       }

       // 2. Start heartbeat loop
       go g.startHeartbeatLoop(ctx)

       // 3. Start HTTP server for WebSocket connections
       return g.startHTTPServer(ctx)
   }
   ```

2. **Implement Streamer Client:**

   ```go
   // internal/api/realtime/streamer_client.go (new)
   type StreamerClient struct {
       baseURL    string
       httpClient *http.Client
   }

   func (c *StreamerClient) Register(ctx context.Context, req *RegisterSubscriptionRequest) error {
       payload, _ := json.Marshal(req)
       resp, err := c.httpClient.Post(
           fmt.Sprintf("%s/internal/v1/subscriptions", c.baseURL),
           "application/json",
           bytes.NewReader(payload))

       if err != nil {
           return fmt.Errorf("failed to register subscription: %w", err)
       }
       defer resp.Body.Close()

       if resp.StatusCode != http.StatusCreated {
           body, _ := io.ReadAll(resp.Body)
           return fmt.Errorf("registration failed: %s", body)
       }

       return nil
   }

   func (c *StreamerClient) Unregister(ctx context.Context, subscriptionID string) error {
       req, _ := http.NewRequestWithContext(ctx, "DELETE",
           fmt.Sprintf("%s/internal/v1/subscriptions/%s", c.baseURL, subscriptionID), nil)

       resp, err := c.httpClient.Do(req)
       if err != nil {
           return err
       }
       defer resp.Body.Close()

       if resp.StatusCode != http.StatusNoContent {
           return fmt.Errorf("unregister failed: status %d", resp.StatusCode)
       }

       return nil
   }

   func (c *StreamerClient) Heartbeat(ctx context.Context, subscriptionID string) error {
       req, _ := http.NewRequestWithContext(ctx, "PUT",
           fmt.Sprintf("%s/internal/v1/subscriptions/%s/heartbeat", c.baseURL, subscriptionID), nil)

       resp, err := c.httpClient.Do(req)
       if err != nil {
           return err
       }
       defer resp.Body.Close()

       return nil
   }
   ```

3. **Modify Client Subscription Flow:**

   ```go
   // internal/api/realtime/hub.go - MODIFY existing HandleSubscribe
   func (h *Hub) HandleSubscribe(client *Client, req *SubscribeRequest) error {
       subscriptionID := fmt.Sprintf("sub-%s", uuid.New().String())

       // 1. Compile CEL (Gateway compiles, Streamer executes)
       celExpr, err := h.celCompiler.CompileFilters(req.Filters)
       if err != nil {
           return &SubscribeError{
               Code:    "CEL_COMPILATION_ERROR",
               Message: fmt.Sprintf("Invalid filters: %v", err),
           }
       }

       // 2. Register with Streamer
       registerReq := &RegisterSubscriptionRequest{
           GatewayID:      h.gateway.id,
           SubscriptionID: subscriptionID,
           Tenant:         req.Tenant,
           Collection:     req.Collection,
           Filters:        req.Filters,
           CELExpression:  celExpr,
           TTL:            300,  // 5 minutes
       }

       if err := h.gateway.streamerClient.Register(context.Background(), registerReq); err != nil {
           return fmt.Errorf("failed to register with Streamer: %w", err)
       }

       // 3. Store subscription locally
       h.gateway.mu.Lock()
       h.gateway.subscriptions[subscriptionID] = &ActiveSubscription{
           ID:         subscriptionID,
           ClientID:   client.id,
           Tenant:     req.Tenant,
           Collection: req.Collection,
           Filters:    req.Filters,
           CreatedAt:  time.Now(),
       }
       h.gateway.mu.Unlock()

       // 4. Store in client
       client.subscriptions[subscriptionID] = &ClientSubscription{
           ID:      subscriptionID,
           Filters: req.Filters,
       }

       return nil
   }
   ```

4. **Replace CEL Evaluation with Simple Forwarding:**

   ```go
   // internal/api/realtime/hub.go - REPLACE broadcast loop
   func (g *Gateway) handleStreamerEvent(msg *nats.Msg) {
       var evt StreamerEvent
       if err := json.Unmarshal(msg.Data, &evt); err != nil {
           log.Error("Failed to decode event: %v", err)
           msg.Nak()
           return
       }

       // Find client and forward (NO CEL evaluation needed - Streamer already did it)
       g.mu.RLock()
       sub, ok := g.subscriptions[evt.SubscriptionID]
       g.mu.RUnlock()

       if !ok {
           log.Warn("Received event for unknown subscription: %s", evt.SubscriptionID)
           msg.Ack()  // Ack anyway to avoid redelivery
           return
       }

       // Find client connection
       client := g.hub.GetClient(sub.ClientID)
       if client == nil {
           log.Debug("Client disconnected for subscription %s", evt.SubscriptionID)
           msg.Ack()
           return
       }

       // Forward to WebSocket
       client.Send(&RealtimeMessage{
           Type:           "event",
           SubscriptionID: evt.SubscriptionID,
           Event:          evt.Event,
       })

       msg.Ack()
   }
   ```

5. **Remove Direct MongoDB Watching:**

   Delete or comment out:
   - `queryService.WatchCollection()` call in `server.go`
   - CEL evaluation loop in `hub.go` broadcast handler

6. **Configuration:**

   ```yaml
   # config/gateway.yaml
   realtime:
     port: 8080

     # NATS JetStream connection
     nats:
       urls:
         - nats://localhost:4222
       stream_name: "GATEWAY_EVENTS"

     # Streamer service connection
     streamer:
       url: "http://streamer:8083"
       timeout: 5s

     # Heartbeat
     heartbeat_interval: 60s
   ```

**Files:**
- `internal/api/realtime/gateway.go` (new or extend server.go)
- `internal/api/realtime/streamer_client.go` (new)
- `internal/api/realtime/hub.go` (modify - remove CEL eval, add forwarding)
- `internal/api/realtime/server.go` (modify - remove WatchCollection)
- `config/gateway.yaml` (update)

**Testing:**
- Unit test: StreamerClient register/unregister/heartbeat
- Integration test: Gateway + Streamer + JetStream
- Integration test: Client WebSocket → Gateway → Streamer → event delivery
- Test Gateway restart with active subscriptions
- Test subscription cleanup on client disconnect
- Test heartbeat mechanism
- Load test: 1000 concurrent WebSocket connections

**Migration Plan:**
1. Deploy Streamer service
2. Update Gateway configuration (keep old code path)
3. Deploy updated Gateway with feature flag
4. Enable new JetStream path for 10% of traffic
5. Monitor for issues
6. Gradually increase to 100%
7. Remove old code path

---

#### Phase 4.1: JetStream Publisher (3 days)

**Goal:** Publish matched events to Gateway via JetStream

**Tasks:**
1. Implement `JetStreamPublisher`:
   ```go
   type JetStreamPublisher struct {
       js nats.JetStreamContext
       streamName string
   }
   ```

2. Implement `Publish(matches)`:
   - For each match, create subject: `events.{gatewayId}.{subscriptionId}`
   - Serialize event to JSON
   - Publish with async acknowledgment for performance
   - Handle publish errors

3. Add batch publishing:
   - Group events by gateway
   - Publish multiple events in parallel
   - Use goroutine pool to limit concurrency

4. Add metrics:
   - `jetstream_publish_total` (Counter)
   - `jetstream_publish_errors_total` (Counter)
   - `jetstream_publish_latency_seconds` (Histogram)

**Files:**
- `internal/streamer/jetstream/publisher.go`
- `internal/streamer/jetstream/publisher_test.go`

**Testing:**
- Test subject routing
- Test batch publishing
- Test error handling
- Test with mock NATS

---

### Phase 5: Observability (Week 6-7)

#### Phase 5.1: Prometheus Metrics (3 days)

**Goal:** Implement all metrics from design

**Tasks:**
1. Create metrics registry:
   ```go
   type StreamerMetrics struct {
       // JetStream Consumer
       EventsConsumed        prometheus.Counter
       ConsumerLatency       prometheus.Histogram
       ConsumerErrors        prometheus.Counter

       // Subscriptions
       SubscriptionsActive    prometheus.Gauge
       SubscriptionsByTenant  *prometheus.GaugeVec
       SubscriptionsExpired   prometheus.Counter

       // Matching
       BloomFilterHits       prometheus.Counter
       BloomFilterMisses     prometheus.Counter
       CELEvaluations        prometheus.Counter
       MatchLatency          prometheus.Histogram
       MatchesFound          prometheus.Histogram

       // JetStream Publisher
       EventsPublished       prometheus.Counter
       PublishLatency        prometheus.Histogram
       PublishErrors         prometheus.Counter

       // Reliability
       DuplicateEvents       prometheus.Counter
   }
   ```

2. Register all metrics with Prometheus
3. Add metrics endpoint: `GET /metrics`
4. Instrument all code paths

**Files:**
- `internal/streamer/observability/metrics.go`
- `internal/streamer/observability/metrics_test.go`

**Testing:**
- Test metric registration
- Test metric updates
- Test metrics endpoint

---

#### Phase 5.2: Structured Logging (2 days)

**Goal:** Add comprehensive logging

**Tasks:**
1. Setup zap logger:
   ```go
   logger, _ := zap.NewProduction(zap.WrapCore(func(core zapcore.Core) zapcore.Core {
       return zapcore.NewSamplerWithOptions(core,
           time.Second,
           100,  // First 100/sec
           10,   // Then 10/sec
       )
   }))
   ```

2. Add structured logging throughout:
   - Subscription lifecycle events
   - Event processing
   - Matching results
   - Delivery attempts
   - Errors and warnings

3. Add log levels:
   - DEBUG: Detailed event processing
   - INFO: Lifecycle events
   - WARN: Backpressure, retries
   - ERROR: Delivery failures, system errors

**Files:**
- Update all existing files with logging
- `internal/streamer/observability/logging.go`

**Testing:**
- Test log output
- Test log sampling
- Test log levels

---

#### Phase 5.3: Health Endpoints (2 days)

**Goal:** Add health checks

**Tasks:**
1. Implement liveness probe:
   ```go
   GET /health/live
   Response: {"status": "alive", "timestamp": "2025-12-29T..."}
   ```

2. Implement readiness probe:
   ```go
   GET /health/ready
   Response: {
       "status": "ready",
       "checks": {
           "mongodb": "ok",
           "etcd": "ok",
           "partitions": "1/1 healthy"
       }
   }
   ```

3. Implement detailed health:
   ```go
   GET /health
   Response: {
       "status": "healthy",
       "uptime": "1h30m",
       "version": "0.1.0",
       "partitions": [
           {"id": 0, "status": "healthy", "lag": "100ms"}
       ],
       "subscriptions": {
           "total": 1000,
           "active": 950
       }
   }
   ```

**Files:**
- `internal/streamer/observability/health.go`
- `internal/streamer/observability/health_test.go`

**Testing:**
- Test all health endpoints
- Test unhealthy states
- Test MongoDB/etcd connection checks

---

### Phase 6: Testing (Week 7-8)

#### Phase 6.1: Integration Testing (3 days)

**Goal:** End-to-end tests with real MongoDB

**Tasks:**
1. Setup test infrastructure:
   - Docker Compose with MongoDB + etcd
   - Test helpers for creating subscriptions
   - Mock Gateway server

2. Implement integration tests:
   - Test full event flow: MongoDB → Streamer → Gateway
   - Test subscription lifecycle
   - Test event matching with various filters
   - Test delivery retry and DLQ

3. Add test scenarios:
   - Single subscription, single event
   - Multiple subscriptions, matching event
   - No matches
   - Tenant isolation
   - Gateway failure and retry

**Files:**
- `tests/integration/streamer_test.go`
- `tests/integration/docker-compose.yml`
- `tests/integration/helpers.go`

**Testing:**
- Run all integration tests
- Verify end-to-end latency
- Check for race conditions

---

#### Phase 6.2: Performance Testing (3 days)

**Goal:** Load test with target metrics

**Tasks:**
1. Setup load test environment:
   - 10k subscriptions
   - 10k events/sec
   - Multiple Gateways

2. Run performance tests:
   - Measure p50, p95, p99 latency
   - Measure throughput
   - Measure resource usage (CPU, memory)
   - Measure Bloom filter false positive rate

3. Identify bottlenecks:
   - Profile CPU usage
   - Profile memory allocations
   - Check for lock contention

4. Optimize as needed:
   - Adjust batch sizes
   - Tune Bloom filter parameters
   - Optimize hot paths

**Files:**
- `tests/performance/load_test.go`
- `tests/performance/benchmarks_test.go`

**Testing:**
- Verify latency < 200ms p99
- Verify throughput >= 10k events/sec
- Verify no memory leaks

---

#### Phase 6.3: Chaos Testing (2 days)

**Goal:** Test failure scenarios

**Tasks:**
1. Implement chaos tests:
   - Node crash during event processing
   - MongoDB connection loss
   - etcd unavailable
   - Gateway unavailable
   - Network partition
   - Resource exhaustion

2. Verify recovery:
   - Resume from checkpoint after crash
   - Reconnect to MongoDB
   - Retry delivery to recovered Gateway
   - No event loss

3. Test graceful shutdown:
   - Stop accepting new subscriptions
   - Flush pending events
   - Checkpoint all partitions
   - Close connections cleanly

**Files:**
- `tests/chaos/crash_test.go`
- `tests/chaos/network_test.go`

**Testing:**
- Verify at-least-once delivery
- Verify no data loss
- Verify graceful degradation

---

### Phase 7: Multi-Backend Support (Week 9) - OPTIONAL

**Goal:** Support multiple MongoDB backends (handled by Puller)

**Note:** With the new architecture, multi-backend support is primarily implemented in the Puller service (Task 016). Streamer automatically supports it by subscribing to all Puller subjects.

**Tasks:**
1. Update subscription index to be aware of backend field (from Puller events)
2. Test with events from multiple backends
3. Verify tenant isolation across backends

**Files:**
- Update `internal/streamer/index/memory.go`

**Testing:**
- Test with 2+ backends from Puller
- Test tenant isolation
- Test independent event flows

---

### Phase 8: Documentation and Migration (Week 10)

**Goal:** Complete documentation and migration guide

**Tasks:**
1. Write operator documentation:
   - Installation guide
   - Configuration reference
   - Deployment guide
   - Monitoring guide
   - Troubleshooting guide

2. Write migration guide:
   - CSP → Streamer migration steps
   - Configuration changes
   - Gateway integration changes
   - Testing checklist

3. Write API documentation:
   - Subscription API reference
   - Event format documentation
   - Error codes reference

4. Update README and CHANGELOG

**Files:**
- `docs/operations/streamer/installation.md`
- `docs/operations/streamer/configuration.md`
- `docs/operations/streamer/monitoring.md`
- `docs/operations/streamer/troubleshooting.md`
- `docs/migrations/csp-to-streamer.md`

---

## Success Criteria

### Functional Requirements
- ✅ Can register 10k+ subscriptions
- ✅ Processes 10k events/sec
- ✅ Correctly matches events to subscriptions
- ✅ Routes events to correct Gateways
- ✅ Handles Gateway failures with retry
- ✅ Supports multi-tenant isolation
- ✅ Resumes from checkpoint after crash

### Performance Requirements
- ✅ P99 latency < 200ms (end-to-end)
- ✅ P99 matching latency < 10ms
- ✅ Bloom filter FP rate < 1%
- ✅ Memory usage < 2GB with 10k subscriptions
- ✅ No memory leaks

### Reliability Requirements
- ✅ At-least-once delivery guarantee
- ✅ Zero event loss during normal operation
- ✅ Graceful degradation under load
- ✅ Survives node crashes
- ✅ Survives network partitions

### Observability Requirements
- ✅ Prometheus metrics for all key operations
- ✅ Structured logging with sampling
- ✅ Health endpoints working
- ✅ Dashboards created
- ✅ Alerts configured

---

## Risk Mitigation

### Technical Risks

**Risk 1: Performance not meeting targets**
- Mitigation: Profile early, benchmark frequently
- Fallback: Reduce target to 5k events/sec for MVP

**Risk 2: Bloom filter false positive rate too high**
- Mitigation: Tune parameters, monitor in production
- Fallback: Increase memory allocation for filters

**Risk 3: etcd becomes bottleneck for checkpoints**
- Mitigation: Use async checkpointing, batch writes
- Fallback: Use MongoDB for checkpoints instead

**Risk 4: Integration with Gateway takes longer than expected**
- Mitigation: Define clear API contract early
- Fallback: Keep CSP running in parallel during migration

### Operational Risks

**Risk 1: Complex deployment**
- Mitigation: Provide Docker images, Kubernetes manifests
- Document common issues

**Risk 2: Difficult to debug**
- Mitigation: Comprehensive logging and metrics
- Build debugging tools

**Risk 3: Migration breaks existing functionality**
- Mitigation: Thorough integration testing
- Support gradual migration (CSP + Streamer side-by-side)

---

## Timeline

| Phase | Duration | Start | End |
|-------|----------|-------|-----|
| Phase 0: Setup | 1 week | Week 1 | Week 1 |
| Phase 1: Subscription Mgmt | 2 weeks | Week 2 | Week 3 |
| Phase 2: JetStream Integration | 1 week | Week 3 | Week 4 |
| Phase 3: Event Matching | 2 weeks | Week 4 | Week 5 |
| Phase 4: JetStream Publishing | 1 week | Week 5 | Week 6 |
| Phase 5: Observability | 1 week | Week 6 | Week 7 |
| Phase 6: Testing | 1 week | Week 7 | Week 8 |
| Phase 7: Multi-Backend (Optional) | 1 week | Week 9 | Week 9 |
| Phase 8: Documentation | 1 week | Week 10 | Week 10 |

**Total:** 7 weeks (MVP) to 10 weeks (with optional features)

---

## Critical Issues and Improvements (from Deep Review)

### Priority 0 - Critical (Execution Blockers)

These issues **MUST** be resolved before starting implementation:

#### P0-1: JetStream Subject Pattern Mismatch

**Location:** Phase 2.0 (lines 554-777), Phase 4.1 (lines 1850-1888)

**Problem:**
- Task 017 uses `puller.events.{collection}.{partition}` (lines 593-606)
- Engine Puller doc (`007.puller.md`) uses same pattern (line 44: `puller.events.{collection}.>`)
- But actual Puller design doc shows multi-backend: `puller.events.{backend}.{collection}.{partition}`
- This inconsistency will cause subscription mismatches

**Impact:** Streamer won't receive events from Puller, complete system failure

**Solution:**
1. Define canonical subject pattern in shared package:
   ```go
   // internal/messaging/subjects.go
   package messaging

   func PullerEventSubject(backend, collection string, partition int) string {
       if backend == "" || backend == "default" {
           return fmt.Sprintf("puller.events.%s.%d", collection, partition)
       }
       return fmt.Sprintf("puller.events.%s.%s.%d", backend, collection, partition)
   }

   func StreamerEventSubject(gatewayID, subscriptionID string) string {
       return fmt.Sprintf("events.%s.%s", gatewayID, subscriptionID)
   }
   ```
2. Update all design docs and code to use this shared definition
3. Add integration test to verify subject pattern matching

**Action Required:** Create shared subject definition before Phase 2

---

#### P0-2: Deduplication Strategy Flaw

**Location:** Phase 2.2 (lines 829-941)

**Problem:**
Current design uses clusterTime-based deduplication assuming monotonic increase per document:
```go
if evt.ClusterTime.T <= lastTime.T && evt.ClusterTime.I <= lastTime.I {
    return true  // Duplicate
}
```

But MongoDB clusterTime is **cluster-wide**, not per-document. Multiple documents can have the same clusterTime. Additionally:
- LRU eviction (line 888-900) can evict entries
- After Streamer restarts, cache is empty
- JetStream redelivery after restart may re-deliver old events
- False positive: Event with same clusterTime as evicted entry will be incorrectly marked as duplicate

**Impact:** **Data loss** - legitimate events may be dropped as duplicates after restart or LRU eviction

**Solution Options:**

**Option A: Hybrid Key (Recommended)**
```go
type DedupKey struct {
    DocumentID  string
    ClusterTime primitive.Timestamp
    TxnNumber   *int64  // Tie-breaker for same clusterTime
}

func (d *Deduplicator) IsDuplicate(evt *NormalizedEvent) bool {
    key := d.makeKey(evt.DocumentID, evt.ClusterTime, evt.TxnNumber)

    if d.cache.Contains(key) {
        return true
    }

    d.cache.Add(key, time.Now())
    return false
}
```

**Option B: JetStream Exactly-Once Delivery**
- Requires NATS JetStream 2.9+
- Use JetStream's deduplication window (requires message ID)
- More complex, but guarantees no duplicates

**Option C: Push Dedup to Gateway/Client**
- Accept at-least-once delivery in Streamer
- Let Gateway or client handle deduplication
- Simpler Streamer, but shifts complexity

**Action Required:** Choose and implement solution before Phase 2.2

---

#### P0-3: Gateway-Streamer Registration Failure Recovery Missing

**Location:** Phase 1.4 (lines 313-551)

**Problem:**
Registration protocol defined (line 324-351) but failure recovery not specified:
- What if Gateway's registration HTTP call fails?
- What if Gateway crashes before receiving 201 response?
- What if Streamer crashes after accepting registration but before persisting?
- Current code (line 508-534) logs error but doesn't retry

**Impact:** Lost subscriptions → clients won't receive events

**Solution:**
```go
// In Gateway
func (g *Gateway) registerWithRetry(ctx context.Context, req *RegisterRequest) error {
    backoff := &exponentialBackoff{
        initial: 100 * time.Millisecond,
        max:     10 * time.Second,
        maxRetries: 10,
    }

    for {
        err := g.streamerClient.Register(ctx, req)
        if err == nil {
            return nil
        }

        if !isRetriable(err) {
            return err
        }

        delay := backoff.Next()
        select {
        case <-time.After(delay):
            continue
        case <-ctx.Done():
            return ctx.Err()
        }
    }
}

// In Streamer - persist BEFORE responding
func (h *SubscriptionHandler) Register(req *RegisterRequest) (*RegisterResponse, error) {
    // 1. Validate request
    // 2. Create subscription
    sub := &Subscription{...}

    // 3. PERSIST to etcd first
    if err := h.persistence.SaveSubscription(sub); err != nil {
        return nil, fmt.Errorf("failed to persist: %w", err)
    }

    // 4. Add to in-memory index
    h.index.Add(sub)

    // 5. Return success
    return &RegisterResponse{SubscriptionID: sub.ID}, nil
}
```

**Action Required:** Add retry logic to Phase 1.4, add persistence step

---

#### P0-4: CEL Compilation Happens Twice (Inconsistency Risk)

**Location:** Phase 3.2 (lines 1164-1481)

**Problem:**
Current design (line 1170):
1. Gateway compiles CEL expression to validate (line 1276)
2. Gateway sends compiled expression **as string** to Streamer (line 1292)
3. Streamer re-compiles from string (line 1377-1392)

Risks:
- Gateway and Streamer may have different CEL library versions
- Compilation could succeed in Gateway but fail in Streamer
- Expression semantics could differ between versions
- Double compilation cost (though cached)

**Impact:** Version skew → inconsistent behavior, hard-to-debug mismatches

**Solution:**
**Simplify: Gateway validates only, Streamer compiles**

```go
// Gateway: Only validate syntax
func (h *Hub) HandleSubscribe(req *SubscribeRequest) error {
    // 1. Build CEL expression string
    celExpr := h.celCompiler.BuildExpression(req.Filters)

    // 2. VALIDATE ONLY (don't store Program)
    if err := h.celCompiler.Validate(celExpr); err != nil {
        return &SubscribeError{
            Code: "CEL_SYNTAX_ERROR",
            Message: err.Error(),
        }
    }

    // 3. Send to Streamer (Streamer will compile)
    streamerReq := &RegisterRequest{
        CELExpression: celExpr,  // String only
        ...
    }

    return h.streamerClient.Register(ctx, streamerReq)
}

// Streamer: Compile and execute
func (m *CELMatcher) Evaluate(sub *Subscription, evt *NormalizedEvent) (bool, error) {
    // Get cached program or compile
    program, err := m.getOrCompileProgram(sub.CELExpression)
    if err != nil {
        // Should not happen since Gateway validated
        log.Error("CEL compilation failed: %v", err)
        return false, err
    }

    return m.executeProgram(program, evt)
}
```

Benefits:
- Single source of truth (Streamer)
- No version skew risk
- Gateway simpler (just validates)

**Action Required:** Simplify CEL ownership before Phase 3.2

---

#### P0-5: Bloom Filter Missing Nested Field Handling

**Location:** Phase 3.1 (lines 946-1161)

**Problem:**
Key extraction algorithm (lines 996-1009) only handles top-level fields:
```go
for field, value := range evt.FullDoc {
    key := fmt.Sprintf("%s:%v", field, value)
    keys = append(keys, key)
}
```

This fails for nested documents:
```json
// Event
{
  "user": {
    "name": "Alice",
    "age": 30
  }
}

// Subscription filter: user.name == "Alice"
// Extracted key: "user:[map[name:Alice age:30]]"  ❌ WRONG
// Expected key: "user.name:Alice"                 ✓ CORRECT
```

**Impact:** Bloom filter won't match nested field filters → CEL evaluation skipped → **no events delivered**

**Solution:**
Add recursive flattening:

```go
func (e *BloomKeyExtractor) ExtractEventKeys(evt *NormalizedEvent) []string {
    keys := make([]string, 0)
    keys = append(keys, fmt.Sprintf("collection:%s", evt.Collection))

    // Recursively flatten all fields
    flattenFields("", evt.FullDoc, &keys)
    return keys
}

func flattenFields(prefix string, data map[string]interface{}, keys *[]string) {
    for field, value := range data {
        fullField := field
        if prefix != "" {
            fullField = prefix + "." + field
        }

        switch v := value.(type) {
        case map[string]interface{}:
            // Recurse into nested object
            flattenFields(fullField, v, keys)
        case []interface{}:
            // Handle arrays
            for i, item := range v {
                key := fmt.Sprintf("%s[%d]:%v", fullField, i, item)
                *keys = append(*keys, key)
            }
        default:
            // Leaf value
            key := fmt.Sprintf("%s:%v", fullField, value)
            *keys = append(*keys, key)
        }
    }
}

// Test case
func TestNestedFieldExtraction(t *testing.T) {
    evt := &NormalizedEvent{
        Collection: "users",
        FullDoc: map[string]interface{}{
            "user": map[string]interface{}{
                "name": "Alice",
                "age":  30,
            },
            "status": "active",
        },
    }

    extractor := &BloomKeyExtractor{}
    keys := extractor.ExtractEventKeys(evt)

    expected := []string{
        "collection:users",
        "user.name:Alice",
        "user.age:30",
        "status:active",
    }

    assert.ElementsMatch(t, expected, keys)
}
```

**Action Required:** Implement nested field extraction in Phase 3.1

---

#### P0-6: Phase Dependencies Not Enforced in Timeline

**Location:** Timeline section (lines 2270-2283)

**Problem:**
Timeline shows sequential phases, but dependencies are implicit:

| Issue | Current Timeline | Problem |
|-------|-----------------|---------|
| Phase 4.0 depends on 4.1 | 4.0: Week 5, 4.1: Week 5 | Gateway integration needs JetStream publisher first |
| Phase 3 depends on 1.4 | 3: Week 4, 1.4: Week 3 | Matching needs registration protocol complete |
| Phase 6 depends on all | 6: Week 7 | Can't test incomplete features |

**Impact:** Implementation stalls when dependencies block progress, timeline slips by 2-3 weeks

**Solution:**
Create explicit dependency graph and reorder:

```
Week 1:  Phase 0 (Setup)
Week 2:  Phase 1.1-1.3 (Subscription Index + API)
Week 3:  Phase 1.4 (Registration Protocol) ←─┐
         Phase 2.0 (JetStream Setup)          │
Week 4:  Phase 2.1-2.2 (Consumer + Dedup)     │ Parallel
         Phase 3.1 (Bloom Filter)              │
Week 5:  Phase 3.2-3.3 (CEL Matching)         │
Week 6:  Phase 4.1 (JetStream Publisher)      │
Week 7:  Phase 4.0 (Gateway Integration) ─────┘ Depends on all above
         Phase 5 (Observability)
Week 8:  Phase 6 (Testing)
Week 9:  Phase 7 (Multi-Backend - Optional)
Week 10: Phase 8 (Documentation)
```

**Action Required:** Reorder phases to respect dependencies before starting

---

#### P0-7: Puller Service Not Implemented (Blocker)

**Location:** Dependencies section (line 19), Architecture diagram (lines 71-83)

**Problem:**
- Task 017 assumes Puller service exists (line 19: "depends on Puller service design")
- Architecture shows Puller as implemented (lines 72-79)
- But Task 016 status is "Planned" (not implemented)
- No indication of when Puller will be ready

**Impact:** **Cannot start Streamer implementation** without Puller

**Solution:**

**Immediate Actions:**
1. Verify Task 016 status with team
2. If not started, either:
   - **Option A:** Prioritize Task 016 completion first (add 2-3 weeks to timeline)
   - **Option B:** Create mock Puller for development:
     ```go
     // internal/streamer/testing/mock_puller.go
     type MockPuller struct {
         js nats.JetStreamContext
     }

     func (m *MockPuller) PublishEvent(evt *NormalizedEvent) error {
         subject := fmt.Sprintf("puller.events.%s.%d",
                                 evt.Collection,
                                 hashPartition(evt.DocumentID))
         data, _ := json.Marshal(evt)
         _, err := m.js.Publish(subject, data)
         return err
     }
     ```
   - **Option C:** Implement minimal Puller as Phase 0.5 of Task 017

**Recommended:** Option A (prioritize Task 016) to avoid rework

**Updated Timeline:**
- Task 016: 2-3 weeks
- Task 017: 7-10 weeks
- **Total: 9-13 weeks**

**Action Required:** Coordinate with team on Puller status BEFORE starting Task 017

---

### Priority 1 - High Priority (Require Immediate Clarification)

#### P1-1: Event Schema Mismatch Between Puller and Streamer

**Location:** Phase 2.1 (lines 780-826), Puller design doc

**Problem:**
Task 017 expects `NormalizedEvent` (lines 860-883):
```go
type NormalizedEvent struct {
    Tenant      string
    Collection  string
    DocumentID  string
    FullDoc     map[string]interface{}
    ClusterTime primitive.Timestamp
    TxnNumber   *int64
}
```

But Puller design doc may use different schema. No shared type definition.

**Solution:**
Create shared event types package:
```go
// internal/events/types.go
package events

type NormalizedEvent struct {
    // Required fields
    Tenant      string                 `json:"tenant"`
    Collection  string                 `json:"collection"`
    DocumentID  string                 `json:"documentId"`

    // Operation
    Type        OperationType          `json:"type"`  // insert/update/delete

    // Document content
    FullDoc     map[string]interface{} `json:"fullDocument,omitempty"`
    UpdateDesc  *UpdateDescription     `json:"updateDescription,omitempty"`

    // Metadata
    ClusterTime primitive.Timestamp    `json:"clusterTime"`
    TxnNumber   *int64                 `json:"txnNumber,omitempty"`

    // Optional backend identifier
    Backend     string                 `json:"backend,omitempty"`
}

type OperationType string
const (
    OpInsert  OperationType = "insert"
    OpUpdate  OperationType = "update"
    OpReplace OperationType = "replace"
    OpDelete  OperationType = "delete"
)
```

**Action Required:** Define shared schema before Phase 2

---

#### P1-2: Gateway Heartbeat Implementation Missing from Codebase

**Location:** Phase 1.4 (lines 434-471)

**Problem:**
Design shows heartbeat implementation but current Gateway code doesn't have it:
- No `startHeartbeatLoop()` in `internal/api/realtime/`
- No Gateway ID generation on startup
- No periodic HTTP PUT to Streamer

**Solution:**
Add heartbeat as separate task for Gateway team:
```markdown
## Task: Add Gateway Heartbeat for Streamer Integration

### Changes to Gateway:
1. Generate unique GatewayID on startup
2. Add heartbeat loop (60s interval)
3. Track active subscriptions for heartbeat
4. Persist gateway ID for restart recovery

### Files to modify:
- internal/api/realtime/gateway.go (new)
- internal/api/realtime/server.go
- config/gateway.yaml
```

**Action Required:** Create separate Gateway integration task

---

#### P1-3: Subscription Index In-Memory Only (No Persistence)

**Location:** Phase 1.1-1.2 (lines 189-263)

**Problem:**
`MemorySubscriptionIndex` (line 196-202) has no persistence:
- Streamer restart → all subscriptions lost
- Gateway heartbeat will re-register, but window of event loss
- No recovery for subscriptions created during outage

**Solution:**
Add persistence layer:
```go
type PersistentSubscriptionIndex struct {
    memory  *MemorySubscriptionIndex
    store   SubscriptionStore  // etcd or MongoDB
    syncMu  sync.Mutex
}

type SubscriptionStore interface {
    Save(sub *Subscription) error
    Load(id string) (*Subscription, error)
    LoadAll() ([]*Subscription, error)
    Delete(id string) error
}

func (idx *PersistentSubscriptionIndex) Register(sub *Subscription) error {
    // 1. Save to persistent store first
    if err := idx.store.Save(sub); err != nil {
        return err
    }

    // 2. Add to memory index
    return idx.memory.Register(sub)
}

func (idx *PersistentSubscriptionIndex) RecoverFromStore() error {
    subs, err := idx.store.LoadAll()
    if err != nil {
        return err
    }

    for _, sub := range subs {
        // Rebuild memory index
        idx.memory.Register(sub)
    }

    return nil
}
```

**Action Required:** Add persistence in Phase 1.3

---

#### P1-4: Bloom Filter Rebuild Blocks All Operations

**Location:** Phase 3.1 (lines 1063-1105)

**Problem:**
`Build()` method (line 1063) takes write lock, blocking all queries:
```go
func (idx *BloomFilterIndex) Build(subs []*Subscription) error {
    // Rebuilds entire filter - takes 100ms+ for 100k subs
    // Blocks all GetCandidates() calls during rebuild
}
```

**Solution:**
Use double-buffering:
```go
type BloomFilterIndex struct {
    current  atomic.Value  // *BloomFilterSet
    building *BloomFilterSet
    mu       sync.Mutex
}

func (idx *BloomFilterIndex) RebuildAsync(subs []*Subscription) error {
    // Build new filter without blocking reads
    newFilter := &BloomFilterSet{}
    newFilter.Build(subs)

    // Atomic swap
    idx.current.Store(newFilter)
    return nil
}
```

**Action Required:** Implement non-blocking rebuild in Phase 3.1

---

#### P1-5: Missing Dead Letter Queue (DLQ) Stream

**Location:** Phase 2.0 (lines 554-777)

**Problem:**
JetStream configuration creates PULLER_EVENTS and GATEWAY_EVENTS streams but no DLQ for failed events:
- What if CEL evaluation panics?
- What if event JSON is malformed?
- What if Gateway is down for >1h (exceeds retention)?

Current handling (line 719-744) uses `msg.Term()` which **deletes** the event.

**Solution:**
Add DLQ stream:
```bash
nats stream add STREAMER_DLQ \
  --subjects "dlq.streamer.>" \
  --storage file \
  --retention limits \
  --max-age 7d \
  --replicas 3
```

```go
func (c *JetStreamEventConsumer) handleMessage(msg *nats.Msg) {
    defer func() {
        if r := recover(); r != nil {
            log.Error("Panic in event processing: %v", r)
            c.sendToDLQ(msg, fmt.Sprintf("panic: %v", r))
            msg.Ack()  // Ack so it doesn't retry
        }
    }()

    // Process event
    if err := c.processEvent(msg); err != nil {
        metadata, _ := msg.Metadata()
        if metadata.NumDelivered > 3 {
            c.sendToDLQ(msg, err.Error())
            msg.Ack()
        } else {
            msg.NakWithDelay(5 * time.Second)
        }
    }
}

func (c *JetStreamEventConsumer) sendToDLQ(msg *nats.Msg, reason string) {
    dlqMsg := &DLQMessage{
        OriginalSubject: msg.Subject,
        Data:            msg.Data,
        Reason:          reason,
        Timestamp:       time.Now(),
        NumDeliveries:   msg.Metadata().NumDelivered,
    }

    data, _ := json.Marshal(dlqMsg)
    c.js.Publish("dlq.streamer.failed", data)
}
```

**Action Required:** Add DLQ stream to Phase 2.0

---

#### P1-6: Checkpoint Storage Choice Not Justified

**Location:** Config section (lines 636-683), line 679: `backend: etcd`

**Problem:**
Design specifies etcd for checkpoints without justification:
- Why etcd over MongoDB?
- What if etcd is unavailable?
- How does this interact with Puller's checkpoint storage?

**Analysis:**

| Storage | Pros | Cons |
|---------|------|------|
| etcd | Strong consistency, watch API | Extra dependency, ops complexity |
| MongoDB | Already available, transactional | Slightly slower writes |
| Memory only | Fast, simple | Lost on restart |

**Question:** Does Streamer even need checkpoints?
- Streamer consumes from JetStream, not MongoDB directly
- JetStream consumer maintains its own offset
- Subscription state needs persistence, but not "checkpoint" per se

**Solution:**
Clarify terminology:
- **JetStream consumer offset**: Managed by NATS automatically (no explicit checkpoint needed)
- **Subscription state**: Store in etcd or MongoDB (choice based on existing infra)
- **Metrics/health**: In-memory, exported via Prometheus

```yaml
streamer:
  state:
    backend: mongodb  # Or etcd, based on existing infra
    mongodb:
      uri: ${MONGO_URI}
      database: syntrix_state
      collection: streamer_subscriptions
    # No explicit checkpoints needed - JetStream handles it
```

**Action Required:** Clarify storage requirements and choose based on existing infrastructure

---

#### P1-7: Integration Test Environment Setup Missing

**Location:** Phase 6.1 (lines 2036-2069)

**Problem:**
Integration tests specified (line 2038-2059) but no setup instructions:
- How to run MongoDB + NATS + Puller + Streamer + Gateway together?
- Docker Compose file not provided
- No test data setup scripts

**Solution:**
Create integration test infrastructure:

```yaml
# tests/integration/docker-compose.yml
version: '3.8'
services:
  mongodb:
    image: mongo:7.0
    command: --replSet rs0
    ports:
      - "27017:27017"

  nats:
    image: nats:2.10-alpine
    command: "-js -sd /data"
    ports:
      - "4222:4222"
      - "8222:8222"

  puller:
    build: ../../
    command: puller
    depends_on:
      - mongodb
      - nats
    environment:
      MONGO_URI: mongodb://mongodb:27017
      NATS_URL: nats://nats:4222

  streamer:
    build: ../../
    command: streamer
    depends_on:
      - nats
      - puller
    ports:
      - "8083:8083"
    environment:
      NATS_URL: nats://nats:4222
```

**Action Required:** Create integration test setup in Phase 6.1

---

#### P1-8: Performance Targets Not Validated Against Hardware

**Location:** Success Criteria (lines 2211-2216)

**Problem:**
Performance targets defined without hardware specs:
- "P99 latency < 200ms" - on what hardware?
- "10k events/sec" - with what CPU/memory?
- "100k subscriptions" - what memory footprint?

**Solution:**
Add hardware assumptions:
```markdown
## Performance Targets

### Test Environment Specification
- **CPU:** 4 cores (2.5 GHz)
- **Memory:** 8 GB RAM
- **Network:** 1 Gbps LAN
- **MongoDB:** 3-node replica set (separate hosts)
- **NATS:** 3-node JetStream cluster

### Expected Performance
| Metric | Target | Notes |
|--------|--------|-------|
| P50 latency | <50ms | End-to-end (MongoDB → Client) |
| P99 latency | <200ms | Including CEL evaluation |
| Throughput | 10k events/sec | Per Streamer instance |
| Max subscriptions | 100k | ~2GB memory footprint |
| Bloom filter FP rate | <1% | With 100k subscriptions |
| CEL evaluation | <10ms p99 | Per subscription |

### Scaling Characteristics
- **Horizontal scaling:** Linear up to 10 instances
- **Memory per 10k subs:** ~200MB
- **CPU per 1k events/sec:** ~10% of 1 core
```

**Action Required:** Validate targets with benchmark before Phase 6.2

---

#### P1-9: Multi-Backend Support Confusion

**Location:** Phase 7 (lines 2145-2163), Subject patterns (lines 243-251)

**Problem:**
- Phase 7 says "optional" (line 2145)
- But subject patterns show multi-backend as primary design (line 247: `puller.events.{backend}.{collection}.{partition}`)
- Implementation plan has conflicting patterns (line 594 vs line 597)

**Clarification Needed:**
Is multi-backend:
- **Required:** Always expect backend in subject (e.g., `puller.events.default.users.0`)
- **Optional:** Single backend = no backend prefix (e.g., `puller.events.users.0`)

**Recommendation:**
Make it **optional with sensible defaults**:
```go
// Subject pattern logic
func PullerSubjectPattern(multiBackend bool) string {
    if multiBackend {
        return "puller.events.*.>"  // Subscribe to all backends
    }
    return "puller.events.>"  // Single backend (no backend prefix)
}

// In Streamer config
streamer:
  puller:
    multi_backend: false  # Default to single backend
    subject_pattern: "puller.events.>"  # Auto-configured
```

**Action Required:** Clarify multi-backend requirements with team

---

### Priority 2 - Medium Priority (Important but Not Blocking)

#### P2-1: Metrics Cardinality Explosion Risk

**Location:** Phase 5.1 (lines 1893-1940), line 1908: `SubscriptionsByTenant`

**Problem:**
```go
SubscriptionsByTenant *prometheus.GaugeVec  // Label: tenant
```

With 1000 tenants × 10 subscriptions each = 10,000 unique metric series. Prometheus best practice is <10k series per metric.

**Solution:**
Use aggregated metrics instead:
```go
// REMOVE high-cardinality metrics
// SubscriptionsByTenant *prometheus.GaugeVec

// ADD low-cardinality metrics
SubscriptionsTotal prometheus.Gauge  // Total across all tenants

// Use separate monitoring system for per-tenant metrics
// e.g., CloudWatch/Datadog with tenant dimension
```

For per-tenant monitoring, use logs:
```go
log.Info("subscription_metrics",
    zap.String("tenant", sub.Tenant),
    zap.Int("count", count))
```

**Action Required:** Review metrics design in Phase 5.1

---

#### P2-2: Log Sampling Could Hide Critical Errors

**Location:** Phase 5.2 (lines 1943-1980), lines 1950-1956

**Problem:**
```go
zapcore.NewSamplerWithOptions(core,
    time.Second,
    100,  // First 100/sec
    10,   // Then 10/sec
)
```

Sampling applies to ALL log levels, including ERROR. A critical error repeated >100 times/sec would be sampled.

**Solution:**
Only sample DEBUG/INFO, never ERROR/WARN:
```go
func NewProductionLogger() (*zap.Logger, error) {
    config := zap.NewProductionConfig()
    core, err := config.Build()
    if err != nil {
        return nil, err
    }

    // Sample only DEBUG and INFO levels
    sampledCore := zapcore.NewSamplerWithOptions(
        zapcore.NewCore(
            zapcore.NewJSONEncoder(config.EncoderConfig),
            os.Stdout,
            zap.LevelEnablerFunc(func(lvl zapcore.Level) bool {
                return lvl == zapcore.DebugLevel || lvl == zapcore.InfoLevel
            }),
        ),
        time.Second, 100, 10,
    )

    // Never sample WARN/ERROR
    unsampledCore := zapcore.NewCore(
        zapcore.NewJSONEncoder(config.EncoderConfig),
        os.Stdout,
        zap.LevelEnablerFunc(func(lvl zapcore.Level) bool {
            return lvl >= zapcore.WarnLevel
        }),
    )

    combinedCore := zapcore.NewTee(sampledCore, unsampledCore)
    return zap.New(combinedCore), nil
}
```

**Action Required:** Fix sampling config in Phase 5.2

---

#### P2-3: Missing Rate Limiting on Subscription API

**Location:** Phase 1.3 (lines 266-310)

**Problem:**
No rate limiting on subscription endpoints:
- Malicious Gateway could create 1M subscriptions/sec
- No per-tenant limits
- No global rate limiting

**Solution:**
Add rate limiting middleware:
```go
type RateLimiter struct {
    perGateway *rate.Limiter  // 100 req/sec per gateway
    global     *rate.Limiter  // 1000 req/sec global
}

func (s *Server) RegisterSubscription(w http.ResponseWriter, r *http.Request) {
    gatewayID := r.Header.Get("X-Gateway-ID")

    // Check gateway-specific limit
    if !s.rateLimiter.AllowGateway(gatewayID) {
        http.Error(w, "Rate limit exceeded for gateway", http.StatusTooManyRequests)
        return
    }

    // Check global limit
    if !s.rateLimiter.AllowGlobal() {
        http.Error(w, "Global rate limit exceeded", http.StatusTooManyRequests)
        return
    }

    // Process request
    s.handleRegister(w, r)
}
```

**Action Required:** Add rate limiting in Phase 1.3

---

#### P2-4: No Circuit Breaker for Gateway Delivery

**Location:** Phase 4.1 (lines 1850-1888)

**Problem:**
JetStream publisher has no circuit breaker. If Gateway is down, Streamer will keep publishing, wasting resources.

**Solution:**
Add circuit breaker:
```go
import "github.com/sony/gobreaker"

type JetStreamPublisher struct {
    js      nats.JetStreamContext
    breaker *gobreaker.CircuitBreaker
}

func NewJetStreamPublisher(js nats.JetStreamContext) *JetStreamPublisher {
    cb := gobreaker.NewCircuitBreaker(gobreaker.Settings{
        Name:        "jetstream-publisher",
        MaxRequests: 3,
        Interval:    10 * time.Second,
        Timeout:     30 * time.Second,
        ReadyToTrip: func(counts gobreaker.Counts) bool {
            return counts.ConsecutiveFailures > 5
        },
    })

    return &JetStreamPublisher{js: js, breaker: cb}
}

func (p *JetStreamPublisher) Publish(ctx context.Context, matches []*MatchResult) error {
    _, err := p.breaker.Execute(func() (interface{}, error) {
        return nil, p.doPublish(ctx, matches)
    })

    if err == gobreaker.ErrOpenState {
        log.Warn("Circuit breaker open, dropping events temporarily")
        return nil  // Don't fail, events will be lost (acceptable with JetStream)
    }

    return err
}
```

**Action Required:** Add circuit breaker in Phase 4.1

---

#### P2-5: Graceful Shutdown May Lose In-Flight Events

**Location:** Threading model (lines 584-631), no shutdown implementation

**Problem:**
No graceful shutdown strategy defined. On SIGTERM:
- In-flight events may be lost
- Subscriptions not de-registered
- JetStream messages not ack'd

**Solution:**
```go
func (s *Streamer) Shutdown() error {
    log.Info("Starting graceful shutdown...")

    // 1. Stop accepting new subscriptions
    s.controlPlane.Close()

    // 2. Wait for in-flight events to complete (with timeout)
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()

    done := make(chan struct{})
    go func() {
        s.pipeline.Wait()  // Wait for all event processing to complete
        close(done)
    }()

    select {
    case <-done:
        log.Info("All in-flight events processed")
    case <-ctx.Done():
        log.Warn("Shutdown timeout, some events may be lost")
    }

    // 3. Close JetStream consumer (will nak unprocessed messages)
    s.jetStreamConsumer.Close()

    // 4. Persist final state
    s.stateManager.Flush()

    log.Info("Shutdown complete")
    return nil
}
```

**Action Required:** Add graceful shutdown in Phase 0

---

#### P2-6: Monitoring Alerts Missing Runbook Links

**Location:** Phase 5.3 (lines 1983-2032)

**Problem:**
Health endpoints defined but no alert definitions. When alerts fire, operators won't know what to do.

**Solution:**
Add alert definitions with runbooks:
```yaml
# alerts/streamer.yml
groups:
  - name: streamer
    interval: 30s
    rules:
      - alert: StreamerHighLatency
        expr: histogram_quantile(0.99, streamer_match_latency_seconds) > 0.2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Streamer p99 latency is {{ $value }}s"
          description: "Event matching is slow, may indicate Bloom filter degradation"
          runbook: "https://wiki.company.com/runbooks/streamer-high-latency"

      - alert: StreamerSubscriptionExpiry
        expr: rate(streamer_subscriptions_expired_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High subscription expiry rate: {{ $value }}/sec"
          description: "Gateways may not be sending heartbeats"
          runbook: "https://wiki.company.com/runbooks/streamer-subscription-expiry"

      - alert: StreamerJetStreamLag
        expr: nats_consumer_num_pending{consumer="streamer-consumer"} > 10000
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "JetStream consumer lag: {{ $value }} messages"
          description: "Streamer cannot keep up with event rate, need horizontal scaling"
          runbook: "https://wiki.company.com/runbooks/streamer-jetstream-lag"
```

**Action Required:** Create alert definitions and runbooks in Phase 5.3

---

#### P2-7: No Schema Evolution Strategy

**Location:** Event types (lines 540-578), NormalizedEvent definition (line 551-560)

**Problem:**
What if we need to add fields to `NormalizedEvent` later?
- Puller publishes new schema
- Old Streamer can't parse it
- Or vice versa

**Solution:**
Add version field and backward compatibility:
```go
type NormalizedEvent struct {
    SchemaVersion int    `json:"_version"`  // Default: 1

    // V1 fields
    Type        EventType              `json:"type"`
    Tenant      string                 `json:"tenant"`
    Collection  string                 `json:"collection"`
    DocumentID  string                 `json:"documentId"`
    FullDoc     map[string]interface{} `json:"fullDocument,omitempty"`
    ClusterTime primitive.Timestamp    `json:"clusterTime"`

    // Future: V2 fields
    // Metadata    map[string]string   `json:"metadata,omitempty"`  // Added in V2
}

func (evt *NormalizedEvent) UnmarshalJSON(data []byte) error {
    type Alias NormalizedEvent
    aux := &struct {
        *Alias
    }{
        Alias: (*Alias)(evt),
    }

    if err := json.Unmarshal(data, &aux); err != nil {
        return err
    }

    // Set default version if not present
    if evt.SchemaVersion == 0 {
        evt.SchemaVersion = 1
    }

    // Handle version-specific logic
    switch evt.SchemaVersion {
    case 1:
        return nil
    case 2:
        // Handle V2-specific fields
        return nil
    default:
        return fmt.Errorf("unsupported schema version: %d", evt.SchemaVersion)
    }
}
```

**Action Required:** Add schema versioning to event types in Phase 0

---

## Risk Assessment Summary

### Execution Risk Matrix

| Priority | Count | Impact | Timeline Impact |
|----------|-------|--------|----------------|
| P0 (Critical) | 7 | Execution blockers | +3-5 weeks |
| P1 (High) | 9 | Require clarification | +1-2 weeks |
| P2 (Medium) | 7 | Quality/maintainability | +1 week |

### Updated Timeline Estimate

**Original:** 7-10 weeks
**Realistic (with P0/P1 fixes):** 9-12 weeks
- Week 0: Resolve P0 issues (1-2 weeks)
- Week 1-8: Implementation (8 weeks)
- Week 9-12: Testing and hardening (3-4 weeks)

### Critical Path

```
Puller Service (Task 016) [2-3 weeks]
    ↓
P0 Issue Resolution [1-2 weeks]
    ↓
Phase 0-1: Foundation [2 weeks]
    ↓
Phase 2-3: Core Logic [3 weeks]
    ↓
Phase 4: Gateway Integration [2 weeks]
    ↓
Phase 5-6: Observability & Testing [2 weeks]
```

**Total: 12-14 weeks** (including Puller)

---

## Next Steps

1. **Review and approve this plan**
2. **CRITICAL: Verify Task 016 (Puller) status** - This is a blocker
3. **Resolve all P0 issues** before starting implementation
4. **Clarify all P1 questions** with team/stakeholders
5. **Set up development environment**
6. **Create GitHub issues for each phase**
7. **Start with Phase 0: Setup and Foundation**
8. **Daily standups to track progress**
9. **Weekly demos to stakeholders**

---

## References

- [Streamer Design Documents](docs/design/server/streamer/)
- [Current CSP Implementation](internal/csp/)
- [Storage Interface](internal/storage/types/)
- [CEL Matching Logic](internal/api/realtime/cel.go)
- [Puller Service Design](docs/design/server/puller/001.architecture.md)
- [Task 016: Puller Implementation](016.2025-12-29-change-stream-puller.md)
